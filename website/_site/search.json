[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "I will replace this later with my actual data"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Why did the conclusion take a break? Because it needed some time to ‘sum up’ its thoughts!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello!\nHello and welcome to my website! My name is Billy and I am a graduate student at Georgetown University. I am currently pursuing a Master’s in Data Science and Analytics and will use this website as a place to display my work. I hope you find my work insightful and enjoy your time on my website!\n\n\nGrowing Up\nI grew up in Scotch Plains, New Jersey along with my two sisters, Melissa and Jessica. We are triplets! Below is a picture of me with my family and another of our dog Roxie.\n\n \n\n\n\nEducation\nI graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics along with a PPE (Politics, Philosophy, & Economics) Concentration and Political Science minor. I absolutely loved my time at Villanova between the academics, friendships, the school winning the NCAA Championship and more!. As an undergrad, I was involved in a number of student organizations, including the Blue Key Society, where I served as a tour guide for undergraduate admissions. I also organized accepted students days for the university.\nOne of the best experiences I had during college was studying abroad. In 2019, I participated in the Institute of Politics and Economics Program at the University of Cambridge where I took a number of economic classes and study international relations and history. Seeing how these subject matters interact with analytics truly fascinated me, which is one reason why I am pursuing an advanced degree in data science. I am extremely excited to continue my education and pursue a masters degree in data science!\n\n\nTop Shelf Designs, LLC\nDuring my senior year at Villanova, I had the opportunity to start my own business with my sisters! Our business, Top Shelf Designs, LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. Being an entrepreneur has been an incredibly rewarding journey thus far, and I am eager to see where it takes us. Here is a link to our website!\n\n\nHobbies\nI’m an avid sports fan, and love watching the New York Yankees, Villanova basketball, and NY Giants. Unfortunately, none of my teams have been doing too well lately, but I’m hoping the Giants can turn it around this season! I’m fascinated by how analytics is being used to transform the sports landscape - even across college sports! While at Villanova, I researched the Houston Astros Cheating Scandal. I would love to go back and take an even deeper dive into the subject one day!\nI love music - you can often find me with headphones or earbuds in while doing work or exploring the DC area! Additionally, I find playing instruments to be a great creative outlet after a long day of work. I grew up taking piano lessons and during COVID started to teach myself how to play the guitar!\nWhenever I have the chance, I try to travel. Wile abroad, I was lucky enough to travel to a few places in Europe. My favorite spot has to be Italy - from the history and culture to the food, wine, and sports, I am counting down the days until I can go back.\nFinally, I thoroughly enjoy books, movies, video games in addition to fun, in-person experiences. Lately, I’ve been exploring the DC area which I am loving so far! If you have any recommendations - please let me know! I’m alwyas willing to try something new!\n\n\nGeneral Info (inc. netID)\n\n\n\nname\nBilly McGloin\n\n\nGU netID\nwtm30\n\n\nLinkedIn\nlink\n\n\n\n\n\nJokes\nI always like to have a laugh so below are some jokes I hope you’ll enjoy!\n\nWebsite Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki\n\n\n\nAssorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!\n\n\n\n\nHoya Saxa!\n\n\n\nme and a bunch of rocks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN_website",
    "section": "",
    "text": "This will be updated with some cool stuff in a bit! Stay tuned!\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  This page shows the raw data, the code used to clean it, and the modified data. It’s a journal of my data cleaning process. Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nBaseballr\n\n\nncaahoopR\nlet’s clean the Villanova 2021-22 data with R:\nhere is a screen shot of the first few rows and columns of the raw data:  \n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\n# let's load in the data\nnova2122 &lt;- read.csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n1140539\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova2122)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova2122 &lt;- nova2122 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n539939\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova2122 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova2122.csv\", row.names = FALSE)\n\n\nHere is a screen shot of the modified data:  \n\n2019-20 Season cleaning\n\n\nCode\n# let's load in the data\nnova1920 &lt;- read.csv('./data/raw_data/villanova1920.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n958139\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova1920)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova1920 &lt;- nova1920 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n454339\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova1920 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova1920.csv\", row.names = FALSE)\n\n\n\n\n\nreddit\n\n\nnewsapi\nlet’s clean this using python:  here is a picture of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n#reading in the file\nnewsapi = pd.read_csv('./data/raw_data/newsapi.csv')\n\n\n\n\nCode\n# what is the shape of this data\nnewsapi.shape\n\n\n(100, 4)\n\n\n\n\nCode\n# what are the column names\nnewsapi.columns\n\n\nIndex(['0', '1', '2', '3'], dtype='object')\n\n\n\n\nCode\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n\n\nTrue\n\n\n\n\nCode\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnewsapi['cleaned_text'] = newsapi['2'].apply(preprocess_text)\n\n\n\n\nCode\n# Initialize the CountVectorizer\ncount_vectorizer = CountVectorizer()\n\n# Fit and transform the preprocessed text data\nX = count_vectorizer.fit_transform(newsapi['cleaned_text'])\n\n# printing part of the sparse matrix\nprint(X[:20, :20].toarray())\n\n\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]]\n\n\n\n\nCode\n# Convert the sparse matrix to a Pandas DataFrame\nX_df = pd.DataFrame(X.toarray())\n\n# Display the first few rows of the DataFrame\nprint(X_df.head())\n\n\n   0    1    2    3    4    5    6    7    8    9    ...  539  540  541  542  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   543  544  545  546  547  548  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 549 columns]\n\n\n\n\nCode\nvocab = count_vectorizer.get_feature_names_out()\n\nprint(vocab)\n\n\n['abnormality' 'abound' 'ac' 'acc' 'action' 'advice' 'aew' 'ahead' 'al'\n 'alds' 'alert' 'already' 'alyssa' 'amid' 'andy' 'angelos' 'another'\n 'answer' 'apple' 'argentina' 'arizona' 'arkansas' 'armondo' 'arsenal'\n 'astonishing' 'austin' 'back' 'balogun' 'ban' 'baseball' 'battle' 'bear'\n 'bearcat' 'beat' 'beaten' 'become' 'becoming' 'behind' 'belief'\n 'bellingham' 'bengal' 'besides' 'best' 'bet' 'better' 'beyond' 'big'\n 'biggs' 'bird' 'blount' 'blue' 'blunder' 'bonus' 'booing' 'booking'\n 'boston' 'bottom' 'brad' 'brain' 'brave' 'breaking' 'breanna' 'brewer'\n 'buccaneer' 'building' 'bukayo' 'bumper' 'call' 'candidate' 'cant' 'card'\n 'cardinal' 'cargill' 'carlos' 'case' 'casino' 'catch' 'central' 'cfb'\n 'champion' 'charge' 'chicago' 'christian' 'cincinnati' 'city' 'climate'\n 'clinch' 'close' 'closer' 'clue' 'clutch' 'coach' 'coaching'\n 'cockeysville' 'college' 'colorado' 'column' 'come' 'coming' 'commander'\n 'commits' 'comparison' 'complete' 'concern' 'conference' 'contender'\n 'corner' 'could' 'cover' 'coverage' 'cowboy' 'cpa' 'craziness' 'crazy'\n 'crop' 'crowd' 'cub' 'cup' 'dalton' 'dame' 'daniel' 'david' 'debut'\n 'defense' 'deion' 'delight' 'deserved' 'desmond' 'desperate' 'despite'\n 'division' 'dolphin' 'doubleheader' 'draw' 'driven' 'drought' 'duck'\n 'duke' 'dy' 'eagle' 'earns' 'eberflus' 'edge' 'emerges' 'end' 'enter'\n 'episode' 'er' 'europe' 'evaluation' 'even' 'event' 'ever' 'everywhere'\n 'evokes' 'excited' 'exit' 'expert' 'eye' 'fact' 'fade' 'falcon' 'famu'\n 'fantasy' 'father' 'favored' 'favorite' 'fiba' 'field' 'fighter'\n 'fighting' 'final' 'fire' 'fired' 'first' 'five' 'fix' 'fiziev' 'fizzle'\n 'florida' 'flyweight' 'focus' 'folarin' 'football' 'force' 'form' 'found'\n 'franklin' 'game' 'gamrot' 'gen' 'giant' 'goal' 'god' 'goff' 'going'\n 'good' 'got' 'grade' 'grasso' 'guez' 'ham' 'hardy' 'harsh' 'head' 'heat'\n 'heavily' 'here' 'hero' 'highlight' 'hit' 'home' 'honor' 'hot' 'huge'\n 'hurricane' 'hype' 'image' 'infamous' 'injures' 'issue' 'ja' 'jack'\n 'jade' 'jay' 'jet' 'jimmy' 'john' 'join' 'jones' 'journey' 'jr' 'jude'\n 'julio' 'justin' 'kansa' 'keep' 'kelce' 'key' 'king' 'knee' 'knockout'\n 'larger' 'laugh' 'leaf' 'league' 'learned' 'life' 'line' 'lion' 'lionel'\n 'live' 'livestream' 'locked' 'lofty' 'logan' 'look' 'loose' 'loser'\n 'loses' 'losing' 'loss' 'lsu' 'lucas' 'madrid' 'mailbag' 'main' 'make'\n 'makeover' 'man' 'manager' 'manchester' 'marquee' 'maryland' 'match'\n 'mateusz' 'matt' 'matter' 'mature' 'meeting' 'megan' 'messi' 'miami'\n 'middleweight' 'milan' 'mlb' 'mojo' 'moment' 'monday' 'morning' 'move'\n 'must' 'mvp' 'nailed' 'napoli' 'nbas' 'near' 'network' 'new' 'newcastle'\n 'nfl' 'night' 'noche' 'normal' 'notify' 'notre' 'nowhere' 'number' 'nwsl'\n 'nycs' 'nz' 'odds' 'ode' 'offense' 'ohio' 'oklahoma' 'onuachu' 'opponent'\n 'option' 'oregon' 'oriole' 'padre' 'panther' 'pass' 'pat' 'patriot'\n 'penn' 'penultimate' 'perfect' 'peter' 'phillies' 'philly' 'pick'\n 'pirate' 'play' 'player' 'playing' 'playoff' 'plenty' 'plus' 'point'\n 'politics' 'pool' 'pound' 'power' 'prediction' 'pretender' 'preview'\n 'previewing' 'problem' 'prop' 'provides' 'pulisic' 'purdy' 'question'\n 'race' 'racer' 'racing' 'rafael' 'ram' 'ranger' 'ranking' 'rapinoe' 'ray'\n 'read' 'real' 'recap' 'record' 'recruit' 'red' 'regular' 'reign' 'relish'\n 'remain' 'removed' 'report' 'resident' 'retro' 'return' 'review' 'revved'\n 'ridder' 'ripe' 'rise' 'river' 'rock' 'rodon' 'rodr' 'rome' 'ross'\n 'rumor' 'run' 'running' 'ryder' 'saka' 'sander' 'say' 'schedule' 'score'\n 'sean' 'search' 'season' 'seat' 'sec' 'secure' 'sends' 'sept' 'series'\n 'served' 'shadow' 'shanahan' 'shevchenko' 'shift' 'show' 'showdown'\n 'since' 'skid' 'slate' 'smack' 'snap' 'sold' 'someone' 'sooner' 'sox'\n 'special' 'speed' 'spiro' 'sport' 'spot' 'spotlight' 'spread' 'st'\n 'stamp' 'stand' 'star' 'start' 'starting' 'state' 'station' 'stats'\n 'stay' 'stellar' 'stewart' 'stoppage' 'straight' 'strategy' 'streak'\n 'stream' 'strickland' 'struggling' 'success' 'summer' 'survivor'\n 'suwinski' 'suzuki' 'sv' 'sweep' 'swift' 'tailspin' 'take' 'takeaway'\n 'taking' 'tale' 'talking' 'tape' 'taylor' 'tcu' 'team' 'ten' 'test'\n 'texas' 'theater' 'thing' 'think' 'thirteen' 'thomas' 'thought' 'three'\n 'thriller' 'thursday' 'tight' 'time' 'title' 'tko' 'tnf' 'today' 'tom'\n 'top' 'total' 'tournament' 'trail' 'transformation' 'travis' 'trea'\n 'trojan' 'troy' 'trust' 'trying' 'tssaa' 'turkey' 'turner' 'tv' 'twin'\n 'two' 'ucf' 'ufc' 'unbeatable' 'unbeaten' 'united' 'unsung' 'upset'\n 'upside' 'usc' 'usmnt' 'uwf' 'vega' 'veloudos' 'video' 'view' 'vlad'\n 'wagon' 'warm' 'watch' 'watching' 'week' 'weekend' 'welcome' 'well'\n 'went' 'white' 'whitner' 'wild' 'wildcat' 'williams' 'wilson' 'win'\n 'winner' 'winning' 'wnba' 'woman' 'world' 'wr' 'wrong' 'wwe' 'yahoo'\n 'yankee' 'year' 'yet' 'youth']\n\n\n\n\nindividual player data\nlet’s clean the aaron judge game data with python:\nhere is a screen shot of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\n#reading in the file\naaronjudge = pd.read_csv('./data/raw_data/AaronJudgeData.csv')\n\n\n\n\nCode\n#how many rows are in this dataset?\naaronjudge.shape\n\n\n(111, 37)\n\n\n\n\nCode\n#what are the column names?\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1',\n       'Events', 'EV.1', 'maxEV', 'LA', 'Barrels', 'Barrel%', 'HardHit',\n       'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n#removing the repeated columns\ncolumns_to_remove = ['Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1']\naaronjudge.drop(columns=columns_to_remove, inplace=True)\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Events', 'EV.1', 'maxEV', 'LA', 'Barrels',\n       'Barrel%', 'HardHit', 'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n# i belive the initial row with the column names is repeated throughou the data. let's check\nprint((aaronjudge['Date'] == 'Date').sum())\n\n\n5\n\n\n\n\nCode\n# let's remove these rows and then check the shape again\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Date'].index, inplace=True)\naaronjudge.shape\n\n\n(106, 32)\n\n\n\n\nCode\n# there is also a total row which I want to remove as well. let's do that now\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Total'].index, inplace=True)\naaronjudge.shape\n\n\n(105, 32)\n\n\n\n\nCode\n# so far, I have removed 6 rows and 5 columns. \n\n# I want to create a \"location\" column based on the \"@\" in the \"Opp\" column\naaronjudge['location'] = aaronjudge['Opp'].apply(lambda x: 'away' if '@' in x else 'home')\n\n# Remove the \"@\" symbol from the values in the \"Opp\" column\naaronjudge['Opp'] = aaronjudge['Opp'].str.replace('@', '')\n\n# check value counts of the new \"location\" column\nprint(aaronjudge['location'].value_counts()) #this seems accurate\n\n\nhome    53\naway    52\nName: location, dtype: int64\n\n\n\n\nCode\nprint(aaronjudge['PA'].dtype)\nprint(aaronjudge['BB%'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# I want to create two new columns. The number of at bats per each game and the number of hard hits in each game. \n# for this project, we are going to calculate at-bats as should be the number of plate appearances minus walks (sacrifices and HBP are not included in this dataset)\n\n#first i have to remove the '%' symbol and convert 'BB%' to a float\n\naaronjudge['BB%'] = aaronjudge['BB%'].astype(str)\naaronjudge['BB%'] = aaronjudge['BB%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'BB%' column to three decimal places\naaronjudge['BB%'] = aaronjudge['BB%'].round(3)\n\n#print(aaronjudge['BB%'].mean())\n\n#convert 'PA' to a float\naaronjudge['PA'] = aaronjudge['PA'].astype(float)\n\n# now I can create the new at_bats column\naaronjudge['at_bats'] = aaronjudge['PA'] * (1 - aaronjudge['BB%'])\n\n#now lets see the average number of at bats vs the average number of plate appearances\nprint(aaronjudge['at_bats'].mean())\nprint(aaronjudge['PA'].mean())\n\n\n3.4857333333333336\n4.314285714285714\n\n\n\n\nCode\n# now I want to create a new column for hard hits per game\n# we can do this by multiplying the hard hit percentage by the events column (these columns were part of a different table that was merged with the original table)\n\nprint(aaronjudge['HardHit%'].dtype)\nprint(aaronjudge['Events'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# this code is very similar to what we just did\n\n#first i have to remove the '%' symbol and convert 'HardHit%' to a float\n\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].astype(str)\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'HardHit%' column to three decimal places\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].round(3)\n\n#print(aaronjudge['HardHit%'].mean())\n\n#convert 'Events' to a float\naaronjudge['Events'] = aaronjudge['Events'].astype(float)\n\n# now I can create the new hard_hits column\naaronjudge['hard_hits'] = (aaronjudge['Events'] * aaronjudge['HardHit%']).round(0)\n\n#now lets see the average number of hard_hits per game\nprint(aaronjudge['hard_hits'].mean())\n\n\n1.52\n\n\n\n\nCode\n# finally, let's create a correct hardhit% column that is based on the number of at-bats, not the number of times a player puts the ball in play\naaronjudge['correct_hardhit%'] = (aaronjudge['hard_hits'] / aaronjudge['at_bats']).round(2)\n\n# now let's see the average correct hardhit% for Aaron Judge\nprint(aaronjudge['correct_hardhit%'].mean())\n\n\n0.42829999999999996\n\n\n\n\nCode\n# sometimes in certain stadiums or based on the weather, the HardHit% data is missing\n# this causes the value of the new correct_hardhit% column to be NaN, so let's remove those few rows\naaronjudge.dropna(subset=['correct_hardhit%'], inplace=True)\n\n#let's check the shape again\naaronjudge.shape #loss of 5 rows\n\n\n(100, 36)\n\n\n\n\nCode\n# now we can save this to a csv file\naaronjudge.to_csv('./data/modified_data/aaronjudge.csv', index=False)\n\n\nhere is a screenshot of the first couple rows of the modified csv file:  \n\n\nExtra Joke\nWhat did the broom say to the vacuum?\n“I’m so tired of people pushing us around.”"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "three more to go!"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "ipsum lorem"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress \n\nnote to self\ndata types descriptive statistics - mean, mode, median, std dev, -&gt; categorical data uses frequency distributions and bar charts make graphs! histograms, box plots, scatter plots, heatmaps correlation analysis - correlation matrices, heat maps, scatterplots hypothesis generation - refine hypotheses and research questions identify outliers report and discuss findings!!!!\n\n\nncaahoopR\n\n2021-22 season\n\nnova2122 &lt;- read.csv('./data/modified_data/nova2122.csv')\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova2122 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\nA tibble: 12 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nJustin Moore\n574\n0.4721254\n\n\nCollin Gillespie\n549\n0.5336976\n\n\nJermaine Samuels\n439\n0.5535308\n\n\nCaleb Daniels\n356\n0.5056180\n\n\nEric Dixon\n340\n0.5794118\n\n\nBrandon Slater\n308\n0.5876623\n\n\nChris Arcidiacono\n69\n0.4927536\n\n\nJordan Longino\n57\n0.4210526\n\n\nBryan Antoine\n46\n0.3043478\n\n\nTrey Patterson\n12\n0.3333333\n\n\nDhamir Cosby-Roundtree\n8\n0.5000000\n\n\nNnanna Njoku\n6\n0.5000000\n\n\n\n\n\n\n\n\n\n# Create lag variables within each shooter and game_id group\nnova2122 &lt;- nova2122 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\nwrite.csv(nova2122, file = \"./data/modified_data/nova2122_updated.csv\", row.names = FALSE)\n\n# View the updated data with lag variables\nhead(nova2122)\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401365747\n4\n1\nJustin Moore\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n7\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n11\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n13\n1\nEric Dixon\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n16\n1\nCollin Gillespie\nmade\nVillanova\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n18\n1\nEric Dixon\nmade\nVillanova\n1\n1\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova2122[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n2019-20 season\n\nnova1920 &lt;- read.csv('./data/modified_data/nova1920.csv')\n\n\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova1920 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\nA tibble: 10 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nCollin Gillespie\n491\n0.4969450\n\n\nSaddiq Bey\n458\n0.5349345\n\n\nJustin Moore\n355\n0.4647887\n\n\nJeremiah Robinson-Earl\n347\n0.5533141\n\n\nJermaine Samuels\n334\n0.5419162\n\n\nCole Swider\n171\n0.4561404\n\n\nBrandon Slater\n68\n0.3823529\n\n\nDhamir Cosby-Roundtree\n36\n0.6666667\n\n\nBryan Antoine\n25\n0.3600000\n\n\nChris Arcidiacono\n6\n0.1666667\n\n\n\n\n\n\n\n\n\n# Create lag variables within each shooter and game_id group\nnova1920 &lt;- nova1920 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\n# View the updated data with lag variables\nhead(nova1920)\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401166061\n2\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n4\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n6\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n8\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n2\n1\n1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n9\n1\nCollin Gillespie\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n11\n1\nCJ Walker\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova1920[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n\n\n\n\n\n\n# i want to create a histogram of games per season with shots for each player and made shots for each player\n\n# i want to do something with my lag variable\n\n\n\n\nText Data - Reddit or News APT\n\n\nIndividual Player Data\n\n#let's import some libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\naaronjudge = pd.read_csv('./data/modified_data/aaronjudge.csv')\n\n\naaronjudge.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 36 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              100 non-null    object \n 1   Team              100 non-null    object \n 2   Opp               100 non-null    object \n 3   BO                100 non-null    int64  \n 4   Pos               100 non-null    object \n 5   PA                100 non-null    float64\n 6   H                 100 non-null    int64  \n 7   2B                100 non-null    int64  \n 8   3B                100 non-null    int64  \n 9   HR                100 non-null    int64  \n 10  R                 100 non-null    int64  \n 11  RBI               100 non-null    int64  \n 12  SB                100 non-null    int64  \n 13  CS                100 non-null    int64  \n 14  BB%               100 non-null    float64\n 15  K%                100 non-null    object \n 16  ISO               100 non-null    float64\n 17  BABIP             100 non-null    float64\n 18  EV                100 non-null    float64\n 19  AVG               100 non-null    float64\n 20  OBP               100 non-null    float64\n 21  SLG               100 non-null    float64\n 22  wOBA              100 non-null    float64\n 23  wRC+              100 non-null    int64  \n 24  Events            100 non-null    float64\n 25  EV.1              100 non-null    float64\n 26  maxEV             100 non-null    float64\n 27  LA                100 non-null    float64\n 28  Barrels           100 non-null    int64  \n 29  Barrel%           100 non-null    object \n 30  HardHit           100 non-null    int64  \n 31  HardHit%          100 non-null    float64\n 32  location          100 non-null    object \n 33  at_bats           100 non-null    float64\n 34  hard_hits         100 non-null    float64\n 35  correct_hardhit%  100 non-null    float64\ndtypes: float64(17), int64(12), object(7)\nmemory usage: 28.2+ KB\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming aaronjudge is your DataFrame\n# Create a pivot table to count the observations\npivot_table = aaronjudge.pivot_table(index='hard_hits', columns='H', aggfunc='size', fill_value=0)\n\n# Create the heatmap\nax = sns.heatmap(pivot_table, cmap=\"Blues\", annot=True, fmt=\"d\")\n\n# Customize the y-axis to start at 0 and increase as you go up\nax.set_yticklabels(ax.get_yticklabels(), rotation=0)\nax.invert_yaxis()\n\n# Customize the plot if needed\nplt.title(\"Heat Map Showing Hits v. Hard Hits\")\nplt.xlabel(\"Hits\")\nplt.ylabel(\"hard_hits\")\n\nplt.show()\n\n\n\n\n\n# Sort the DataFrame by Date in ascending order\naaronjudge = aaronjudge.sort_values(by='Date')\n\n# Create subplots with 2 rows and 1 column\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\n\n# First subplot - correct_hardhit%\nsns.barplot(data=aaronjudge, y='correct_hardhit%', x='Date', ax=axes[0])\naxes[0].set_title(\"correct_hardhit%\")\naxes[0].set_xlabel(\"Date\")\naxes[0].set_ylabel(\"correct_hardhit%\")\n# Get the x-axis tick positions\nx_ticks = axes[0].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[0].set_xticks(visible_ticks)\n\n# Second subplot - H\nsns.barplot(data=aaronjudge, y='H', x='Date', ax=axes[1])\naxes[1].set_title(\"H\")\naxes[1].set_xlabel(\"Date\")\naxes[1].set_ylabel(\"H\")\n# Get the x-axis tick positions\nx_ticks = axes[1].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[1].set_xticks(visible_ticks)\n\n# Adjust the layout to avoid overlap\nplt.tight_layout()\n\n# Show the combined figure\nplt.show()\n\n\n\n\n\n\nHypothesis Refinement\nyoda data master!"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  This page will take you through the data sources and methodologies employed in this specific project. Furthermore, you can find brief descriptions/images/tables of the various datasets mentioned. Data must be acquired using at least one Python API and one R API. This project will use various data formats that may include labeled data, qualitative data, text data, geo data, record-data, etc.\n\nBaseballr\n“Baseballr” is a package in R that focuses on baseball analytics, also known as sabremetrics. It includes various functions that can be used for scraping data from websites like FanGraphs.com, Baseball-Reference.com, and BaseballSavant.mlb.com. It also includes functions for calculating specific baseball metrics such as wOBA (weighted on-base average) and FIP (fielding independent pitching). I will mainly use this package to gather data (which uses an API as can be seen below).\n\nSource Code\nThe below source code was pulled from the baseballr github repository. This specific code uses a mlb api to acquire play-by-play data for a specific game. I will use these functions later on through the baseballr package.\n\n\nCode\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\nmlb_api_call &lt;- function(url){\n  res &lt;-\n    httr::RETRY(\"GET\", url)\n  \n  json &lt;- res$content %&gt;%\n    rawToChar() %&gt;%\n    jsonlite::fromJSON(simplifyVector = T)\n  \n  return(json)\n}\n\nmlb_stats_endpoint &lt;- function(endpoint){\n  all_endpoints = c(\n    \"v1/attendance\",#\n    \"v1/conferences\",#\n    \"v1/conferences/{conferenceId}\",#\n    \"v1/awards/{awardId}/recipients\",#\n    \"v1/awards\",#\n    \"v1/baseballStats\",#\n    \"v1/eventTypes\",#\n    \"v1/fielderDetailTypes\",#\n    \"v1/gameStatus\",#\n    \"v1/gameTypes\",#\n    \"v1/highLow/types\",#\n    \"v1/hitTrajectories\",#\n    \"v1/jobTypes\",#\n    \"v1/languages\",\n    \"v1/leagueLeaderTypes\",#\n    \"v1/logicalEvents\",#\n    \"v1/metrics\",#\n    \"v1/pitchCodes\",#\n    \"v1/pitchTypes\",#\n    \"v1/playerStatusCodes\",#\n    \"v1/positions\",#\n    \"v1/reviewReasons\",#\n    \"v1/rosterTypes\",#\n    \"v1/runnerDetailTypes\",#\n    \"v1/scheduleEventTypes\",#\n    \"v1/situationCodes\",#\n    \"v1/sky\",#\n    \"v1/standingsTypes\",#\n    \"v1/statGroups\",#\n    \"v1/statTypes\",#\n    \"v1/windDirection\",#\n    \"v1/divisions\",#\n    \"v1/draft/{year}\",#\n    \"v1/draft/prospects/{year}\",#\n    \"v1/draft/{year}/latest\",#\n    \"v1.1/game/{gamePk}/feed/live\",\n    \"v1.1/game/{gamePk}/feed/live/diffPatch\",#\n    \"v1.1/game/{gamePk}/feed/live/timestamps\",#\n    \"v1/game/changes\",##x\n    \"v1/game/analytics/game\",##x\n    \"v1/game/analytics/guids\",##x\n    \"v1/game/{gamePk}/guids\",##x\n    \"v1/game/{gamePk}/{GUID}/analytics\",##x\n    \"v1/game/{gamePk}/{GUID}/contextMetricsAverages\",##x\n    \"v1/game/{gamePk}/contextMetrics\",#\n    \"v1/game/{gamePk}/winProbability\",#\n    \"v1/game/{gamePk}/boxscore\",#\n    \"v1/game/{gamePk}/content\",#\n    \"v1/game/{gamePk}/feed/color\",##x\n    \"v1/game/{gamePk}/feed/color/diffPatch\",##x\n    \"v1/game/{gamePk}/feed/color/timestamps\",##x\n    \"v1/game/{gamePk}/linescore\",#\n    \"v1/game/{gamePk}/playByPlay\",#\n    \"v1/gamePace\",#\n    \"v1/highLow/{orgType}\",#\n    \"v1/homeRunDerby/{gamePk}\",#\n    \"v1/homeRunDerby/{gamePk}/bracket\",#\n    \"v1/homeRunDerby/{gamePk}/pool\",#\n    \"v1/league\",#\n    \"v1/league/{leagueId}/allStarBallot\",#\n    \"v1/league/{leagueId}/allStarWriteIns\",#\n    \"v1/league/{leagueId}/allStarFinalVote\",#\n    \"v1/people\",#\n    \"v1/people/freeAgents\",#\n    \"v1/people/{personId}\",##U\n    \"v1/people/{personId}/stats/game/{gamePk}\",#\n    \"v1/people/{personId}/stats/game/current\",#\n    \"v1/jobs\",#\n    \"v1/jobs/umpires\",#\n    \"v1/jobs/datacasters\",#\n    \"v1/jobs/officialScorers\",#\n    \"v1/jobs/umpires/games/{umpireId}\",##x\n    \"v1/schedule/\",#\n    \"v1/schedule/games/tied\",#\n    \"v1/schedule/postseason\",#\n    \"v1/schedule/postseason/series\",#\n    \"v1/schedule/postseason/tuneIn\",##x\n    \"v1/seasons\",#\n    \"v1/seasons/all\",#\n    \"v1/seasons/{seasonId}\",#\n    \"v1/sports\",#\n    \"v1/sports/{sportId}\",#\n    \"v1/sports/{sportId}/players\",#\n    \"v1/standings\",#\n    \"v1/stats\",#\n    \"v1/stats/metrics\",##x\n    \"v1/stats/leaders\",#\n    \"v1/stats/streaks\",##404\n    \"v1/teams\",#\n    \"v1/teams/history\",#\n    \"v1/teams/stats\",#\n    \"v1/teams/stats/leaders\",#\n    \"v1/teams/affiliates\",#\n    \"v1/teams/{teamId}\",#\n    \"v1/teams/{teamId}/stats\",#\n    \"v1/teams/{teamId}/affiliates\",#\n    \"v1/teams/{teamId}/alumni\",#\n    \"v1/teams/{teamId}/coaches\",#\n    \"v1/teams/{teamId}/personnel\",#\n    \"v1/teams/{teamId}/leaders\",#\n    \"v1/teams/{teamId}/roster\",##x\n    \"v1/teams/{teamId}/roster/{rosterType}\",#\n    \"v1/venues\"#\n  )\n  base_url = glue::glue('http://statsapi.mlb.com/api/{endpoint}')\n  return(base_url)\n}\n\n\n\n\n\nCode\nx &lt;- \"http://statsapi.mlb.com/api/v1/game/575156/playByPlay\"\n\noutput &lt;- mlb_api_call(x)\n\n\n“output” is a very messy list that is extremely long. Instead of printing “output”, below are three images of part of the list.\n  \nThe below code builds on the previous code, returning a tibble that includes over 100 columns of data provided by the MLB Stats API at a pitch level. As you will see, the output is much cleaner and easier to work with.\n\n\nCode\n#' @rdname mlb_pbp\n#' @title **Acquire pitch-by-pitch data for Major and Minor League games**\n#'\n#' @param game_pk The date for which you want to find game_pk values for MLB games\n#' @importFrom jsonlite fromJSON\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#'\n#' Some data will vary depending on the\n#' park and the league level, as most sensor data is not available in\n#' minor league parks via this API. Note that the column names have mostly\n#' been left as-is and there are likely duplicate columns in terms of the\n#' information they provide. I plan to clean the output up down the road, but\n#' for now I am leaving the majority as-is.\n#'\n#' Both major and minor league pitch-by-pitch data can be pulled with this function.\n#' \n#'  |col_name                       |types     |\n#'  |:------------------------------|:---------|\n#'  |game_pk                        |numeric   |\n#'  |game_date                      |character |\n#'  |index                          |integer   |\n#'  |startTime                      |character |\n#'  |endTime                        |character |\n#'  |isPitch                        |logical   |\n#'  |type                           |character |\n#'  |playId                         |character |\n#'  |pitchNumber                    |integer   |\n#'  |details.description            |character |\n#'  |details.event                  |character |\n#'  |details.awayScore              |integer   |\n#'  |details.homeScore              |integer   |\n#'  |details.isScoringPlay          |logical   |\n#'  |details.hasReview              |logical   |\n#'  |details.code                   |character |\n#'  |details.ballColor              |character |\n#'  |details.isInPlay               |logical   |\n#'  |details.isStrike               |logical   |\n#'  |details.isBall                 |logical   |\n#'  |details.call.code              |character |\n#'  |details.call.description       |character |\n#'  |count.balls.start              |integer   |\n#'  |count.strikes.start            |integer   |\n#'  |count.outs.start               |integer   |\n#'  |player.id                      |integer   |\n#'  |player.link                    |character |\n#'  |pitchData.strikeZoneTop        |numeric   |\n#'  |pitchData.strikeZoneBottom     |numeric   |\n#'  |details.fromCatcher            |logical   |\n#'  |pitchData.coordinates.x        |numeric   |\n#'  |pitchData.coordinates.y        |numeric   |\n#'  |hitData.trajectory             |character |\n#'  |hitData.hardness               |character |\n#'  |hitData.location               |character |\n#'  |hitData.coordinates.coordX     |numeric   |\n#'  |hitData.coordinates.coordY     |numeric   |\n#'  |actionPlayId                   |character |\n#'  |details.eventType              |character |\n#'  |details.runnerGoing            |logical   |\n#'  |position.code                  |character |\n#'  |position.name                  |character |\n#'  |position.type                  |character |\n#'  |position.abbreviation          |character |\n#'  |battingOrder                   |character |\n#'  |atBatIndex                     |character |\n#'  |result.type                    |character |\n#'  |result.event                   |character |\n#'  |result.eventType               |character |\n#'  |result.description             |character |\n#'  |result.rbi                     |integer   |\n#'  |result.awayScore               |integer   |\n#'  |result.homeScore               |integer   |\n#'  |about.atBatIndex               |integer   |\n#'  |about.halfInning               |character |\n#'  |about.inning                   |integer   |\n#'  |about.startTime                |character |\n#'  |about.endTime                  |character |\n#'  |about.isComplete               |logical   |\n#'  |about.isScoringPlay            |logical   |\n#'  |about.hasReview                |logical   |\n#'  |about.hasOut                   |logical   |\n#'  |about.captivatingIndex         |integer   |\n#'  |count.balls.end                |integer   |\n#'  |count.strikes.end              |integer   |\n#'  |count.outs.end                 |integer   |\n#'  |matchup.batter.id              |integer   |\n#'  |matchup.batter.fullName        |character |\n#'  |matchup.batter.link            |character |\n#'  |matchup.batSide.code           |character |\n#'  |matchup.batSide.description    |character |\n#'  |matchup.pitcher.id             |integer   |\n#'  |matchup.pitcher.fullName       |character |\n#'  |matchup.pitcher.link           |character |\n#'  |matchup.pitchHand.code         |character |\n#'  |matchup.pitchHand.description  |character |\n#'  |matchup.splits.batter          |character |\n#'  |matchup.splits.pitcher         |character |\n#'  |matchup.splits.menOnBase       |character |\n#'  |batted.ball.result             |factor    |\n#'  |home_team                      |character |\n#'  |home_level_id                  |integer   |\n#'  |home_level_name                |character |\n#'  |home_parentOrg_id              |integer   |\n#'  |home_parentOrg_name            |character |\n#'  |home_league_id                 |integer   |\n#'  |home_league_name               |character |\n#'  |away_team                      |character |\n#'  |away_level_id                  |integer   |\n#'  |away_level_name                |character |\n#'  |away_parentOrg_id              |integer   |\n#'  |away_parentOrg_name            |character |\n#'  |away_league_id                 |integer   |\n#'  |away_league_name               |character |\n#'  |batting_team                   |character |\n#'  |fielding_team                  |character |\n#'  |last.pitch.of.ab               |character |\n#'  |pfxId                          |character |\n#'  |details.trailColor             |character |\n#'  |details.type.code              |character |\n#'  |details.type.description       |character |\n#'  |pitchData.startSpeed           |numeric   |\n#'  |pitchData.endSpeed             |numeric   |\n#'  |pitchData.zone                 |integer   |\n#'  |pitchData.typeConfidence       |numeric   |\n#'  |pitchData.plateTime            |numeric   |\n#'  |pitchData.extension            |numeric   |\n#'  |pitchData.coordinates.aY       |numeric   |\n#'  |pitchData.coordinates.aZ       |numeric   |\n#'  |pitchData.coordinates.pfxX     |numeric   |\n#'  |pitchData.coordinates.pfxZ     |numeric   |\n#'  |pitchData.coordinates.pX       |numeric   |\n#'  |pitchData.coordinates.pZ       |numeric   |\n#'  |pitchData.coordinates.vX0      |numeric   |\n#'  |pitchData.coordinates.vY0      |numeric   |\n#'  |pitchData.coordinates.vZ0      |numeric   |\n#'  |pitchData.coordinates.x0       |numeric   |\n#'  |pitchData.coordinates.y0       |numeric   |\n#'  |pitchData.coordinates.z0       |numeric   |\n#'  |pitchData.coordinates.aX       |numeric   |\n#'  |pitchData.breaks.breakAngle    |numeric   |\n#'  |pitchData.breaks.breakLength   |numeric   |\n#'  |pitchData.breaks.breakY        |numeric   |\n#'  |pitchData.breaks.spinRate      |integer   |\n#'  |pitchData.breaks.spinDirection |integer   |\n#'  |hitData.launchSpeed            |numeric   |\n#'  |hitData.launchAngle            |numeric   |\n#'  |hitData.totalDistance          |numeric   |\n#'  |injuryType                     |character |\n#'  |umpire.id                      |integer   |\n#'  |umpire.link                    |character |\n#'  |isBaseRunningPlay              |logical   |\n#'  |isSubstitution                 |logical   |\n#'  |about.isTopInning              |logical   |\n#'  |matchup.postOnFirst.id         |integer   |\n#'  |matchup.postOnFirst.fullName   |character |\n#'  |matchup.postOnFirst.link       |character |\n#'  |matchup.postOnSecond.id        |integer   |\n#'  |matchup.postOnSecond.fullName  |character |\n#'  |matchup.postOnSecond.link      |character |\n#'  |matchup.postOnThird.id         |integer   |\n#'  |matchup.postOnThird.fullName   |character |\n#'  |matchup.postOnThird.link       |character |\n#' @export\n#' @examples \\donttest{\n#'   try(mlb_pbp(game_pk = 632970))\n#' }\n\nmlb_pbp &lt;- function(game_pk) {\n  \n  mlb_endpoint &lt;- mlb_stats_endpoint(glue::glue(\"v1.1/game/{game_pk}/feed/live\"))\n  \n  tryCatch(\n    expr = {\n      payload &lt;- mlb_endpoint %&gt;% \n        mlb_api_call() %&gt;% \n        jsonlite::toJSON() %&gt;% \n        jsonlite::fromJSON(flatten = TRUE)\n      \n      plays &lt;- payload$liveData$plays$allPlays$playEvents %&gt;% \n        dplyr::bind_rows()\n      \n      at_bats &lt;- payload$liveData$plays$allPlays\n      \n      current &lt;- payload$liveData$plays$currentPlay\n      \n      game_status &lt;- payload$gameData$status$abstractGameState\n      \n      home_team &lt;- payload$gameData$teams$home$name\n      \n      home_level &lt;- payload$gameData$teams$home$sport\n      \n      home_league &lt;- payload$gameData$teams$home$league\n      \n      away_team &lt;- payload$gameData$teams$away$name\n      \n      away_level &lt;- payload$gameData$teams$away$sport\n      \n      away_league &lt;- payload$gameData$teams$away$league\n      \n      columns &lt;- lapply(at_bats, function(x) class(x)) %&gt;%\n        dplyr::bind_rows(.id = \"variable\")\n      cols &lt;- c(colnames(columns))\n      classes &lt;- c(t(unname(columns[1,])))\n      \n      df &lt;- data.frame(cols, classes)\n      list_columns &lt;- df %&gt;%\n        dplyr::filter(.data$classes == \"list\") %&gt;%\n        dplyr::pull(\"cols\")\n      \n      at_bats &lt;- at_bats %&gt;%\n        dplyr::select(-c(tidyr::one_of(list_columns)))\n      \n      pbp &lt;- plays %&gt;%\n        dplyr::left_join(at_bats, by = c(\"endTime\" = \"playEndTime\"))\n      \n      pbp &lt;- pbp %&gt;%\n        tidyr::fill(\"atBatIndex\":\"matchup.splits.menOnBase\", .direction = \"up\") %&gt;%\n        dplyr::mutate(\n          game_pk = game_pk,\n          game_date = substr(payload$gameData$datetime$dateTime, 1, 10)) %&gt;%\n        dplyr::select(\"game_pk\", \"game_date\", tidyr::everything())\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::mutate(\n          matchup.batter.fullName = factor(.data$matchup.batter.fullName),\n          matchup.pitcher.fullName = factor(.data$matchup.pitcher.fullName),\n          atBatIndex = factor(.data$atBatIndex)\n          # batted.ball.result = case_when(!result.event %in% c(\n          #   \"Single\", \"Double\", \"Triple\", \"Home Run\") ~ \"Out/Other\",\n          #   TRUE ~ result.event),\n          # batted.ball.result = factor(batted.ball.result,\n          #                             levels = c(\"Single\", \"Double\", \"Triple\", \"Home Run\", \"Out/Other\"))\n        ) %&gt;%\n        dplyr::mutate(\n          home_team = home_team,\n          home_level_id = home_level$id,\n          home_level_name = home_level$name,\n          home_parentOrg_id = payload$gameData$teams$home$parentOrgId,\n          home_parentOrg_name = payload$gameData$teams$home$parentOrgName,\n          home_league_id = home_league$id,\n          home_league_name = home_league$name,\n          away_team = away_team,\n          away_level_id = away_level$id,\n          away_level_name = away_level$name,\n          away_parentOrg_id = payload$gameData$teams$away$parentOrgId,\n          away_parentOrg_name = payload$gameData$teams$away$parentOrgName,\n          away_league_id = away_league$id,\n          away_league_name = away_league$name,\n          batting_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                       .data$home_team,\n                                       .data$away_team)),\n          fielding_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                        .data$away_team,\n                                        .data$home_team)))\n      pbp &lt;- pbp %&gt;%\n        dplyr::arrange(desc(.data$atBatIndex), desc(.data$pitchNumber))\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::group_by(.data$atBatIndex) %&gt;%\n        dplyr::mutate(\n          last.pitch.of.ab =  ifelse(.data$pitchNumber == max(.data$pitchNumber), \"true\", \"false\"),\n          last.pitch.of.ab = factor(.data$last.pitch.of.ab)) %&gt;%\n        dplyr::ungroup()\n      \n      pbp &lt;- dplyr::bind_rows(baseballr::stats_api_live_empty_df, pbp)\n      \n      check_home_level &lt;- pbp %&gt;%\n        dplyr::distinct(.data$home_level_id) %&gt;%\n        dplyr::pull()\n      \n      # this will need to be updated in the future to properly estimate X,Z coordinates at the minor league level\n      \n      # if(check_home_level != 1) {\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.pX_est = predict(x_model, pbp),\n      #                   pitchData.coordinates.pZ_est = predict(y_model, pbp))\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      # }\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::rename(\n          \"count.balls.start\" = \"count.balls.x\",\n          \"count.strikes.start\" = \"count.strikes.x\",\n          \"count.outs.start\" = \"count.outs.x\",\n          \"count.balls.end\" = \"count.balls.y\",\n          \"count.strikes.end\" = \"count.strikes.y\",\n          \"count.outs.end\" = \"count.outs.y\") %&gt;%\n        make_baseballr_data(\"MLB Play-by-Play data from MLB.com\",Sys.time())\n    },\n    error = function(e) {\n      message(glue::glue(\"{Sys.time()}: Invalid arguments provided\"))\n    },\n    finally = {\n    }\n  ) \n  return(pbp)\n}\n\n#' @rdname get_pbp_mlb\n#' @title **(legacy) Acquire pitch-by-pitch data for Major and Minor League games**\n#' @inheritParams mlb_pbp\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#' @keywords legacy\n#' @export\n# get_pbp_mlb &lt;- mlb_pbp\n\n\n\n\nExample\nHere is an example using the mlb_pbp function.\n\n\nCode\nexample &lt;- (mlb_pbp(575156))\nhead(example)\n\n\n2023-10-12 13:40:46.684707: Invalid arguments provided\n\n\n\n\nA tibble: 6 x 146\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nabout.isTopInning\nmatchup.postOnFirst.id\nmatchup.postOnFirst.fullName\nmatchup.postOnFirst.link\nmatchup.postOnSecond.id\nmatchup.postOnSecond.fullName\nmatchup.postOnSecond.link\nmatchup.postOnThird.id\nmatchup.postOnThird.fullName\nmatchup.postOnThird.link\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n575156\n2019-06-01\n5\n2019-06-01T15:38:42.000Z\n2019-06-01T19:38:07.354Z\nTRUE\npitch\n05751566-0846-0063-000c-f08cd117d70a\n6\nIn play, out(s)\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n4\n2019-06-01T15:38:19.000Z\n2019-06-01T15:38:42.000Z\nTRUE\npitch\n05751566-0846-0053-000c-f08cd117d70a\n5\nFoul\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n3\n2019-06-01T15:38:02.000Z\n2019-06-01T15:38:19.000Z\nTRUE\npitch\n05751566-0846-0043-000c-f08cd117d70a\n4\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n2\n2019-06-01T15:37:45.000Z\n2019-06-01T15:38:02.000Z\nTRUE\npitch\n05751566-0846-0033-000c-f08cd117d70a\n3\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n1\n2019-06-01T15:37:31.000Z\n2019-06-01T15:37:45.000Z\nTRUE\npitch\n05751566-0846-0023-000c-f08cd117d70a\n2\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n0\n2019-06-01T15:37:15.000Z\n2019-06-01T15:37:31.000Z\nTRUE\npitch\n05751566-0846-0013-000c-f08cd117d70a\n1\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAcquiring Data\nI will pull more data eventually, but for now I am scraping two series of games from the 2023 season.\n\n\nCode\nlibrary(baseballr)\n\n\nThe below code allows me to find the correct game_pk values that I can then use to pull play-by-play data.\n\n\nCode\n#mlb_game_pks(\"2023-06-25\")\n# mlb_game_pks(\"2023-06-24\")\n# mlb_game_pks(\"2023-06-23\")\n\n\n\n\nCode\n#game_pk values\n\n#diamondbacks/giants - 717641, 717639, 717612\n\n#mariners/orioles - 717651, 717628, 717627\n\n\n\n\nCode\nx &lt;- c(717641, 717639, 717612, 717651, 717628, 717627)\nresult &lt;- lapply(x, mlb_pbp)\ncombined_tibble &lt;- bind_rows(result)\n# Save the data to a CSV file\nwrite.csv(combined_tibble, file = \"./data/raw_data/baseballr_six_games.csv\", row.names = FALSE)\nhead(combined_tibble)\n\n\n\nA baseballr_data: 6 x 160\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nncaahoopR\n“ncaahoopR” is an R package tailored for NCAA Basketball Play-by-Play Data analysis. It excels at retrieving play-by-play data in a tidy format. For the purposes of this project, I will start by scraping play-by-play data for the Villanova Wildcats Men’s Basketball team from both the 2019-20 and 2021-22 seasons (the 2020-21 was shortened due to COVID-19).\n\n\nCode\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lbenz730/ncaahoopR\")\nlibrary(ncaahoopR)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/lb/dk54cbx965z7nj61zps2fzr00000gn/T//RtmpqXinFj/downloaded_packages\n\n\nSkipping install of 'ncaahoopR' from a github remote, the SHA1 (9bd97fec) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n\n\nCode\nVillanova1920 &lt;- get_pbp(\"Villanova\", \"2019-20\")\nVillanova2122 &lt;- get_pbp(\"Villanova\", \"2021-22\")\nwrite.csv(Villanova1920, file = \"./data/raw_data/villanova1920.csv\", row.names = FALSE)\nwrite.csv(Villanova2122, file = \"./data/raw_data/villanova2122.csv\", row.names = FALSE)\n\n\n\n\nCode\nhead(Villanova1920)\n\n\n\nA data.frame: 6 x 39\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\nshot_y\nshot_team\nshot_outcome\nshooter\nassist\nthree_pt\nfree_throw\npossession_before\npossession_after\nwrong_time\n\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\n401169778\n2019-11-05\nVillanova\nArmy\n1\n1\n19:37\n2377\n2377\nSaddiq Bey made Jumper.\n...\nNA\nVillanova\nmade\nSaddiq Bey\nNA\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n2\n401169778\n2019-11-05\nVillanova\nArmy\n2\n1\n19:16\n2356\n2356\nTucker Blackwell made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nTucker Blackwell\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n3\n401169778\n2019-11-05\nVillanova\nArmy\n3\n1\n19:01\n2341\n2341\nFoul on Jermaine Samuels.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n4\n401169778\n2019-11-05\nVillanova\nArmy\n4\n1\n19:01\n2341\n2341\nJermaine Samuels Turnover.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n5\n401169778\n2019-11-05\nVillanova\nArmy\n5\n1\n18:42\n2322\n2322\nMatt Wilson made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nMatt Wilson\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n6\n401169778\n2019-11-05\nVillanova\nArmy\n6\n1\n18:31\n2311\n2311\nJeremiah Robinson-Earl made Jumper. Assisted by Justin Moore.\n...\nNA\nVillanova\nmade\nJeremiah Robinson-Earl\nJustin Moore\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n\n\n\n\n\nReddit\n\n\nCode\n#install.packages(\"RedditExtractoR\") #only executable in Rstudio\nlibrary(RedditExtractoR)\n\n\nsubreddit &lt;- \"baseball\"\n\n# Get posts from the r/baseball subreddit\nstreaks &lt;- find_thread_urls(keywords = \"streak\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nhot &lt;- find_thread_urls(keywords = \"hot\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nwrite.csv(streaks, file = \"./streaks.csv\", row.names = FALSE)\nwrite.csv(hot, file = \"./hot.csv\", row.names = FALSE)\n\n\nBelow you can see the first few rows of both the “hot” and “streaks” csv files.\n \n\n\nNews API\n\n\nCode\nAPI_KEY='05d7ae99b5b7455191c97c2c5c3a1f9b'\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n\n\nCode\n%%capture\n\nTOPIC = 'hot streak sports'\n\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\n#print(\"AVAILABLE KEYS:\")\n#print(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        # if(key=='description'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        # if(key=='content'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\ndf1 = pd.DataFrame(cleaned_data)\ndf1.to_csv('./data/raw_data/newsapi.csv', index=False) #,index_label=['title','src','author','date','description'])\n\n\nBelow you can see the first few rows of the newsapi.csv file:  \n\n\nIndividual Player Data\nI also want to eventually scrape specific data from fangraphs. For now, I was able to download a few tables that had game data for Aaron Judge and then merge them together. Below are screen shots of the initial csv file.   \n\n\nExtra Joke\nHow much data can be stored in a glacier? A frostbite!"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "more files!"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM",
    "section": "",
    "text": "Why did the arm apply for a job? Because it wanted to lend a helping hand!"
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "What did the tree do when the bank closed?\nIt started its own branch."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Why did the linear regression model break up with the logistic regression model? Because it wanted a more ‘linear’ relationship!"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress"
  },
  {
    "objectID": "about.html#my-story",
    "href": "about.html#my-story",
    "title": "About me",
    "section": "",
    "text": "I am a graduate student at Georgetown University studying Data Science and Analytics. I graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics with a PPE (Politics, Philosophy, & Economics) Concentration and a Minor in Political Science. While a student at Villanova, I studied abroad at the University of Cambridge. During my senior year at Villanova, I had the opportunity to start my own business along with my sisters - we are triplets! Our business, Top Shelf Designs LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. I am extremely excited to continue my education and pursue a masters degree in data science."
  },
  {
    "objectID": "about.html#website-jokes",
    "href": "about.html#website-jokes",
    "title": "About me",
    "section": "Website Jokes",
    "text": "Website Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki"
  },
  {
    "objectID": "about.html#assorted-jokes",
    "href": "about.html#assorted-jokes",
    "title": "About me",
    "section": "Assorted Jokes",
    "text": "Assorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!"
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "introduction.html#why-this-is-important",
    "href": "introduction.html#why-this-is-important",
    "title": "Introduction",
    "section": "Why this is important",
    "text": "Why this is important"
  },
  {
    "objectID": "introduction.html#why-the-reader-should-continue",
    "href": "introduction.html#why-the-reader-should-continue",
    "title": "Introduction",
    "section": "Why the reader should continue",
    "text": "Why the reader should continue"
  },
  {
    "objectID": "introduction.html#what-work-has-been-done-in-the-past",
    "href": "introduction.html#what-work-has-been-done-in-the-past",
    "title": "Introduction",
    "section": "what work has been done in the past",
    "text": "what work has been done in the past"
  },
  {
    "objectID": "introduction.html#what-are-the-different-points-of-viewsinterpretations-in-the-literature",
    "href": "introduction.html#what-are-the-different-points-of-viewsinterpretations-in-the-literature",
    "title": "Introduction",
    "section": "what are the different points of views/interpretations in the literature",
    "text": "what are the different points of views/interpretations in the literature"
  },
  {
    "objectID": "introduction.html#what-i-am-exploring",
    "href": "introduction.html#what-i-am-exploring",
    "title": "Introduction",
    "section": "what I am exploring",
    "text": "what I am exploring"
  },
  {
    "objectID": "introduction.html#questions",
    "href": "introduction.html#questions",
    "title": "Introduction",
    "section": "10 Questions",
    "text": "10 Questions\n\nWhat data is available for my topic?\nWhat does current literature on the topic argue?\nHow can I build off of the current research and approach the topic in a novel way?\nShould I limit the scope of my topic to sports or can I expand past that?\nHow should I define success? (the concept of the hot hand is that success breeds success)\nWhat is the best way to visualize the data?\nDo athletes and/or the public believe in this phenomenon?\nIs there any evidence that the hot hand exists?\nDoes the hot hand impact strategy of a game? Should it impact strategy?\nIf the hot hand does exist, in what sport (or area outside of sports) is there the most evidence in support of the phenomenon?"
  },
  {
    "objectID": "introduction.html#goals-and-hypothesis",
    "href": "introduction.html#goals-and-hypothesis",
    "title": "Introduction",
    "section": "Goals and Hypothesis",
    "text": "Goals and Hypothesis\n\nI would love to find some evidence that the hot hand exists"
  },
  {
    "objectID": "introduction.html#prior-research",
    "href": "introduction.html#prior-research",
    "title": "Introduction",
    "section": "Prior Research",
    "text": "Prior Research\nThe initial investigation into this topic Gilovich, Vallone, and Tversky (1985) was published in 1985. It analyzed assorted data, including professional basketball field goal data from the 1980-1981 season, professional basketball free-throw data from the 1980-1982 seasons, and a controlled shooting experiment. While the study found that over 91% of fans agreed that a player has a better chance of making a shot after having just made his last two or three shots than he does after having just missed his last two or three shots, none of their data showed any evidence of this phenomenon. Instead, Gilovich, Vallone, and Tversky (1985) argued that there is a wide-spread misperception of random sequences:\n\nPeople’s intuitive conceptions of randomness depart systematically from the laws of chance. It appears that people expect the essential characteristics of a chance process to be represented not only globally in the entire sequences, but also locally, in each of its parts. For instance, people expect even short sequences of heads and tails to reflect the fairness of a coin and contain roughly 50% heads and 50% tails. This conception of chance has been described as a ‘belief in the law of small numbers’ according to which the law of large numbers applies to small samples as well. A locally representative sequence, however, deviates systematically from chance expectation: It contains too many alternations and not enough long runs.\n\nBar-Eli, Avugos, and Raab (2006) published a review and critique of 20 years of “hot hand” research in 2006. This paper reviewed the Gilovich, Vallone, and Tversky (1985) study in addition to subsequent research that used data from various sports including basketball, baseball, golf, darts, tennis, bowling, and more. Baseball and basketball studies dominate the literature on this subject, yet the strongest support for the “hot hand” can be found in more individual sports such as horseshoe pitching and tennis.\n\nDemonstrations of hot hands per se are rare and often weak, due to various reasons: using and unrealistic model and questionable data, setting questionable definitions for hot and cold players, relating streakiness to difficulty of task, combining and analyzing data of all players a as a group, and other constraints related to the kind of sport studied.\n\nIn the end, this study found that the question remains unresolved.  MORE RESEARCH TO COME"
  },
  {
    "objectID": "introduction.html#the-debate",
    "href": "introduction.html#the-debate",
    "title": "Introduction",
    "section": "The Debate",
    "text": "The Debate\nMORE TO BE ADDED"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilovich, Vallone, and Tversky (1985)↩︎\nBar-Eli, Avugos, and Raab (2006)↩︎"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress \n\nIntroduction to Naive Bayes\nNaive Bayes, a widely acclaimed machine learning algorithm, harnesses Bayes’ Theorem to categorize data into predefined classes or categories. Praised for its simplicity, swift training capabilities, and robust performance, it stands as a foundational tool in data science. At its core, Bayes’ Theorem calculates the probability of event A given the occurrence of event B, expressed as: \\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\] Naive Bayes accomplishes classifications by leveraging feature vectors and the principles of Bayes’ Theorem to assess values. The ‘naive’ label in its name stems from its assumption of independence among predictors, simplifying computational tasks. This algorithm shines in contexts featuring text and categorical data, excelling in applications like spam email identification, sentiment analysis, and document categorization. Despite its seemingly ‘naive’ premise, Naive Bayes consistently delivers impressive real-world performance, making it a crucial tool for various data science classification tasks.  Common varients of Naive Bayes include Multinomial, Guassian, and Bernoulli Naive Bayes. Multinomial Naive Bayes is the most common variant as is often used for text classification. Gaussian Naive Bayes is appropriate for continuous numerical data while Bernoulli Naive Bayes is a derivation of Multinomial Naive Bayes that is appropriate for binary or boolean data.  The purpose of this page is to implement Naïve Bayes classification on a variety of datasets, some of which may be more for suitable than others for this method. This work is a component of my DSAN 5000 class project.\n\n\nData Preparation\nData must initially be prepared to utilize a Naive Bayes model. Although a substantial part of this process has been covered in the data cleaning and exploratory data analysis (EDA) phases, we must still convert all categorical and label columns into factor types. Additionally, data must be split into training and test subsets. In the following code, we will complete the preparation of the 2021-22 NCAA data and text data for modeling.\n\nNCAA Data\n\nnova2122 &lt;- read.csv('./data/modified_data/nova2122_updated.csv')\n\n\n# Load relevant libraries\nlibrary(tidyverse)\nlibrary(caret)\n\n\nstr(nova2122)\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 -1 1 1 -1 1 -1 -1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : int  NA NA -1 NA NA -1 NA NA 1 NA ...\n $ lag2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\nnova2122$lag1 &lt;- as.factor(nova2122$lag1)\nnova2122$lag2 &lt;- as.factor(nova2122$lag2)\nnova2122$lag3 &lt;- as.factor(nova2122$lag3)\nnova2122$lag4 &lt;- as.factor(nova2122$lag4)\nnova2122$lag5 &lt;- as.factor(nova2122$lag5)\nnova2122$lag6 &lt;- as.factor(nova2122$lag6)\nnova2122$shot_outcome_numeric &lt;- as.factor(nova2122$shot_outcome_numeric)\n\n\nstr(nova2122)\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: Factor w/ 2 levels \"-1\",\"1\": 1 1 1 1 2 2 1 2 1 1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : Factor w/ 2 levels \"-1\",\"1\": NA NA 1 NA NA 1 NA NA 2 NA ...\n $ lag2                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n# Set a seed for reproducibility\nset.seed(137)\n\n# Create an index for splitting the data (70% for training, 30% for validation)\nindex &lt;- createDataPartition(y = nova2122$shot_outcome_numeric, p = 0.7, list = FALSE)\n\n# Create the training and validation subsets\ntraining_data &lt;- nova2122[index, ]\nvalidation_data &lt;- nova2122[-index, ]\n\nwrite.csv(training_data, file = \"./data/modified_data/nova2122_training.csv\", row.names = FALSE)\nwrite.csv(validation_data, file = \"./data/modified_data/nova2122_validation.csv\", row.names = FALSE)\n\n\n\nText Data\n\n\n\nFeature Selection\n\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nimport itertools\nfrom scipy.stats import spearmanr\n\n\ntraining = pd.read_csv('./data/modified_data/nova2122_training.csv')\nvalidation = pd.read_csv('./data/modified_data/nova2122_validation.csv')\n\n# Convert DataFrames to numpy arrays\nx = training[['lag1', 'lag2', 'lag3']].values\nx = np.nan_to_num(x, nan=0)\ny = training[['shot_outcome_numeric']].values\n\ndef merit(x, y, correlation='pearson'):\n    k = x.shape[1]\n    \n    if correlation == 'pearson':\n        rho_xx = np.mean(np.corrcoef(x, x, rowvar = False))\n        rho_xy = np.mean(np.corrcoef(x, y, rowvar = False))\n    elif correlation == 'spearman':\n        rho_xx = np.mean(spearmanr(x, x, axis = 0)[0])\n        rho_xy = np.mean(spearmanr(x, y, axis = 0)[0])\n    else:\n        raise ValueError(\"Error: Unsupported Correlation Method. Try Again.\")\n    \n    merit_numerator = k * np.absolute(rho_xy)\n    merit_denominator = np.sqrt(k + k * (k - 1) * np.absolute(rho_xx))\n    merit_score = merit_numerator / merit_denominator\n    \n    return merit_score\n\n\ndef maximize_CFS(x, y):\n    num_features = x.shape[1]\n    max_merit = 0\n    optimal_subset = None\n    list1 = [*range(0, num_features)]\n    for L in range(1, len(list1) + 1):\n        for subset in itertools.combinations(list1, L):\n            x_subset = x[:, list(subset)]\n            subset_merit = merit(x_subset, y)\n            if subset_merit &gt; max_merit:\n                max_merit = subset_merit\n                optimal_subset = list(subset)\n    return optimal_subset  # Return the indices of selected features\n\nselected_indices = maximize_CFS(x, y)\nprint(selected_indices)\n\n[0]\n\n\n\nz = training['lag1'].values\nz = np.nan_to_num(x, nan=0)\n\nmerit(z,y)\n\n0.3818269784940867\n\n\nAn output of [0] in the above code indicates that, according to the Correlation-based Feature Selection (CFS) algorithm, the optimal subset comprises only the ‘lag1’ feature. This suggests that, under the criteria applied, ‘lag1’ provides the most valuable information for classifying the ‘shot_outcome_numeric’ variable.\n‘lag2’ and ‘lag3’ are considered less informative for predicting ‘shot_outcome_numeric’ using this particular feature selection approach and correlation-based merit score.\nAdditionally, a merit score of 0.3818 indicates a moderate positive correlation between the ‘lag1’ feature and ‘shot_outcome_numeric,’ suggesting that ‘lag1’ contains relevant information for predicting the target variable.\n\n\nNaive Bayes with Labeled Record Data\n\nnova2122_training &lt;- read.csv(\"./data/modified_data/nova2122_training.csv\")\nnova2122_validation &lt;- read.csv('./data/modified_data/nova2122_validation.csv')\n\n\n\n\n'data.frame':   3781 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 16 18 19 21 23 28 35 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Collin Gillespie\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"made\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 1 1 -1 1 -1 -1 1 ...\n $ shot_sequence       : int  -1 -1 -2 1 1 -1 1 -1 -1 1 ...\n $ previous_shots      : int  0 0 -1 0 -1 0 0 1 -1 -1 ...\n $ lag1                : int  NA NA -1 NA -1 NA NA 1 -1 -1 ...\n $ lag2                : int  NA NA NA NA NA NA NA NA 1 1 ...\n $ lag3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n'data.frame':   1618 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  13 25 31 33 38 40 52 60 62 66 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Eric Dixon\" \"Josh Nickelberry\" \"Collin Gillespie\" \"Sherif Kenney\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"Villanova\" \"La Salle\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 -1 -1 -1 -1 1 1 -1 ...\n $ shot_sequence       : int  -1 -1 -1 -2 -1 -1 -3 3 4 -2 ...\n $ previous_shots      : int  0 0 1 -1 0 0 -2 2 3 -1 ...\n $ lag1                : int  NA NA 1 -1 NA NA -1 1 1 -1 ...\n $ lag2                : int  NA NA NA NA NA NA -1 1 1 NA ...\n $ lag3                : int  NA NA NA NA NA NA NA NA 1 NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n# Load the e1071 package\nlibrary(e1071)\n\n# Load your training and validation data\nnova2122_training &lt;- read.csv(\"./data/modified_data/nova2122_training.csv\")\nnova2122_validation &lt;- read.csv('./data/modified_data/nova2122_validation.csv')\n\nnova2122_training$lag1 &lt;- as.factor(nova2122_training$lag1)\nnova2122_training$shot_outcome_numeric &lt;- as.factor(nova2122_training$shot_outcome_numeric)\nnova2122_validation$lag1 &lt;- as.factor(nova2122_validation$lag1)\nnova2122_validation$shot_outcome_numeric &lt;- as.factor(nova2122_validation$shot_outcome_numeric)\n\n# Create a Naive Bayes model\nnb_model &lt;- naiveBayes(shot_outcome_numeric ~ lag1, data = nova2122_training)\n\n# Make predictions on the validation set\nvalidation_predictions &lt;- predict(nb_model, nova2122_validation, type = \"class\")\n\n# Assess the accuracy of the model\naccuracy &lt;- mean(validation_predictions == nova2122_validation$shot_outcome_numeric)\ncat(\"Accuracy of the Naive Bayes model:\", accuracy, \"\\n\")\n\nAccuracy of the Naive Bayes model: 0.5463535 \n\n\n\n# Create a confusion matrix\nconf_matrix &lt;- confusionMatrix(data = validation_predictions, reference = nova2122_validation$shot_outcome_numeric)\n\n# Print the confusion matrix\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  -1   1\n        -1 476 400\n        1  334 408\n                                          \n               Accuracy : 0.5464          \n                 95% CI : (0.5217, 0.5708)\n    No Information Rate : 0.5006          \n    P-Value [Acc &gt; NIR] : 0.0001276       \n                                          \n                  Kappa : 0.0926          \n                                          \n Mcnemar's Test P-Value : 0.0164312       \n                                          \n            Sensitivity : 0.5877          \n            Specificity : 0.5050          \n         Pos Pred Value : 0.5434          \n         Neg Pred Value : 0.5499          \n             Prevalence : 0.5006          \n         Detection Rate : 0.2942          \n   Detection Prevalence : 0.5414          \n      Balanced Accuracy : 0.5463          \n                                          \n       'Positive' Class : -1              \n                                          \n\n\n\n#using ggplot\n\n# Create the confusion matrix data\nconf_matrix_data &lt;- data.frame(\n  Prediction = c(\"missed\", \"made\", \"missed\", \"made\"),\n  Reference = c(\"missed\", \"missed\", \"made\", \"made\"),\n  Count = c(476, 334, 400, 408)\n)\n\n# Create the ggplot\ngg &lt;- ggplot(data = conf_matrix_data, aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Count)) +\n  geom_text(aes(label = Count), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Prediction\",\n    y = \"Reference\",\n    fill = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12))\n\ngg + ggtitle(\"Confusion Matrix for 2021-22 NCAA Villanova MBB Shot Data\")\n\n\n\n\nI tried applying Naive Bayes to my time series data. Since it is time series data, the procedures for classifications models like Naive Bayes are not suitable for a scenario like this.\n\n\nNaive Bayes with Labeled Text Data\n\n\nExtra Joke\nAre monsters good at math? Not unless you Count Dracula.\nWhat’s the official animal of Pi day? The Pi-thon!\nWhat do you call a number that can’t sit still? A roamin’ numeral!"
  }
]