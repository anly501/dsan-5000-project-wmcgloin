[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "I will replace this later with my actual data"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nBrief Introduction to EDA\nExploratory Data Analysis (EDA) is a fundamental starting point in data analysis, helping us grasp the data’s characteristics, patterns, and possible outliers. It provides essential insights for making informed modeling decisions.\nBy analyzing the below data, I hope to gain an understanding of overall trends that can aid in refining my hypothesis and inform the construction of a more accurate model.\n\n\nncaahoopR\n\n2021-22 season\n\n\nCode\n# let's read in the data and load in relevant libraries\nnova2122 &lt;- read.csv('./data/modified_data/nova2122.csv')\n\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova2122 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player (aka field goal percentage)\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\n\nA tibble: 12 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nJustin Moore\n574\n0.4721254\n\n\nCollin Gillespie\n549\n0.5336976\n\n\nJermaine Samuels\n439\n0.5535308\n\n\nCaleb Daniels\n356\n0.5056180\n\n\nEric Dixon\n340\n0.5794118\n\n\nBrandon Slater\n308\n0.5876623\n\n\nChris Arcidiacono\n69\n0.4927536\n\n\nJordan Longino\n57\n0.4210526\n\n\nBryan Antoine\n46\n0.3043478\n\n\nTrey Patterson\n12\n0.3333333\n\n\nDhamir Cosby-Roundtree\n8\n0.5000000\n\n\nNnanna Njoku\n6\n0.5000000\n\n\n\n\n\n\n\n\nThe table displayed above, arranged in descending order based on the number of shots attempted, presents the field goal percentages of Villanova Men’s Basketball (MBB) players for the 2021-22 season. The accompanying ggplot-generated graph visually represents the count of both missed and successful shots for each player. This visualization emphasizes the significant variation in the number of shots taken by different players, which could offer richer data and potential insights for subsequent modeling.\n\n\nCode\n# Create lag variables within each shooter and game_id group\nnova2122 &lt;- nova2122 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\nwrite.csv(nova2122, file = \"./data/modified_data/nova2122_updated.csv\", row.names = FALSE)\n\n# View the updated data with lag variables\nhead(nova2122)\n\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401365747\n4\n1\nJustin Moore\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n7\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n11\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n13\n1\nEric Dixon\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n16\n1\nCollin Gillespie\nmade\nVillanova\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n18\n1\nEric Dixon\nmade\nVillanova\n1\n1\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova2122[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe correlation heatmap presented above carries an intriguing insight. Although it may not reveal strong correlations between “shot_outcome_numeric” and the lag variables individually, a notable descending trend emerges from “lag1” to “lag6.” This observation could provide valuable insight, suggesting that a player’s shot outcome is more likely to be influenced by their immediate prior shot, rather than a shot taken several attempts ago.\n\n\n2019-20 season\nTo assess potential disparities, let’s replicate the same analysis for the 2019-20 season and compare the resulting graphs and tables with those generated earlier. This comparative approach will help us identify any noticeable differences and potential insights.\n\n\nCode\n#let's read in the data\nnova1920 &lt;- read.csv('./data/modified_data/nova1920.csv')\n\n\n\n\nCode\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova1920 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\n\nA tibble: 10 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nCollin Gillespie\n491\n0.4969450\n\n\nSaddiq Bey\n458\n0.5349345\n\n\nJustin Moore\n355\n0.4647887\n\n\nJeremiah Robinson-Earl\n347\n0.5533141\n\n\nJermaine Samuels\n334\n0.5419162\n\n\nCole Swider\n171\n0.4561404\n\n\nBrandon Slater\n68\n0.3823529\n\n\nDhamir Cosby-Roundtree\n36\n0.6666667\n\n\nBryan Antoine\n25\n0.3600000\n\n\nChris Arcidiacono\n6\n0.1666667\n\n\n\n\n\n\n\n\n\n\nCode\n# Create lag variables within each shooter and game_id group\nnova1920 &lt;- nova1920 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\n# View the updated data with lag variables\nhead(nova1920)\n\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401166061\n2\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n4\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n6\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n8\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n2\n1\n1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n9\n1\nCollin Gillespie\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n11\n1\nCJ Walker\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova1920[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n\n\n\n\n\nWe can observe that, despite some player variations, most of the graphs maintain a substantial degree of consistency, which further supports the earlier findings.\n\n\n\nNews API\n\n\nCode\n#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnews_api = pd.read_csv('./data/modified_data/sentiment_scores_with_titles.csv')\n\n\n\n\nCode\n#what does this data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\n\n\n\n\n\n\n\n\n\nCode\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnews_api['cleaned_text'] = news_api['Description'].apply(preprocess_text)\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\n\nCode\nnews_api.to_csv('./data/modified_data/news_api_naive.csv', index=False)\n\n#what does the new column of data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\n\n\nCode\n# Import more necessary libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Define the function to plot the word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(10, 6))\n    # Display the word cloud\n    plt.imshow(wordcloud)\n    # Remove axis details\n    plt.axis(\"off\")\n    # Show the word cloud\n    plt.show()\n\n# Define the function to generate and display the word cloud\ndef generate_word_cloud(my_text):\n    # Generate the word cloud\n    wordcloud = WordCloud(\n        width=800,\n        height=400,\n        background_color='white',\n        colormap='viridis',\n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n    # Plot and display the word cloud\n    plot_cloud(wordcloud)\n\n# let's pass the 'cleaned_text' column to the function\ngenerate_word_cloud(' '.join(news_api['cleaned_text']))\n\n\n\n\n\nWithin the word cloud, generated from articles collected through the news API, notable recurring terms include “win,” “losing,” “victory,” “winning,” “matchup,” and others. These terms hold the potential to offer insights into the articles’ context and serve as valuable cues for conducting sentiment analysis.\n\n\nIndividual Player Data\n\n\nCode\n#let's import some libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\naaronjudge = pd.read_csv('./data/modified_data/aaronjudge.csv')\n\n\n\n\nCode\n#let's learn about the data\naaronjudge.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 36 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              100 non-null    object \n 1   Team              100 non-null    object \n 2   Opp               100 non-null    object \n 3   BO                100 non-null    int64  \n 4   Pos               100 non-null    object \n 5   PA                100 non-null    float64\n 6   H                 100 non-null    int64  \n 7   2B                100 non-null    int64  \n 8   3B                100 non-null    int64  \n 9   HR                100 non-null    int64  \n 10  R                 100 non-null    int64  \n 11  RBI               100 non-null    int64  \n 12  SB                100 non-null    int64  \n 13  CS                100 non-null    int64  \n 14  BB%               100 non-null    float64\n 15  K%                100 non-null    object \n 16  ISO               100 non-null    float64\n 17  BABIP             100 non-null    float64\n 18  EV                100 non-null    float64\n 19  AVG               100 non-null    float64\n 20  OBP               100 non-null    float64\n 21  SLG               100 non-null    float64\n 22  wOBA              100 non-null    float64\n 23  wRC+              100 non-null    int64  \n 24  Events            100 non-null    float64\n 25  EV.1              100 non-null    float64\n 26  maxEV             100 non-null    float64\n 27  LA                100 non-null    float64\n 28  Barrels           100 non-null    int64  \n 29  Barrel%           100 non-null    object \n 30  HardHit           100 non-null    int64  \n 31  HardHit%          100 non-null    float64\n 32  location          100 non-null    object \n 33  at_bats           100 non-null    float64\n 34  hard_hits         100 non-null    float64\n 35  correct_hardhit%  100 non-null    float64\ndtypes: float64(17), int64(12), object(7)\nmemory usage: 28.2+ KB\n\n\n\n\nCode\n# Create a pivot table to count the observations\npivot_table = aaronjudge.pivot_table(index='hard_hits', columns='H', aggfunc='size', fill_value=0)\n\n# Create a heatmap\nax = sns.heatmap(pivot_table, cmap=\"Blues\", annot=True, fmt=\"d\")\n\n# Customize the y-axis to start at 0 and increase as you go up\nax.set_yticklabels(ax.get_yticklabels(), rotation=0)\nax.invert_yaxis()\n\n# Customize the plot if needed\nplt.title(\"Heat Map Showing Hits v. Hard Hits\")\nplt.xlabel(\"Hits\")\nplt.ylabel(\"hard_hits\")\n\nplt.show()\n\n\n\n\n\nIn this heatmap, hits are represented on the x-axis, while hard hits are depicted on the y-axis. The unit of observation corresponds to a player’s at-bats within a game. Notably, there are instances, such as nine games for the specific player Aaron Judge, where he had two hard-hit balls but only managed to secure one hit. While the seaborn-generated graph above indeed suggests a positive correlation between these variables, there are discernible distinctions between them. It prompts the consideration that using hard hit percentage as a target variable to measure success may offer a more robust approach, as it mitigates factors beyond the batter’s control. For example, a batter might make solid contact (barrel the ball) but hit it directly to a fielder, categorizing it as a hard hit ball without resulting in a hit. Hence, hard hit percentage emerges as a more suitable target variable for assessment.\n\n\nCode\n# Sort the DataFrame by Date in ascending order\naaronjudge = aaronjudge.sort_values(by='Date')\n\n# Create subplots with 2 rows and 1 column\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\n\n# First subplot - correct_hardhit%\nsns.barplot(data=aaronjudge, y='correct_hardhit%', x='Date', ax=axes[0])\naxes[0].set_title(\"Aaron Judge Hard Hit Percentage (per each individual game) over the course of the 2023 season\")\naxes[0].set_xlabel(\"Date\")\naxes[0].set_ylabel(\"hard hit %\")\n# Get the x-axis tick positions\nx_ticks = axes[0].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[0].set_xticks(visible_ticks)\n\n# Second subplot - H\nsns.barplot(data=aaronjudge, y='H', x='Date', ax=axes[1])\naxes[1].set_title(\"Aaron Judge Hits over the course of the 2023 season\")\naxes[1].set_xlabel(\"Date\")\naxes[1].set_ylabel(\"Hits\")\n# Get the x-axis tick positions\nx_ticks = axes[1].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[1].set_xticks(visible_ticks)\n\n# Adjust the layout to avoid overlap\nplt.tight_layout()\n\n# Show the combined figure\nplt.show()\n\n\n\n\n\nThe depicted graph highlights the potential for uncovering meaningful trends in hard hit data, surpassing the simplistic examination of hits alone. It suggests the feasibility of leveraging past hard hit data to predict future hard hit performance, potentially driven by autocorrelation or seasonality. This insight holds promise for enhancing the precision of future models.\n\n\nHypothesis Refinement\nFollowing the above analysis, my null hypothesis, asserting that the “hot hand” is actually a fallacy, remains unchanged. However, the alternative hypotheses have been refined for both basketball and baseball analyses based on the insights drawn from the visualizations. In the context of basketball, the refined alternative hypothesis suggests that a player’s shot outcome is more likely to be influenced by their immediately preceding shot, rather than one taken several attempts ago. In the context of baseball, my alternative hypothesis suggests that a batter’s hard hit percentage is more likely influenced by their prior hard hit percentage rather than solely assessing success or streaks based on hits.\n\n\nExtra Joke\nWhat kind of car does Darth Vader drive? A toy-Yoda!  \n\n\nWatch Out!\nyou can’t be too careful when exploring!"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "The Hot Hand: Unraveling the Myth",
    "section": "",
    "text": "Introduction to the Hot Hand Phenomenon:\nIn the dynamic realm of sports, the concepts of a “hot hand” or being “in the zone” have become ubiquitous, often used to describe a player’s extraordinary performance streak. Originating in basketball, the term suggests that a player on a successful streak is more likely to continue their success. This belief, rooted in the increased confidence of the player, extends beyond basketball, finding resonance in baseball and other fields. Despite its prevalence, statistical evidence supporting the existence of the hot hand is scarce, with numerous studies suggesting it is a fallacy. This project embarks on an exploration within the sports domain to scrutinize the mystery of streaks.\n\n\nDiverse Dataset Exploration:\nThroughout the semester, I undertook a comprehensive exploration of the hot hand phenomenon, employing an array of datasets with varying complexities. The NCAA basketball data, scraped from the Villanova Men’s Basketball team in R, offered insights into shot data. The baseball data, sourced from FanGraphs and scraped in R, included both individual player data and detailed pitch-by-pitch information. Additionally, news text data was gathered using an API in Python. Each dataset’s unique cleaning process laid the foundation for an in-depth Exploratory Data Analysis (EDA), allowing for the refinement of hypotheses and the delineation of the investigative path.\n\n\nEDA Signals and Insights:\nPromising signals emerged during the EDA phase, particularly in individual player baseball data and the NCAA shot data. The former hinted at leveraging past hard-hit data to predict future performance, showcasing potential autocorrelation or seasonality effects. This finding holds promise for refining future models. The NCAA shot data, while lacking strong correlations individually, exhibited a descending trend in lag variables, suggesting the immediate prior shot’s influence on the current outcome.\n\n\nModel Performance and Limitations:\nThe Naive Bayes model, designed to predict made or missed shots, failed to outperform random guessing, aligning with the null hypothesis that the hot hand does not exist. Further stages of analysis required the addition of numerical feature variables for the NCAA data, including shot value, score differential, and shooter field goal percentage.  While I was successful in reducing dimensions (4 variables accounted for over 75% of overall variance), the various clustering methods (KMeans, DBSCAN, and Hierarchical Clustering) failed to conclusively establish the presence of discernible data clusters in the shot data. Decision trees and random forests, though outperforming the Naive Bayes classifier, primarily relied on shot value rather than the lag variable, reaffirming its lower predictive power.\n\n\nLooking Ahead:\nI recognize the limitations of predominantly relying on NCAA shot data to confirm the hot hand fallacy. I plan to revisit this topic at a later date, armed with a deeper understanding of time series data and an expanded arsenal of machine learning algorithms. Future analyses may explore this topic using alternative definitions of success, such as launch speeds and angles in baseball. For now, my findings align with prior research, reinforcing the notion that the hot hand is indeed a fallacy.\n\n\nExtra Joke\nWhat do you get when you cross a pirate with a data scientist?    Someone who specializes in Rrrr.  Thank you for taking the time to explore my project, and kudos for enduring all the humor along the way!"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nIntroduction to Naive Bayes\nNaive Bayes, a widely acclaimed machine learning algorithm, harnesses Bayes’ Theorem to categorize data into predefined classes or categories. Praised for its simplicity, swift training capabilities, and robust performance, it stands as a foundational tool in data science. At its core, Bayes’ Theorem calculates the probability of event A given the occurrence of event B, expressed as: \\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\] Naive Bayes accomplishes classifications by leveraging feature vectors and the principles of Bayes’ Theorem to assess values. The ‘naive’ label in its name stems from its assumption of independence among predictors, simplifying computational tasks. This algorithm excels in contexts featuring text and categorical data, such as in applications like spam email identification, sentiment analysis, and document categorization. Despite its seemingly ‘naive’ premise, Naive Bayes consistently delivers impressive real-world performance, making it a crucial tool for various data science classification tasks.  Common varients of Naive Bayes include Multinomial, Guassian, and Bernoulli Naive Bayes. Multinomial Naive Bayes is the most common variant and is often used for text classification. Gaussian Naive Bayes is appropriate for continuous numerical data, while Bernoulli Naive Bayes is a derivation of Multinomial Naive Bayes that is appropriate for binary or boolean data.  The purpose of this page is to implement Naïve Bayes classification on a variety of datasets, some of which may be more for suitable than others for this method. This work is a component of my DSAN 5000 class project.\n\n\nData Preparation\nData must initially be prepared to utilize a Naive Bayes model. Although a substantial part of this process has been covered in the data cleaning and exploratory data analysis (EDA) phases, all categorical and label columns must be converted into factor types. Additionally, data must be split into training and test subsets. This is done to train the model on one subset and subsequently evaluate the model’s performance on an independent dataset which can be used to asses the bias and variance of the machine learning model. In the following code, we will complete the preparation of the 2021-22 NCAA data and news data for modeling.\n\nNCAA Data\nFor this section, we will be using R. After reading in the data and loading in relevant libraries, let’s take another look at the dataset.\n\n\nCode\n#read in the dataset\nnova2122 &lt;- read.csv('./data/modified_data/nova2122_updated.csv')\n\n# Load relevant libraries\nlibrary(tidyverse)\nlibrary(caret)\n\n#let's take another look at the dataset\nstr(nova2122)\n\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 -1 1 1 -1 1 -1 -1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : int  NA NA -1 NA NA -1 NA NA 1 NA ...\n $ lag2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe need to change the lag variables and the shot_outcome_numeric column to be factors. Let’s visualize this change in the output.\n\n\nCode\n# changing columns to become a factor\nnova2122$lag1 &lt;- as.factor(nova2122$lag1)\nnova2122$lag2 &lt;- as.factor(nova2122$lag2)\nnova2122$lag3 &lt;- as.factor(nova2122$lag3)\nnova2122$lag4 &lt;- as.factor(nova2122$lag4)\nnova2122$lag5 &lt;- as.factor(nova2122$lag5)\nnova2122$lag6 &lt;- as.factor(nova2122$lag6)\nnova2122$shot_outcome_numeric &lt;- as.factor(nova2122$shot_outcome_numeric)\n                                           \n#looking at how this changed the dataset\nstr(nova2122)\n\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: Factor w/ 2 levels \"-1\",\"1\": 1 1 1 1 2 2 1 2 1 1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : Factor w/ 2 levels \"-1\",\"1\": NA NA 1 NA NA 1 NA NA 2 NA ...\n $ lag2                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n\n\nAfter setting a set for reproducibility, we create an index for splitting the data (70% for training, 30% for validation). We then use the index to subset the data and save them for later use (switching to python code for feature selection).\n\n\nCode\n# Set a seed for reproducibility\nset.seed(137)\n\n# Create an index for splitting the data (70% for training, 30% for validation)\nindex &lt;- createDataPartition(y = nova2122$shot_outcome_numeric, p = 0.7, list = FALSE)\n\n# Create the training and validation subsets\ntraining_data &lt;- nova2122[index, ]\nvalidation_data &lt;- nova2122[-index, ]\n\n#save these for later use\nwrite.csv(training_data, file = \"./data/modified_data/nova2122_training.csv\", row.names = FALSE)\nwrite.csv(validation_data, file = \"./data/modified_data/nova2122_validation.csv\", row.names = FALSE)\n\n\n\n\nNews Data\nFor the news data, we will be using python. After loading in relevant libraries and reading in the data, let’s see what the cleaned data looks like.\n\n\nCode\n#load in relevant libraries and the cleaned data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnewsapi = pd.read_csv('./data/modified_data/news_api_naive.csv')\n\n# let's take another look at the data\nnewsapi.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\nThe below code make the sentiment label a categorical variables, drop rows with missing data, removes unnecessary columns, and splits the data into training, test, and validation subsets.\n\n\nCode\n# Make \"Sentiment Label\" a categorical variable\nnewsapi['Sentiment Label'] = newsapi['Sentiment Label'].astype('category')\n\n# Remove rows with missing data\nnewsapi.dropna(inplace=True)\n\n# Remove unnecessary columns\nnewsapi = newsapi[['Sentiment Label', 'cleaned_text']]\n\n# Split the data into training, test, and validation subsets\ntrain_data, test_data = train_test_split(newsapi, test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n\ntrain_data.head()\n\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n69\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n85\nnegative\nminnesota twin lost straight postseason game t...\n\n\n97\nnegative\npenn state spread season surprise many two les...\n\n\n38\npositive\nround scottish woman premier league celtic ran...\n\n\n2\npositive\nwant watch minnesota viking play green bay pac...\n\n\n\n\n\n\n\n\n\n\nFeature Selection\nFeature selection involves choosing a subset of pertinent features from a larger set to accurately and efficiently represent all variables, with the goal of enhancing model performance and reducing complexity. This process will be implemented using Python on both the NCAA and news data, beginning by loading the necessary libraries.\n\n\nCode\n#load in relevant libraries\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nimport itertools\nfrom scipy.stats import spearmanr\n\n\nThe below code creates two functions. The first function, merit, computes the figure of merit of a given subset of features. It works for both the Pearson and Spearman correlation matrix. The second function, maximize_CFS, takes two matrices x and y, iterates over all possible subset combinations of the x features, and computes the figure of merit for each subset. It keeps track of the max merit and returns the indices of the features that correspond to the max merit.\n\n\nCode\ndef merit(x, y, correlation='pearson'):\n    k = x.shape[1]\n    \n    if correlation == 'pearson':\n        rho_xx = np.mean(np.corrcoef(x, x, rowvar = False))\n        rho_xy = np.mean(np.corrcoef(x, y, rowvar = False))\n    elif correlation == 'spearman':\n        rho_xx = np.mean(spearmanr(x, x, axis = 0)[0])\n        rho_xy = np.mean(spearmanr(x, y, axis = 0)[0])\n    else:\n        raise ValueError(\"Error: Unsupported Correlation Method. Try Again.\")\n    \n    merit_numerator = k * np.absolute(rho_xy)\n    merit_denominator = np.sqrt(k + k * (k - 1) * np.absolute(rho_xx))\n    merit_score = merit_numerator / merit_denominator\n    \n    return merit_score\n\n\ndef maximize_CFS(x, y):\n    num_features = x.shape[1]\n    max_merit = 0\n    optimal_subset = None\n    list1 = [*range(0, num_features)]\n    for L in range(1, len(list1) + 1):\n        print(L/(len(list1)+1))\n        for subset in itertools.combinations(list1, L):\n            x_subset = x[:, list(subset)]\n            subset_merit = merit(x_subset, y)\n            if subset_merit &gt; max_merit:\n                max_merit = subset_merit\n                optimal_subset = list(subset)\n    return optimal_subset  # Return the indices of selected features\n\n\n\nNCAA Data\nUsing python, we will now implement feature selection on the NCAA data. We begin by loading in the data and then convert the dataframes to numpy arrays. The data is then fed into the maximize_CFS function to find the best subset of features.\n\n\nCode\ntraining = pd.read_csv('./data/modified_data/nova2122_training.csv')\nvalidation = pd.read_csv('./data/modified_data/nova2122_validation.csv')\n\n# Convert DataFrames to numpy arrays\nx = training[['lag1', 'lag2', 'lag3']].values\nx = np.nan_to_num(x, nan=0)\ny = training[['shot_outcome_numeric']].values\n\nselected_indices = maximize_CFS(x, y)\nprint(selected_indices)\n\n\n[0]\n\n\n\n\nCode\nz = training['lag1'].values\nz = np.nan_to_num(x, nan=0)\n\nmerit(z,y)\n\n\n0.3818269784940867\n\n\nAn output of [0] in the above code indicates that, according to the Correlation-based Feature Selection (CFS) algorithm, the optimal subset comprises only the ‘lag1’ feature. This suggests that, under the criteria applied, ‘lag1’ provides the most valuable information for classifying the ‘shot_outcome_numeric’ variable.\n‘lag2’ and ‘lag3’ are considered less informative for predicting ‘shot_outcome_numeric’ using this particular feature selection approach and correlation-based merit score.\nAdditionally, a merit score of 0.3818 indicates a moderate positive correlation between the ‘lag1’ feature and ‘shot_outcome_numeric,’ suggesting that ‘lag1’ contains relevant information for predicting the target variable.\n\n\nNews Data\nAfter loading in relevant libraries the below code defines the features and target variable.\n\n\nCode\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Define features and target variable \nX_train = train_data['cleaned_text'].tolist()\ny_train = train_data['Sentiment Label']\n\n\nThe target variable (sentiment_label) needs to be labeled numerically. Let’s fix that now by mapping the labels to numeric values.\n\n\nCode\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\n\n# Assuming you have a DataFrame 'df' and a column 'Sentiment Label' that you want to map\ny_train = y_train.map(label_mapping)\n\ny_train.head()\n\n\n69    0\n85   -1\n97   -1\n38    1\n2     1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\nThe below code uses the CountVectorizer from scikit-learn to convert text data into numerical features, specifically for training a Naive Bayes model. It transforms the training text data into a dataframe to examine its structure before applying it to the model.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the text data\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# Convert text data to numerical features using CountVectorizer\nvectorizer = CountVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# convert the vectorized training data into a dataframe\ndf = pd.DataFrame(X_train_vectorized.toarray())\n\n#let's look at the new dataframe that will be used for training the naive bayes model\ndf.describe()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\ncount\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n...\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n\n\nmean\n0.013889\n0.013889\n0.013889\n0.013889\n0.041667\n0.013889\n0.013889\n0.013889\n0.013889\n0.013889\n...\n0.013889\n0.013889\n0.013889\n0.027778\n0.097222\n0.013889\n0.055556\n0.027778\n0.041667\n0.013889\n\n\nstd\n0.117851\n0.117851\n0.117851\n0.117851\n0.201229\n0.117851\n0.117851\n0.117851\n0.117851\n0.117851\n...\n0.117851\n0.117851\n0.117851\n0.165489\n0.298339\n0.117851\n0.230669\n0.165489\n0.201229\n0.117851\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 625 columns\n\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 625 columns\n\n\n\nIt would take way too long to iterate every possible combination of subsets for x features, so let’s reduce the number of features. We can do this by taking out words that do not appear often (the sum of the word across all documents is 5 or less). This leaves us with 19 features.\n\n\nCode\n# let's calculate the sum of each column\ncolumn_sums = df.sum()\n\n# Find columns where the sum is 5 or less\ncolumns_to_remove = column_sums[column_sums &lt; 6].index\n\n# Remove the selected columns from the DataFrame\ndf = df.drop(columns=columns_to_remove)\n\ndf.head()\n\n\n\n\n\n\n\n\n\n80\n182\n201\n235\n267\n280\n302\n322\n355\n359\n474\n503\n519\n527\n542\n551\n601\n610\n619\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nThe above dataframe is much smaller and more managable than what we had before. We must convert both x and y to arrays and implement feature selection using the maximize_CFS function, selecting the optimal subset of features.\n\n\nCode\n# the above df is much smaller and more managable than what we had before\n\n#convert both to arrays\nx = df.values\ny = y_train.values\n\n# Implement feature selection using the maximize_CFS function\noptimal_subset_indices = maximize_CFS(x, y)\n\n# Select the optimal subset of features for training and validation data\nprint(optimal_subset_indices)\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n[17]\n\n\nThe numbers above [17] served as a progress indicator, representing the completion percentage of the maximize_CFS function. The earlier simplification was necessary to prevent the function from running indefinitely.  Much like the NCAA output mentioned earlier, the [17] index signifies the optimal subset as determined by the correlation-based feature selection algorithm. In this case, that index corresponds to the word “win,” as evident in the code below. This observation implies that, based on the given criteria, the presence of the word “win” offers the most significant information for accurately classifying the sentiment label variable.\n\n\nCode\nx_opt = df.iloc[:, 17]\nx_opt.head()\n\n\n0    0\n1    0\n2    0\n3    1\n4    0\nName: 610, dtype: int64\n\n\n\n\nCode\nword_index = 610 # The column index I want to look up\n\n# Get the vocabulary (word to column index mapping)\nvocabulary = vectorizer.vocabulary_\n\n# Inverse the vocabulary mapping to find the word for the given column index\nword = next(word for word, index in vocabulary.items() if index == word_index)\n\nprint(\"Word at column 610:\", word)\n\n\nWord at column 610: win\n\n\n\n\n\nNaive Bayes with Labeled Record Data\nWe will now implement Naive Bayes classification on the NCAA data using R. As always, we begin by loading in relevant libraries and reading in the data. We then create a naive bayes model and use it to make predictions on the validation set. The accuracy of the model is then computed and displayed.\n\n\nCode\n# Load the e1071 package\nlibrary(e1071)\n\n# let's read in the data\nnova2122_training &lt;- read.csv(\"./data/modified_data/nova2122_training.csv\")\nnova2122_validation &lt;- read.csv('./data/modified_data/nova2122_validation.csv')\n\n# loading in the data caused some of the variables to become numeric, let's change them back to factors\nnova2122_training$lag1 &lt;- as.factor(nova2122_training$lag1)\nnova2122_training$shot_outcome_numeric &lt;- as.factor(nova2122_training$shot_outcome_numeric)\nnova2122_validation$lag1 &lt;- as.factor(nova2122_validation$lag1)\nnova2122_validation$shot_outcome_numeric &lt;- as.factor(nova2122_validation$shot_outcome_numeric)\n\n# Create a Naive Bayes model\nnb_model &lt;- naiveBayes(shot_outcome_numeric ~ lag1, data = nova2122_training)\n\n# Make predictions on the validation set\nvalidation_predictions &lt;- predict(nb_model, nova2122_validation, type = \"class\")\n\n# Assess the accuracy of the model\naccuracy &lt;- mean(validation_predictions == nova2122_validation$shot_outcome_numeric)\ncat(\"Accuracy of the Naive Bayes model:\", accuracy, \"\\n\")\n\n\nAccuracy of the Naive Bayes model: 0.5463535 \n\n\n\n\nCode\n# Create a confusion matrix\nconf_matrix &lt;- confusionMatrix(data = validation_predictions, reference = nova2122_validation$shot_outcome_numeric)\n\n# Print the confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  -1   1\n        -1 476 400\n        1  334 408\n                                          \n               Accuracy : 0.5464          \n                 95% CI : (0.5217, 0.5708)\n    No Information Rate : 0.5006          \n    P-Value [Acc &gt; NIR] : 0.0001276       \n                                          \n                  Kappa : 0.0926          \n                                          \n Mcnemar's Test P-Value : 0.0164312       \n                                          \n            Sensitivity : 0.5877          \n            Specificity : 0.5050          \n         Pos Pred Value : 0.5434          \n         Neg Pred Value : 0.5499          \n             Prevalence : 0.5006          \n         Detection Rate : 0.2942          \n   Detection Prevalence : 0.5414          \n      Balanced Accuracy : 0.5463          \n                                          \n       'Positive' Class : -1              \n                                          \n\n\nThe accuracy of the above model, 0.5464, indicates that the model is accurate 54.64% of the time. The model has a precision of 0.5499, inicating that 54.99% of the model’s positive predictions are correct. The recall of the above model, 0.5877, indicates that the model correctly predicted 58.77% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.5681 which can be used to compare performance to other classifiers.\n\n\nCode\n#using ggplot\n\n# Create the confusion matrix data\nconf_matrix_data &lt;- data.frame(\n  Prediction = c(\"missed\", \"made\", \"missed\", \"made\"),\n  Reference = c(\"missed\", \"missed\", \"made\", \"made\"),\n  Count = c(476, 334, 400, 408)\n)\n\n# Create the ggplot\ngg &lt;- ggplot(data = conf_matrix_data, aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Count)) +\n  geom_text(aes(label = Count), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Prediction\",\n    y = \"Reference\",\n    fill = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12))\n\ngg + ggtitle(\"Confusion Matrix for 2021-22 NCAA Villanova MBB Shot Data\")\n\n\n\n\n\nAs evident in the initial matrix, the subsequent table, and the confusion matrix created using ggplot, the performance of the Naive Bayes model appears to be rather lackluster. With an accuracy rate slightly exceeding 50%, the model’s predictive ability seems only marginally better than random chance. This outcome aligns with the null hypothesis, indicating that the concept of a “hot hand” is likely a misconception, and past performance may not be a reliable predictor of success. It’s worth highlighting that applying Naive Bayes to time series data, as done here, may not be the most suitable approach, given the unique characteristics of this type of data.\n\n\nNaive Bayes with Labeled Text Data\nSwitching back to Python, let’s perform Naive Bayes classification on the news data. Let’s take another look at the data.\n\n\nCode\n# what does the test data look like? \ntest_data.head()\n\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n83\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n53\nneutral\nnfl week odds includes division matchup sunday...\n\n\n70\nneutral\nremoved\n\n\n45\npositive\nreal madrid look continue winning way keep per...\n\n\n44\npositive\nflorida gator hope end road game loss streak w...\n\n\n\n\n\n\n\nWe need to separate our features from our target variable which is done via the code below.\n\n\nCode\n# let's separate our feature(s) from our target variable\nX_test = test_data['cleaned_text']\ny_test = test_data['Sentiment Label']\nX_test.head()\n\n\n83    cbs sport network weekly coverage season keep ...\n53    nfl week odds includes division matchup sunday...\n70                                              removed\n45    real madrid look continue winning way keep per...\n44    florida gator hope end road game loss streak w...\nName: cleaned_text, dtype: object\n\n\nSince we know the feature column only includes the occurances of the word “win”, let’s create a new column ‘win_count’.\n\n\nCode\nX_test_win = X_test.str.lower().str.count('win')\nX_test_win.head()\n\n\n83    0.0\n53    0.0\n70    0.0\n45    1.0\n44    0.0\nName: cleaned_text, dtype: float64\n\n\nOnce again, we must convert the target variable to have numeric labels.\n\n\nCode\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\ny_test = test_data['Sentiment Label'].map(label_mapping)\n\ny_test.head()\n\n\n83    0\n53    0\n70    0\n45    1\n44    1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\nThe following code chunk uses a Categorical Naive Bayes model from scikit-learn to predict the target variable (sentiment label) based on input features (win_count). It prepares the training and test datasets, handles missing values, fits the model, makes predictions, and evaluates the accuracy of the model on the validation set.\n\n\nCode\nimport pandas as pd\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.metrics import accuracy_score\n\n# Convert features to a DataFrame\nnews_training_x = pd.Series(x_opt, name='x')\nnews_training_y = y_train\nnews_test_x = pd.Series(X_test_win, name='x')\nnews_test_y = y_test\n\n# Create a training DataFrame\ntraining_df = pd.DataFrame({'x': news_training_x, 'y': news_training_y})\n\n# Create a test DataFrame\ntest_df = pd.DataFrame({'x': news_test_x, 'y': news_test_y})\n\n# Create a Naive Bayes model\nnb_model = CategoricalNB()\n\n# Remove rows with NaN values\ntraining_df.dropna(subset=['x'], inplace=True)\ntest_df.dropna(subset=['x'], inplace=True)\n\n# Remove rows with NaN values in the target variable 'y'\ntraining_df.dropna(subset=['y'], inplace=True)\ntest_df.dropna(subset=['y'], inplace=True)\n\n# Now, let's fit the model\nnb_model.fit(training_df[['x']], training_df['y'])\n\n# Make predictions on the validation set\nvalidation_predictions = nb_model.predict(test_df[['x']])\n\n# and we can finally assess the accuracy of the model\naccuracy = accuracy_score(test_df['y'], validation_predictions)\nprint(\"Accuracy of the Naive Bayes model:\", accuracy)\n\n\nAccuracy of the Naive Bayes model: 0.5\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(validation_predictions, test_df['y'])\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n\nConfusion Matrix:\n[[ 0  0  0]\n [ 0  0  0]\n [ 5  5 10]]\n\n\n\n\nCode\n#calculate the various metrics\naccuracy = 10/20\nprecision = 10/20\nrecall = 10/10\nf1 = 2 * ((precision * recall) / (precision + recall))\n\n# Create a dictionary with the metrics\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n    'Value': [accuracy, precision, recall, f1]\n}\n\n# Create a DataFrame from the dictionary\nmetrics_df = pd.DataFrame(metrics)\n\nprint(metrics_df)\n\n\n      Metric     Value\n0   Accuracy  0.500000\n1  Precision  0.500000\n2     Recall  1.000000\n3         F1  0.666667\n\n\nThe accuracy of the above model, 0.5, indicates that the model is accurate 50% of the time. The model has a precision of 0.5, inicating that 50% of the model’s positive predictions are correct. The recall of the above model, 1, indicates that the model correctly predicted 100% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.67 which can be used to compare performance to other classifiers.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'conf_matrix' is the confusion matrix obtained previously\n\n# Create a Seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=0.5, cbar=False,\n            xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n\n# Add labels and title\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Confusion Matrix')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\nUpon reviewing the initial matrix, subsequent table, and the Seaborn-generated confusion matrix, it becomes apparent that the Naive Bayes model’s performance is notably subpar. Despite achieving a 50% accuracy, a significant challenge emerges: the consistent prediction of positive label values points to a considerable underfitting problem. This result indicates that the employed Naive Bayes classifier was overly simplistic, resulting in inaccurate sentiment predictions for the news articles.\n\n\nExtra Joke (x2)\nAre monsters good at math? Not unless you Count Dracula.    What’s the official animal of Pi day? The Pi-thon!"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "This page will take you through the data sources and methodologies employed in this specific project. Furthermore, you can find brief descriptions/images/tables of the various datasets mentioned. Data must be acquired using at least one Python API and one R API. This project will use various data formats that may include labeled data, qualitative data, text data, geo data, record-data, etc.\n\nncaahoopR\n“ncaahoopR” is an R package tailored for NCAA Basketball Play-by-Play Data analysis. It excels at retrieving play-by-play data in a tidy format. For the purposes of this project, I will start by scraping play-by-play data for the Villanova Wildcats Men’s Basketball team from both the 2019-20 and 2021-22 seasons (the 2020-21 was shortened due to COVID-19).\n\n\nCode\nlibrary(tidyverse)\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lbenz730/ncaahoopR\")\nlibrary(ncaahoopR)\n\n\nOnce scraped, the data can be saved into a csv file for later cleaning.\n\n\nCode\nVillanova1920 &lt;- get_pbp(\"Villanova\", \"2019-20\")\nVillanova2122 &lt;- get_pbp(\"Villanova\", \"2021-22\")\nwrite.csv(Villanova1920, file = \"./data/raw_data/villanova1920.csv\", row.names = FALSE)\nwrite.csv(Villanova2122, file = \"./data/raw_data/villanova2122.csv\", row.names = FALSE)\n\n\nBefore we move on, let’s take a look at what the raw data looks like:\n\n\nCode\nhead(Villanova1920)\n\n\n\nA data.frame: 6 x 39\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\nshot_y\nshot_team\nshot_outcome\nshooter\nassist\nthree_pt\nfree_throw\npossession_before\npossession_after\nwrong_time\n\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\n401169778\n2019-11-05\nVillanova\nArmy\n1\n1\n19:37\n2377\n2377\nSaddiq Bey made Jumper.\n...\nNA\nVillanova\nmade\nSaddiq Bey\nNA\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n2\n401169778\n2019-11-05\nVillanova\nArmy\n2\n1\n19:16\n2356\n2356\nTucker Blackwell made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nTucker Blackwell\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n3\n401169778\n2019-11-05\nVillanova\nArmy\n3\n1\n19:01\n2341\n2341\nFoul on Jermaine Samuels.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n4\n401169778\n2019-11-05\nVillanova\nArmy\n4\n1\n19:01\n2341\n2341\nJermaine Samuels Turnover.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n5\n401169778\n2019-11-05\nVillanova\nArmy\n5\n1\n18:42\n2322\n2322\nMatt Wilson made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nMatt Wilson\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n6\n401169778\n2019-11-05\nVillanova\nArmy\n6\n1\n18:31\n2311\n2311\nJeremiah Robinson-Earl made Jumper. Assisted by Justin Moore.\n...\nNA\nVillanova\nmade\nJeremiah Robinson-Earl\nJustin Moore\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n\n\n\n\n\nBaseballr\n“Baseballr” is a package in R that focuses on baseball analytics, also known as sabremetrics. It includes various functions that can be used for scraping data from websites like FanGraphs.com, Baseball-Reference.com, and BaseballSavant.mlb.com. It also includes functions for calculating specific baseball metrics such as wOBA (weighted on-base average) and FIP (fielding independent pitching). I will mainly use this package to gather data (which uses an API as can be seen below).\n\nSource Code\nThe below source code was pulled from the baseballr github repository. This specific code uses a mlb api to acquire play-by-play data for a specific game. I will use these functions later on through the baseballr package.\n\n\nCode\nmlb_api_call &lt;- function(url){\n  res &lt;-\n    httr::RETRY(\"GET\", url)\n  \n  json &lt;- res$content %&gt;%\n    rawToChar() %&gt;%\n    jsonlite::fromJSON(simplifyVector = T)\n  \n  return(json)\n}\n\nmlb_stats_endpoint &lt;- function(endpoint){\n  all_endpoints = c(\n    \"v1/attendance\",#\n    \"v1/conferences\",#\n    \"v1/conferences/{conferenceId}\",#\n    \"v1/awards/{awardId}/recipients\",#\n    \"v1/awards\",#\n    \"v1/baseballStats\",#\n    \"v1/eventTypes\",#\n    \"v1/fielderDetailTypes\",#\n    \"v1/gameStatus\",#\n    \"v1/gameTypes\",#\n    \"v1/highLow/types\",#\n    \"v1/hitTrajectories\",#\n    \"v1/jobTypes\",#\n    \"v1/languages\",\n    \"v1/leagueLeaderTypes\",#\n    \"v1/logicalEvents\",#\n    \"v1/metrics\",#\n    \"v1/pitchCodes\",#\n    \"v1/pitchTypes\",#\n    \"v1/playerStatusCodes\",#\n    \"v1/positions\",#\n    \"v1/reviewReasons\",#\n    \"v1/rosterTypes\",#\n    \"v1/runnerDetailTypes\",#\n    \"v1/scheduleEventTypes\",#\n    \"v1/situationCodes\",#\n    \"v1/sky\",#\n    \"v1/standingsTypes\",#\n    \"v1/statGroups\",#\n    \"v1/statTypes\",#\n    \"v1/windDirection\",#\n    \"v1/divisions\",#\n    \"v1/draft/{year}\",#\n    \"v1/draft/prospects/{year}\",#\n    \"v1/draft/{year}/latest\",#\n    \"v1.1/game/{gamePk}/feed/live\",\n    \"v1.1/game/{gamePk}/feed/live/diffPatch\",#\n    \"v1.1/game/{gamePk}/feed/live/timestamps\",#\n    \"v1/game/changes\",##x\n    \"v1/game/analytics/game\",##x\n    \"v1/game/analytics/guids\",##x\n    \"v1/game/{gamePk}/guids\",##x\n    \"v1/game/{gamePk}/{GUID}/analytics\",##x\n    \"v1/game/{gamePk}/{GUID}/contextMetricsAverages\",##x\n    \"v1/game/{gamePk}/contextMetrics\",#\n    \"v1/game/{gamePk}/winProbability\",#\n    \"v1/game/{gamePk}/boxscore\",#\n    \"v1/game/{gamePk}/content\",#\n    \"v1/game/{gamePk}/feed/color\",##x\n    \"v1/game/{gamePk}/feed/color/diffPatch\",##x\n    \"v1/game/{gamePk}/feed/color/timestamps\",##x\n    \"v1/game/{gamePk}/linescore\",#\n    \"v1/game/{gamePk}/playByPlay\",#\n    \"v1/gamePace\",#\n    \"v1/highLow/{orgType}\",#\n    \"v1/homeRunDerby/{gamePk}\",#\n    \"v1/homeRunDerby/{gamePk}/bracket\",#\n    \"v1/homeRunDerby/{gamePk}/pool\",#\n    \"v1/league\",#\n    \"v1/league/{leagueId}/allStarBallot\",#\n    \"v1/league/{leagueId}/allStarWriteIns\",#\n    \"v1/league/{leagueId}/allStarFinalVote\",#\n    \"v1/people\",#\n    \"v1/people/freeAgents\",#\n    \"v1/people/{personId}\",##U\n    \"v1/people/{personId}/stats/game/{gamePk}\",#\n    \"v1/people/{personId}/stats/game/current\",#\n    \"v1/jobs\",#\n    \"v1/jobs/umpires\",#\n    \"v1/jobs/datacasters\",#\n    \"v1/jobs/officialScorers\",#\n    \"v1/jobs/umpires/games/{umpireId}\",##x\n    \"v1/schedule/\",#\n    \"v1/schedule/games/tied\",#\n    \"v1/schedule/postseason\",#\n    \"v1/schedule/postseason/series\",#\n    \"v1/schedule/postseason/tuneIn\",##x\n    \"v1/seasons\",#\n    \"v1/seasons/all\",#\n    \"v1/seasons/{seasonId}\",#\n    \"v1/sports\",#\n    \"v1/sports/{sportId}\",#\n    \"v1/sports/{sportId}/players\",#\n    \"v1/standings\",#\n    \"v1/stats\",#\n    \"v1/stats/metrics\",##x\n    \"v1/stats/leaders\",#\n    \"v1/stats/streaks\",##404\n    \"v1/teams\",#\n    \"v1/teams/history\",#\n    \"v1/teams/stats\",#\n    \"v1/teams/stats/leaders\",#\n    \"v1/teams/affiliates\",#\n    \"v1/teams/{teamId}\",#\n    \"v1/teams/{teamId}/stats\",#\n    \"v1/teams/{teamId}/affiliates\",#\n    \"v1/teams/{teamId}/alumni\",#\n    \"v1/teams/{teamId}/coaches\",#\n    \"v1/teams/{teamId}/personnel\",#\n    \"v1/teams/{teamId}/leaders\",#\n    \"v1/teams/{teamId}/roster\",##x\n    \"v1/teams/{teamId}/roster/{rosterType}\",#\n    \"v1/venues\"#\n  )\n  base_url = glue::glue('http://statsapi.mlb.com/api/{endpoint}')\n  return(base_url)\n}\n\n\n\nBelow is an example usage of the api call using a random game id.\n\n\nCode\nx &lt;- \"http://statsapi.mlb.com/api/v1/game/575156/playByPlay\"\n\noutput &lt;- mlb_api_call(x)\n\n\n“output” is a very messy list that is extremely long. Instead of printing “output”, below are three images of part of the list.\n  \nThe below code builds on the previous code, returning a tibble that includes over 100 columns of data provided by the MLB Stats API at a pitch level. As you will see, the output is much cleaner and easier to work with.\n\n\nCode\n#' @rdname mlb_pbp\n#' @title **Acquire pitch-by-pitch data for Major and Minor League games**\n#'\n#' @param game_pk The date for which you want to find game_pk values for MLB games\n#' @importFrom jsonlite fromJSON\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#'\n#' Some data will vary depending on the\n#' park and the league level, as most sensor data is not available in\n#' minor league parks via this API. Note that the column names have mostly\n#' been left as-is and there are likely duplicate columns in terms of the\n#' information they provide. I plan to clean the output up down the road, but\n#' for now I am leaving the majority as-is.\n#'\n#' Both major and minor league pitch-by-pitch data can be pulled with this function.\n#' \n#'  |col_name                       |types     |\n#'  |:------------------------------|:---------|\n#'  |game_pk                        |numeric   |\n#'  |game_date                      |character |\n#'  |index                          |integer   |\n#'  |startTime                      |character |\n#'  |endTime                        |character |\n#'  |isPitch                        |logical   |\n#'  |type                           |character |\n#'  |playId                         |character |\n#'  |pitchNumber                    |integer   |\n#'  |details.description            |character |\n#'  |details.event                  |character |\n#'  |details.awayScore              |integer   |\n#'  |details.homeScore              |integer   |\n#'  |details.isScoringPlay          |logical   |\n#'  |details.hasReview              |logical   |\n#'  |details.code                   |character |\n#'  |details.ballColor              |character |\n#'  |details.isInPlay               |logical   |\n#'  |details.isStrike               |logical   |\n#'  |details.isBall                 |logical   |\n#'  |details.call.code              |character |\n#'  |details.call.description       |character |\n#'  |count.balls.start              |integer   |\n#'  |count.strikes.start            |integer   |\n#'  |count.outs.start               |integer   |\n#'  |player.id                      |integer   |\n#'  |player.link                    |character |\n#'  |pitchData.strikeZoneTop        |numeric   |\n#'  |pitchData.strikeZoneBottom     |numeric   |\n#'  |details.fromCatcher            |logical   |\n#'  |pitchData.coordinates.x        |numeric   |\n#'  |pitchData.coordinates.y        |numeric   |\n#'  |hitData.trajectory             |character |\n#'  |hitData.hardness               |character |\n#'  |hitData.location               |character |\n#'  |hitData.coordinates.coordX     |numeric   |\n#'  |hitData.coordinates.coordY     |numeric   |\n#'  |actionPlayId                   |character |\n#'  |details.eventType              |character |\n#'  |details.runnerGoing            |logical   |\n#'  |position.code                  |character |\n#'  |position.name                  |character |\n#'  |position.type                  |character |\n#'  |position.abbreviation          |character |\n#'  |battingOrder                   |character |\n#'  |atBatIndex                     |character |\n#'  |result.type                    |character |\n#'  |result.event                   |character |\n#'  |result.eventType               |character |\n#'  |result.description             |character |\n#'  |result.rbi                     |integer   |\n#'  |result.awayScore               |integer   |\n#'  |result.homeScore               |integer   |\n#'  |about.atBatIndex               |integer   |\n#'  |about.halfInning               |character |\n#'  |about.inning                   |integer   |\n#'  |about.startTime                |character |\n#'  |about.endTime                  |character |\n#'  |about.isComplete               |logical   |\n#'  |about.isScoringPlay            |logical   |\n#'  |about.hasReview                |logical   |\n#'  |about.hasOut                   |logical   |\n#'  |about.captivatingIndex         |integer   |\n#'  |count.balls.end                |integer   |\n#'  |count.strikes.end              |integer   |\n#'  |count.outs.end                 |integer   |\n#'  |matchup.batter.id              |integer   |\n#'  |matchup.batter.fullName        |character |\n#'  |matchup.batter.link            |character |\n#'  |matchup.batSide.code           |character |\n#'  |matchup.batSide.description    |character |\n#'  |matchup.pitcher.id             |integer   |\n#'  |matchup.pitcher.fullName       |character |\n#'  |matchup.pitcher.link           |character |\n#'  |matchup.pitchHand.code         |character |\n#'  |matchup.pitchHand.description  |character |\n#'  |matchup.splits.batter          |character |\n#'  |matchup.splits.pitcher         |character |\n#'  |matchup.splits.menOnBase       |character |\n#'  |batted.ball.result             |factor    |\n#'  |home_team                      |character |\n#'  |home_level_id                  |integer   |\n#'  |home_level_name                |character |\n#'  |home_parentOrg_id              |integer   |\n#'  |home_parentOrg_name            |character |\n#'  |home_league_id                 |integer   |\n#'  |home_league_name               |character |\n#'  |away_team                      |character |\n#'  |away_level_id                  |integer   |\n#'  |away_level_name                |character |\n#'  |away_parentOrg_id              |integer   |\n#'  |away_parentOrg_name            |character |\n#'  |away_league_id                 |integer   |\n#'  |away_league_name               |character |\n#'  |batting_team                   |character |\n#'  |fielding_team                  |character |\n#'  |last.pitch.of.ab               |character |\n#'  |pfxId                          |character |\n#'  |details.trailColor             |character |\n#'  |details.type.code              |character |\n#'  |details.type.description       |character |\n#'  |pitchData.startSpeed           |numeric   |\n#'  |pitchData.endSpeed             |numeric   |\n#'  |pitchData.zone                 |integer   |\n#'  |pitchData.typeConfidence       |numeric   |\n#'  |pitchData.plateTime            |numeric   |\n#'  |pitchData.extension            |numeric   |\n#'  |pitchData.coordinates.aY       |numeric   |\n#'  |pitchData.coordinates.aZ       |numeric   |\n#'  |pitchData.coordinates.pfxX     |numeric   |\n#'  |pitchData.coordinates.pfxZ     |numeric   |\n#'  |pitchData.coordinates.pX       |numeric   |\n#'  |pitchData.coordinates.pZ       |numeric   |\n#'  |pitchData.coordinates.vX0      |numeric   |\n#'  |pitchData.coordinates.vY0      |numeric   |\n#'  |pitchData.coordinates.vZ0      |numeric   |\n#'  |pitchData.coordinates.x0       |numeric   |\n#'  |pitchData.coordinates.y0       |numeric   |\n#'  |pitchData.coordinates.z0       |numeric   |\n#'  |pitchData.coordinates.aX       |numeric   |\n#'  |pitchData.breaks.breakAngle    |numeric   |\n#'  |pitchData.breaks.breakLength   |numeric   |\n#'  |pitchData.breaks.breakY        |numeric   |\n#'  |pitchData.breaks.spinRate      |integer   |\n#'  |pitchData.breaks.spinDirection |integer   |\n#'  |hitData.launchSpeed            |numeric   |\n#'  |hitData.launchAngle            |numeric   |\n#'  |hitData.totalDistance          |numeric   |\n#'  |injuryType                     |character |\n#'  |umpire.id                      |integer   |\n#'  |umpire.link                    |character |\n#'  |isBaseRunningPlay              |logical   |\n#'  |isSubstitution                 |logical   |\n#'  |about.isTopInning              |logical   |\n#'  |matchup.postOnFirst.id         |integer   |\n#'  |matchup.postOnFirst.fullName   |character |\n#'  |matchup.postOnFirst.link       |character |\n#'  |matchup.postOnSecond.id        |integer   |\n#'  |matchup.postOnSecond.fullName  |character |\n#'  |matchup.postOnSecond.link      |character |\n#'  |matchup.postOnThird.id         |integer   |\n#'  |matchup.postOnThird.fullName   |character |\n#'  |matchup.postOnThird.link       |character |\n#' @export\n#' @examples \\donttest{\n#'   try(mlb_pbp(game_pk = 632970))\n#' }\n\nmlb_pbp &lt;- function(game_pk) {\n  \n  mlb_endpoint &lt;- mlb_stats_endpoint(glue::glue(\"v1.1/game/{game_pk}/feed/live\"))\n  \n  tryCatch(\n    expr = {\n      payload &lt;- mlb_endpoint %&gt;% \n        mlb_api_call() %&gt;% \n        jsonlite::toJSON() %&gt;% \n        jsonlite::fromJSON(flatten = TRUE)\n      \n      plays &lt;- payload$liveData$plays$allPlays$playEvents %&gt;% \n        dplyr::bind_rows()\n      \n      at_bats &lt;- payload$liveData$plays$allPlays\n      \n      current &lt;- payload$liveData$plays$currentPlay\n      \n      game_status &lt;- payload$gameData$status$abstractGameState\n      \n      home_team &lt;- payload$gameData$teams$home$name\n      \n      home_level &lt;- payload$gameData$teams$home$sport\n      \n      home_league &lt;- payload$gameData$teams$home$league\n      \n      away_team &lt;- payload$gameData$teams$away$name\n      \n      away_level &lt;- payload$gameData$teams$away$sport\n      \n      away_league &lt;- payload$gameData$teams$away$league\n      \n      columns &lt;- lapply(at_bats, function(x) class(x)) %&gt;%\n        dplyr::bind_rows(.id = \"variable\")\n      cols &lt;- c(colnames(columns))\n      classes &lt;- c(t(unname(columns[1,])))\n      \n      df &lt;- data.frame(cols, classes)\n      list_columns &lt;- df %&gt;%\n        dplyr::filter(.data$classes == \"list\") %&gt;%\n        dplyr::pull(\"cols\")\n      \n      at_bats &lt;- at_bats %&gt;%\n        dplyr::select(-c(tidyr::one_of(list_columns)))\n      \n      pbp &lt;- plays %&gt;%\n        dplyr::left_join(at_bats, by = c(\"endTime\" = \"playEndTime\"))\n      \n      pbp &lt;- pbp %&gt;%\n        tidyr::fill(\"atBatIndex\":\"matchup.splits.menOnBase\", .direction = \"up\") %&gt;%\n        dplyr::mutate(\n          game_pk = game_pk,\n          game_date = substr(payload$gameData$datetime$dateTime, 1, 10)) %&gt;%\n        dplyr::select(\"game_pk\", \"game_date\", tidyr::everything())\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::mutate(\n          matchup.batter.fullName = factor(.data$matchup.batter.fullName),\n          matchup.pitcher.fullName = factor(.data$matchup.pitcher.fullName),\n          atBatIndex = factor(.data$atBatIndex)\n          # batted.ball.result = case_when(!result.event %in% c(\n          #   \"Single\", \"Double\", \"Triple\", \"Home Run\") ~ \"Out/Other\",\n          #   TRUE ~ result.event),\n          # batted.ball.result = factor(batted.ball.result,\n          #                             levels = c(\"Single\", \"Double\", \"Triple\", \"Home Run\", \"Out/Other\"))\n        ) %&gt;%\n        dplyr::mutate(\n          home_team = home_team,\n          home_level_id = home_level$id,\n          home_level_name = home_level$name,\n          home_parentOrg_id = payload$gameData$teams$home$parentOrgId,\n          home_parentOrg_name = payload$gameData$teams$home$parentOrgName,\n          home_league_id = home_league$id,\n          home_league_name = home_league$name,\n          away_team = away_team,\n          away_level_id = away_level$id,\n          away_level_name = away_level$name,\n          away_parentOrg_id = payload$gameData$teams$away$parentOrgId,\n          away_parentOrg_name = payload$gameData$teams$away$parentOrgName,\n          away_league_id = away_league$id,\n          away_league_name = away_league$name,\n          batting_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                       .data$home_team,\n                                       .data$away_team)),\n          fielding_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                        .data$away_team,\n                                        .data$home_team)))\n      pbp &lt;- pbp %&gt;%\n        dplyr::arrange(desc(.data$atBatIndex), desc(.data$pitchNumber))\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::group_by(.data$atBatIndex) %&gt;%\n        dplyr::mutate(\n          last.pitch.of.ab =  ifelse(.data$pitchNumber == max(.data$pitchNumber), \"true\", \"false\"),\n          last.pitch.of.ab = factor(.data$last.pitch.of.ab)) %&gt;%\n        dplyr::ungroup()\n      \n      pbp &lt;- dplyr::bind_rows(baseballr::stats_api_live_empty_df, pbp)\n      \n      check_home_level &lt;- pbp %&gt;%\n        dplyr::distinct(.data$home_level_id) %&gt;%\n        dplyr::pull()\n      \n      # this will need to be updated in the future to properly estimate X,Z coordinates at the minor league level\n      \n      # if(check_home_level != 1) {\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.pX_est = predict(x_model, pbp),\n      #                   pitchData.coordinates.pZ_est = predict(y_model, pbp))\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      # }\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::rename(\n          \"count.balls.start\" = \"count.balls.x\",\n          \"count.strikes.start\" = \"count.strikes.x\",\n          \"count.outs.start\" = \"count.outs.x\",\n          \"count.balls.end\" = \"count.balls.y\",\n          \"count.strikes.end\" = \"count.strikes.y\",\n          \"count.outs.end\" = \"count.outs.y\") %&gt;%\n        make_baseballr_data(\"MLB Play-by-Play data from MLB.com\",Sys.time())\n    },\n    error = function(e) {\n      message(glue::glue(\"{Sys.time()}: Invalid arguments provided\"))\n    },\n    finally = {\n    }\n  ) \n  return(pbp)\n}\n\n#' @rdname get_pbp_mlb\n#' @title **(legacy) Acquire pitch-by-pitch data for Major and Minor League games**\n#' @inheritParams mlb_pbp\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#' @keywords legacy\n#' @export\n# get_pbp_mlb &lt;- mlb_pbp\n\n\n\n\nExample\nHere is an example using the mlb_pbp function.\n\n\nCode\nexample &lt;- (mlb_pbp(575156))\nhead(example)\n\n\n2023-10-12 13:40:46.684707: Invalid arguments provided\n\n\n\n\nA tibble: 6 x 146\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nabout.isTopInning\nmatchup.postOnFirst.id\nmatchup.postOnFirst.fullName\nmatchup.postOnFirst.link\nmatchup.postOnSecond.id\nmatchup.postOnSecond.fullName\nmatchup.postOnSecond.link\nmatchup.postOnThird.id\nmatchup.postOnThird.fullName\nmatchup.postOnThird.link\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n575156\n2019-06-01\n5\n2019-06-01T15:38:42.000Z\n2019-06-01T19:38:07.354Z\nTRUE\npitch\n05751566-0846-0063-000c-f08cd117d70a\n6\nIn play, out(s)\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n4\n2019-06-01T15:38:19.000Z\n2019-06-01T15:38:42.000Z\nTRUE\npitch\n05751566-0846-0053-000c-f08cd117d70a\n5\nFoul\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n3\n2019-06-01T15:38:02.000Z\n2019-06-01T15:38:19.000Z\nTRUE\npitch\n05751566-0846-0043-000c-f08cd117d70a\n4\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n2\n2019-06-01T15:37:45.000Z\n2019-06-01T15:38:02.000Z\nTRUE\npitch\n05751566-0846-0033-000c-f08cd117d70a\n3\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n1\n2019-06-01T15:37:31.000Z\n2019-06-01T15:37:45.000Z\nTRUE\npitch\n05751566-0846-0023-000c-f08cd117d70a\n2\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n0\n2019-06-01T15:37:15.000Z\n2019-06-01T15:37:31.000Z\nTRUE\npitch\n05751566-0846-0013-000c-f08cd117d70a\n1\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAcquiring Data\nI might use more data eventually, but for now I am scraping two series of games from the 2023 season.\n\n\nCode\nlibrary(baseballr)\n\n\nThe below code allows me to find the correct game_pk values that I can then use to pull play-by-play data. The output is hidden from this code chunk because it is very long and not necessary for understanding.\n\n\nCode\n#mlb_game_pks(\"2023-06-25\")\n# mlb_game_pks(\"2023-06-24\")\n# mlb_game_pks(\"2023-06-23\")\n\n\nThe two series of games that I am scraping are Diamondbacks/Giants and Mariners/Orioles. The game_pk values, which were acquired using the above code, are as follows: 717641, 717639, 717612, 717651, 717628, 717627. In the following chunk I will use the mlb_pbp function to pull play-by-play data for these games and save it to a csv file. You can see what this data looks like below:\n\n\nCode\nx &lt;- c(717641, 717639, 717612, 717651, 717628, 717627)\nresult &lt;- lapply(x, mlb_pbp)\ncombined_tibble &lt;- bind_rows(result)\n# Save the data to a CSV file\nwrite.csv(combined_tibble, file = \"./data/raw_data/baseballr_six_games.csv\", row.names = FALSE)\nhead(combined_tibble)\n\n\n\nA baseballr_data: 6 x 160\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nNews API\nIn Python, we can leverage the News API to retrieve textual data related to our topic, aiming to glean insights into public perspectives on streaks in sports.\n\n\nCode\nAPI_KEY='05d7ae99b5b7455191c97c2c5c3a1f9b'\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nAfter loading in relevant libraries and setting up the API, we can use the following code to retrieve the top 100 headlines related to our topic.\n\n\nCode\n#updated code\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\ndef gettingdata(TOPIC):\n    URLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n    print(baseURL)\n    print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    #print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n\n    def string_cleaner(input_string):\n        try: \n            out=re.sub(r\"\"\"\n                        [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                        \\ *           # plus zero or more copies of a space,\n                        \"\"\",\n                         \" \",          # and replace it with a single space\n                        input_string, flags=re.VERBOSE)\n\n            #REPLACE SELECT CHARACTERS WITH NOTHING\n            out = re.sub('[’.]+', '', input_string)\n\n            #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n            out = re.sub(r'\\s+', ' ', out)\n\n            #CONVERT TO LOWER CASE\n            out=out.lower()\n        except:\n            print(\"ERROR\")\n            out=''\n        return out\n    \n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    #print(\"AVAILABLE KEYS:\")\n    #print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            if(verbose):\n                print(\"----------------\")\n                print(key)\n                print(article[key])\n                print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    text = cleaned_data\n    return text\n\n\nNow we can use the above functiont to gather textual data related to the topic.\n\n\nCode\n# Obtain data using gettingdata function\nTOPIC = 'sports streak'\nhotstreak = gettingdata(TOPIC)\n\n\nFinally, let’s use the NLTK library in Python to analyze the sentiment of textual data (titles and descriptions) from the hotstreak dataset. The following code calculates sentiment scores using the VADER sentiment analysis tool and then saves the results, including titles, descriptions, and sentiment scores, into a JSON file named ‘sentiment_scores.json’. This will be useful for later analysis.\n\n\nCode\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nnltk.download('vader_lexicon')\nsia = SentimentIntensityAnalyzer()\n\nsentiment_scores = []\n\nfor text_pair in hotstreak:\n    title, description = text_pair\n    score = sia.polarity_scores(description)\n\n    sentiment_scores.append({'title': title, 'description': description, 'sentiment_score': score})\n\n\nwith open('sentiment_scores.json', 'w') as json_file:\n    json.dump(sentiment_scores, json_file, indent=4)\n\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nBelow you can see the first few rows of the newsapi.csv file:  \n\n\nIndividual Player Data\nI also want to scrape specific data from fangraphs. I downloaded a few tables that had game data for Aaron Judge and then merged them together. Later research might scrape more player data. Below are screen shots of the initial csv file.   \n\n\nExtra Joke\nHow much data can be stored in a glacier? A frostbite!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN_website",
    "section": "",
    "text": "Welcome to my website, where I’ll be sharing various projects from my time in the DSAN program at Georgetown University. Expect regular updates as I delve into new and exciting topics - stay tuned for more! While you’re here, try the snake game below (code altered from here)!"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Introduction\nThis study delves into Villanova’s 2021-22 season NCAA shot data, spotlighting six key features. Using Python and sklearn, we employ Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. This approach trims features while preserving variance, simplifying data for improved model comprehension and visualization.\n\n\nDimensionality Reduction with PCA\nPrincipal Component Analysis (PCA) is a valuable machine learning technique used to simplify large datasets by reducing their dimensionality. The primary goal is to decrease the number of variables while retaining crucial information. Explore the PCA process as I walk you through my code and showcase the corresponding output below. \n\nLoad in relevant libraries and data\n\n\nCode\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nnova = pd.read_csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# only keeping the shot data\nnova = nova.dropna(subset=['shooter'])\n\n# Creating a new column to specify the team of the shooter\nnova['shooter_team'] = np.where(nova['action_team'] == \"home\", nova['home'], nova['away'])\n\n# only keeping the villanova shots\nnova = nova[nova['shooter_team'] == 'Villanova']\n\n# changing shot outcome to numeric\nnova['shot_outcome_numeric'] = nova['shot_outcome'].apply(lambda x: 1 if x == 'made' else 0)\n\n\n\n\nCode\n#creating a new column called shot value\nnova['shot_value'] = 2  # Default value for shots that are not free throws or three-pointers\nnova.loc[nova['free_throw'], 'shot_value'] = 1\nnova.loc[nova['three_pt'], 'shot_value'] = 3\n\n# Calculate the mean of shot_outcome for each player (field goal percentage)\nmean_and_count_data = nova.groupby('shooter').agg(\n    shots=('shot_outcome', 'count'),\n    field_goal_percentage=('shot_outcome_numeric', lambda x: x[x == 1].count() / len(x) if len(x) &gt; 0 else 0)\n).sort_values(by='shots', ascending=False)\n\n# Add the calculated field goal percentage to the original DataFrame\nnova = nova.merge(mean_and_count_data[['field_goal_percentage']], left_on='shooter', right_index=True, how='left').round(4)\n\n# create a lag variable for the previous shot (1 indicates made shot, -1 indicates miss, 0 indicates no previous shot in half\nnova = nova.sort_values(by=['shooter', 'game_id', 'play_id'])  # Arrange the data by shooter, game_id, and play_id\nnova['lag1'] = nova.groupby(['shooter', 'game_id'])['shot_outcome_numeric'].shift(1)\nnova['lag1'] = nova['lag1'].replace({0: -1}).fillna(0)  # Replace initial 0 values with -1, and NaN values with 0\nnova = nova.sort_values(by=['game_id', 'play_id'])\n\n# reset the index\nnova = nova.reset_index(drop=True)\n\n# create a new column for the home crowd\nnova['home_crowd'] = (nova['home'] == 'Villanova').astype(int)\n\n# create a new column for the game number in the season\nnova['game_num'] = nova['game_id'].astype('category').cat.codes + 1\n\nnova.head()\n\n\n\n\n\n\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\npossession_before\npossession_after\nwrong_time\nshooter_team\nshot_outcome_numeric\nshot_value\nfield_goal_percentage\nlag1\nhome_crowd\ngame_num\n\n\n\n\n0\n401365747\n2021-11-28\nLa Salle\nVillanova\n4\n1\n19:22\n2362\n2362\nJustin Moore missed Three Point Jumper.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n3\n0.4721\n0.0\n0\n1\n\n\n1\n401365747\n2021-11-28\nLa Salle\nVillanova\n13\n1\n18:32\n2312\n2312\nEric Dixon missed Dunk.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n2\n0.5794\n0.0\n0\n1\n\n\n2\n401365747\n2021-11-28\nLa Salle\nVillanova\n16\n1\n18:18\n2298\n2298\nCollin Gillespie made Three Point Jumper.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n3\n0.5337\n0.0\n0\n1\n\n\n3\n401365747\n2021-11-28\nLa Salle\nVillanova\n18\n1\n17:35\n2255\n2255\nEric Dixon made Layup.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5794\n-1.0\n0\n1\n\n\n4\n401365747\n2021-11-28\nLa Salle\nVillanova\n21\n1\n16:59\n2219\n2219\nJermaine Samuels made Layup. Assisted by Eric ...\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5535\n0.0\n0\n1\n\n\n\n\n5 rows × 46 columns\n\n\n\n\n\nCode\n# subsetting my data into feature varaibles and target variable\nfeature_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num']\nnova_features = nova[feature_columns].copy()\n\ntarget_column = ['shot_outcome_numeric']\nnova_target = nova[target_column].copy()\n\nall_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num', 'shot_outcome_numeric']\nnova_final = nova[all_columns].copy()\n\n# save feature_columns to csv for clustering\nnova_features.to_csv('./data/modified_data/nova_features.csv', index=False)\n\n# save nova_final to csv for decision trees\nnova_final.to_csv('./data/modified_data/nova_final.csv', index=False)\n\n\nnova_features.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2764 entries, 0 to 2763\nData columns (total 6 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   shot_value             2764 non-null   int64  \n 1   field_goal_percentage  2764 non-null   float64\n 2   lag1                   2764 non-null   float64\n 3   home_crowd             2764 non-null   int64  \n 4   score_diff             2764 non-null   int64  \n 5   game_num               2764 non-null   int8   \ndtypes: float64(2), int64(3), int8(1)\nmemory usage: 110.8 KB\n\n\n\n\nStandardization\nNormalize data to have a mean of 0 and a standard deviation of 1.\n\n\nCode\n# Standardization\nscaler = StandardScaler()\nnova_features_standardized = scaler.fit_transform(nova_features)\n\n\n\n\nCovariance Matrix Computation\nCalculate the covariance matrix to understand variable relationships.\n\n\nCode\n#covariance matrix\nco_ma = np.cov(nova_features_standardized, rowvar=False)\nprint(co_ma)\n\n\n[[ 1.00036193 -0.1413293  -0.16465368  0.00400472  0.0018319   0.00327538]\n [-0.1413293   1.00036193  0.08216798 -0.05311615 -0.04587765 -0.01289989]\n [-0.16465368  0.08216798  1.00036193  0.04351646  0.04402943 -0.00288759]\n [ 0.00400472 -0.05311615  0.04351646  1.00036193  0.49188691  0.31830859]\n [ 0.0018319  -0.04587765  0.04402943  0.49188691  1.00036193  0.12171352]\n [ 0.00327538 -0.01289989 -0.00288759  0.31830859  0.12171352  1.00036193]]\n\n\n\n\nComputing Eigenvectors and Eigenvalues\nIdentify principal components using eigenvectors and eigenvalues.\n\n\nCode\n#eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(co_ma)\nprint(\"Eigenvalues\\n\",\"----------------------\")\nprint(eigenvalues)\nprint(\"\\nEigenvectors\\n\",\"----------------------\")\nprint(eigenvectors)\n\n\nEigenvalues\n ----------------------\n[1.65668919 1.26200685 0.46449209 0.81824262 0.8669812  0.9337596 ]\n\nEigenvectors\n ----------------------\n[[-0.01233438  0.63776031  0.00198086 -0.75043735 -0.17142614  0.02371907]\n [ 0.09828166 -0.51666678 -0.01569294 -0.30589464 -0.50573557  0.61139994]\n [-0.06667471 -0.57041517  0.01469836 -0.56821948  0.30014023 -0.50695901]\n [-0.66744252 -0.01515272 -0.73678     0.0201096  -0.1050718  -0.00127727]\n [-0.5923083  -0.02488482  0.60566041  0.08768672 -0.46111118 -0.24781971]\n [-0.43524065  0.00974191  0.29977402 -0.11093025  0.63332208  0.55425972]]\n\n\n\n\nFeature Vectors\nSelect eigenvectors as new feature vectors.\n\n\nCode\n# choosing principal components\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigenvalues)[::-1]\nsorted_eigenvalue = eigenvalues[sorted_index]\n\n\n\n\nRecasting Data Among Principal Component Axis\nTransform data using chosen principal components.\n\n\nCode\n# PCA with components decided above\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\ndesired_variance = 0.75 \nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\n\npca = PCA(n_components=num_components)\nnova_pca = pca.fit_transform(nova_features_standardized)\n\n\n\n\nDeciding optimal number of components\nTo decide the optimal number of components, we can use both a cumulative explained variance plot and a scree plot to visualize explained variance ratio.\n\n\nCode\n# Cumulative Explained Variance Plot\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', color='#FFB6C1')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Scree Plot\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(\"Explained Variance Ratio for Each Component:\")\nprint(explained_variance_ratio)\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='#FFB6C1')\nplt.title('Scree Plot')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n\n# find the number of variables it takes to reach a variance of 0.75\ndesired_variance = 0.75\nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\nprint(f\"Number of components to capture {desired_variance * 100}% variance: {num_components}\")\n\n\nExplained Variance Ratio for Each Component:\n[0.27601497 0.21025838 0.1555703  0.14444459]\nNumber of components to capture 75.0% variance: 4\n\n\n\n\n\nAs a general guideline, the goal is to retain at least 80% of the variance. However, given the relatively small size of our dataset, we have adjusted the threshold to 75%. Therefore, we will select 4 components, ensuring the cumulative explained variance surpasses 75%.\n\n\nVisualizing reduced-dimensional data\nNow, let’s visualize the reduced-dimensional data using a scatter plot of the first two principal components.\n\n\nCode\n# pca scatter plot\nplt.scatter(nova_pca[:, 0], nova_pca[:, 1], alpha=0.5, color='#D8BFD8')\nplt.title('PCA Scatter Plot')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n# limit PCA to 4 components\npca = PCA(n_components=4)\n\n# save nova_pca to csv\nnova_pca_df = pd.DataFrame(nova_pca)\nnova_pca_df.to_csv('./data/modified_data/nova_pca.csv', index=False)\n\n\n\n\n\nThe scree plot guides us in determining that capturing 75% of the variance necessitates employing four principal components. The scatter plot, showcasing the reduced-dimensional data, visually represents patterns within the dataset. You many notice that a seperation occurs where Principal Component 1 equals 0. While PCA excels at identifying linear relationships, it’s important to acknowledge that observations with higher variability may be distant from the main cluster. These steps underscore how PCA simplifies dimensionality reduction, fostering a deeper understanding of the dataset. It’s worth noting that the sklearn library’s PCA function automates these procedures for ease of implementation.\n\n\n\nDimensionality Reduction with t-SNE\nt-SNE, or t-distributed Stochastic Neighbor Embedding, is an unsupervised non-linear dimensionality reduction technique designed to explore and visualize high-dimensional data. It transforms complex datasets into a lower-dimensional space, emphasizing preserving local relationships among data points. By finding similarity measures between pairs of instances in higher and lower dimensional spaces and optimizing these measures, t-SNE enhances our ability to interpret intricate datasets.\nAdditionally, exploring clustering in this context allows me to identify distinct groups or patterns within the NCAA shot data. By combining t-SNE, a dimensionality reduction technique, with KMeans clustering, I can uncover and visualize natural structures or associations in the dataset. The choice of three clusters is informed by the results obtained on the clustering page. Exploring different perplexity values enhances the flexibility of my analysis, helping me discover nuanced patterns at varying levels of detail.\n\n\nCode\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport pandas as pd\n\ndef explore_tsne(perplexity_value):\n    X = nova_features.iloc[:, :]\n\n    # t-SNE for 3 dimensions with different perplexity\n    tsne = TSNE(n_components=3, random_state=1, perplexity=perplexity_value)\n    X_tsne = tsne.fit_transform(X)\n\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    clusters = kmeans.fit_predict(X)\n\n    # Create a DataFrame with 3D data\n    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2', 'Dimension 3'])\n    tsne_df['Cluster'] = clusters\n\n    # Interactive 3D scatter plot with plotly\n    fig = px.scatter_3d(tsne_df, x='Dimension 1', y='Dimension 2', z='Dimension 3',\n                        color='Cluster', symbol='Cluster', opacity=0.7, size_max=10,\n                        title=f't-SNE 3D Visualization (Perplexity={perplexity_value})',\n                        labels={'Cluster': 'Cluster'})\n\n    # Show the plot\n    fig.show()\n\n# Explore t-SNE with different perplexity values\nperplexities = [5, 20, 40]  # Add more values as needed\nfor perplexity_value in perplexities:\n    explore_tsne(perplexity_value)\n\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nPerplexity in t-SNE determines the balance between capturing local and global relationships in the data’s low-dimensional representation. Lower perplexity values focus on local details, higher values emphasize global structures, while moderate values strike a balance. Experimenting with different perplexity values helps find an optimal configuration for visualizing and understanding the dataset. As I explored various perplexity values, I noticed that with larger perplexity values, clusters became more distinct, revealing clearer patterns and structures within the data. This observation underscores the importance of choosing an appropriate perplexity value for the specific characteristics of the dataset, ultimately enhancing the effectiveness of t-SNE in revealing underlying structures.\n\n\nEvaluation & Comparison\nIn summary, PCA efficiently preserves the overall structure, making it well-suited for large datasets with linear relationships. Conversely, t-SNE excels at unveiling local structures and clusters, offering enhanced visualization for smaller datasets. The decision between these techniques hinges on factors like dataset size, structure, and specific analysis goals.\nIn my analysis, it became evident that certain variables play a crucial role in explaining most of the variance in our dataset. Despite having only six feature variables, retaining four allows us to preserve over 75% of the variance, indicating limited redundancy. The application of t-SNE for cluster visualization proved insightful, revealing subtle overlaps within the clusters. This aligns with previous observations, reinforcing that identifying distinct clusters in this dataset poses challenges.\n\n\nExtra Joke\nIf we were compressed down to a single dimension… what would be the point of it all?"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "three more to go!"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction\nOn this page, I’ll employ three clustering techniques to analyze Villanova’s 2021-22 NCAA shot data (the same data from the dimensionality-reduction tab), considering all six features: lag1 (previous shot), shot_value, field_goal_percentage, game_num, home_crowd, and score_diff. The primary objective is to uncover patterns within the dataset through k-means, DBSCAN, and hierarchical clustering. Each step of the process will be explained in a straightfowrad manner with interpretations of the results.\n\nImport libraries and load the dataset\n\n\nCode\n# import the necessary packages...\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# read in the feature dataset\ndf = pd.read_csv('./data/modified_data/nova_features.csv')\n\n\n\n\n\nKMeans\n\nTheory\nK-Means clustering, a widely recognized algorithm, is valued for its simplicity and effectiveness, making it particularly appealing in various applications. This technique involves grouping data points into ‘k’ clusters, where ‘k’ is a user-specified parameter. The algorithm starts by assigning random points to these clusters, with centroids acting as their centers. Distances, usually calculated using Euclidean distance, guide the process. Iteratively, points shift between clusters, and new centroids emerge through Lloyd’s algorithm until no better cluster assignments are feasible.  K-Means’ core lies in optimizing the sum of squared distances between data points and their assigned cluster mean. The choice of ‘k’ dictates the number of centroids, which represent cluster centers. The algorithm strategically redistributes points to minimize the in-cluster sum of squares. Its simplicity and widespread usage make it a fundamental tool in unsupervised machine learning, showcasing its ability to uncover inherent patterns in data.  In a broader context, K-Means clustering serves as a technique to group data points based on their similarity to an average grouping. Distance metrics, such as Euclidean or Manhattan distance, play a pivotal role, and normalizing input data becomes crucial for robust performance. Centroids, serving as the algorithm’s starting point, evolve through successive iterations, refining cluster assignments and centroids’ positions. The algorithm’s convergence reveals meaningful clusters, transforming data chaos into structured insights. Implementation through sklearn’s KMeans algorithm enhances efficiency, and feature selection enables practitioners to customize the clustering process to their data’s nuances.\n\n\nImplementation\n\n\nCode\n# import relevent libraries for clustering. we will use KMeans, AgglomerativeClustering, MeanShift, Birch, and DBSCAN\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.cluster import MeanShift, Birch\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport random\n\n\nThe below code performs K-Means clustering for different numbers of clusters (ranging from 2 to 10). For each cluster number, it calculates and stores the distortion, which measures the average Euclidean distance between each data point and its assigned cluster center. Inertia is also computed and stored, representing the sum of squared distances of data points to their closest cluster center. Additionally, the silhouette score is determined and recorded, offering insights into how well-defined and separated the clusters are. The results, encompassing these crucial metrics—distortion, inertia, and silhouette score—are then organized into a DataFrame for detailed analysis and printed. This is in an attempt to identify the optimal number of clusters based on a comprehensive evaluation of these key clustering indicators.\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create empty lists to store the results\nclus = []\ndistortions = []\ninertias = []\nsilhouette_scores = []\n\n# Loop through the range of clusters\nfor i in range(2, 11):  # Silhouette score is not defined for a single cluster\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(df)  \n    clus.append(i)\n    centers = kmeans.cluster_centers_\n    distortions.append(sum(np.min(pairwise_distances(df, centers, metric='euclidean'), axis=1)) / df.shape[0])\n    inertias.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(df, kmeans.labels_))\n\n# Create a DataFrame from the lists\nresults = pd.DataFrame({'Cluster': clus, 'Distortion': distortions, 'Inertia': inertias, 'Silhouette Score': silhouette_scores})\n\nprint(results)\n\n\n   Cluster  Distortion        Inertia  Silhouette Score\n0        2   10.622510  447410.233953          0.372238\n1        3    8.828917  272970.775541          0.408870\n2        4    7.974259  215311.617146          0.390174\n3        5    6.985968  163526.599625          0.380241\n4        6    6.394550  134984.307974          0.368484\n5        7    5.753101  113388.811614          0.384348\n6        8    5.309537   94219.154891          0.403127\n7        9    5.020084   83855.750340          0.403238\n8       10    4.698719   75156.752577          0.386414\n\n\nAmong the tested cluster counts (2 to 10), the silhouette score, a measure of cluster quality, is highest when there are three clusters. A silhouette score close to 1 indicates well-separated clusters. In this case, the silhouette score peaks at three clusters, suggesting that the data is most naturally organized into this number of distinct groups. This finding signifies a meaningful and clear grouping in the data, enabling better understanding and interpretation of underlying patterns.  The subsequent block of code generates three plots: one for distortion, one for inertia, and one for the silhouette score. Each plot has the number of clusters on the x-axis and the corresponding metric on the y-axis. These visualizations help interpret the results, allowing for the identification of trends and patterns.\n\n\nCode\n# Create subplots with 1 row and 3 columns\nfig, ax = plt.subplots(1, 3, figsize=(18, 4))\n\n# Plot Distortion\nax[0].plot(results['Cluster'], results['Distortion'], marker='o')\nax[0].set_title('Distortion')\nax[0].set_xlabel('Cluster')\nax[0].set_ylabel('Distortion')\n\n# Plot Inertia\nax[1].plot(results['Cluster'], results['Inertia'], marker='o')\nax[1].set_title('Inertia')\nax[1].set_xlabel('Cluster')\nax[1].set_ylabel('Inertia')\n\n# Plot Silhouette Score\nax[2].plot(results['Cluster'], results['Silhouette Score'], marker='o')\nax[2].set_title('Silhouette Score')\nax[2].set_xlabel('Cluster')\nax[2].set_ylabel('Silhouette Score')\n\n# Display the side-by-side plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe analysis of the graphs and results indicates that opting for 3 clusters is appropriate. The elbow and silhouette score methods suggest that this choice effectively captures meaningful patterns within the data. Having 3 clusters strikes a balance between simplicity and preserving relevant information. The subsequent 3D scatter plot visually represents the clustered data points in a three-dimensional space, offering a comprehensive view of the results.\n\n\nCode\npca_result = pd.read_csv('./data/modified_data/nova_pca.csv')\n\nimport plotly.express as px\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(df)\n\n# Get cluster labels\nlabels = kmeans.labels_\n\n# Assuming pca_result has columns '0', '1', and '2'\nfig = px.scatter_3d(pca_result, x='0', y='1', z='2', color=labels, symbol=labels, opacity=0.7,\n                    size_max=10, title='3D Scatter Plot of PC1, PC2, and PC3 with Cluster Labels',\n                    labels={'0': 'Principal Component 1 (PC1)',\n                            '1': 'Principal Component 2 (PC2)',\n                            '2': 'Principal Component 3 (PC3)',\n                            'color': 'Cluster'},\n                    )\nfig.show()\n\n\n\n                                                \n\n\n\n\n\nDBSCAN\n\nTheory\nDBSCAN, or Density-Based Spatial Clustering of Applications with Noise, is a robust clustering algorithm that distinguishes clusters based on the density of data points, incorporating both distance metrics and a minimum number of points. Unlike K-Means, DBSCAN doesn’t require users to specify the number of clusters (‘k’) in advance; instead, it dynamically identifies clusters by expanding neighborhoods around data points.  The algorithm starts by randomly selecting and expanding a neighborhood around a data point. If the density within this neighborhood is sufficient, a cluster is formed, and the process iterates until no more data points can be added. Outliers are identified as points in low-density regions. DBSCAN’s strength lies in its ability to uncover clusters of arbitrary shapes and sizes, making it particularly valuable for datasets where the number of clusters is unknown.  DBSCAN requires two key parameters: epsilon (ε) and minimum samples. Epsilon defines the maximum distance between two points for them to be considered in the same cluster, while minimum samples specify the minimum number of points required in each cluster. Experimentation is often needed to find optimal parameters, as the algorithm’s outcome is sensitive to these choices.  In summary, DBSCAN offers a unique approach to clustering, emphasizing data density over distances to centroids. Its flexibility in identifying clusters of varying shapes and the ability to mark outliers enhances its effectiveness in exploring complex datasets. Implementation through sklearn’s DBSCAN module provides a practical means to apply this algorithm to diverse datasets.\n\n\nImplementation\nThe below code performs an exhaustive search for optimal parameters (epsilon and minimum samples) for the DBSCAN algorithm. It calculates silhouette scores for different combinations of epsilon values (z1) and minimum sample sizes (z2), aiming to identify the best configuration that yields the highest silhouette score and the corresponding number of clusters. The resulting dataframe, df1, is then printed and visualized with a line plot.\n\n\nCode\nbest_scores = []\neps = []\nclus = []\nz1 = [i / 10 for i in range(5, 20)]\nz2 = range(2, 10) # explain why 2 to 10 or just do 1 to 10 but then u have to fix smth in the code if i dont remember wrong. i suggest explaining is a common assumption to do here\n\nfor i in z1:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in z2:\n        model = DBSCAN(eps=i, min_samples=j)\n        predics = model.fit_predict(df)\n        num_clusters = len(pd.Series(predics).unique())\n        if num_clusters &gt; 1:\n            score = silhouette_score(df, predics)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    clus.append(best_cluster)\n    eps.append(best_eps)\n\ndf1 = pd.DataFrame({'Epsilons': eps, 'Best_Clusters': clus, 'Best_Silhouette': best_scores})\nprint(df1.sort_values(by=\"Best_Silhouette\", ascending=False))\nsns.lineplot(data=df1, x='Best_Clusters',y='Best_Silhouette')\nplt.show()\n\n\n    Epsilons  Best_Clusters  Best_Silhouette\n0        0.5            595         0.065783\n1        0.6            595         0.065783\n2        0.7            595         0.065783\n3        0.8            595         0.065783\n4        0.9            595         0.065783\n5        1.0            456         0.020257\n10       1.5             33        -0.029439\n11       1.6             33        -0.029439\n12       1.7             33        -0.029439\n6        1.1            216        -0.040136\n7        1.2            216        -0.040136\n8        1.3            216        -0.040136\n9        1.4            216        -0.040136\n13       1.8             22        -0.117656\n14       1.9             22        -0.117656\n\n\n\n\n\nThe results suggest that varying epsilon values from 0.5 to 0.9 consistently yield a high number of clusters (595) with a low silhouette score (0.065783). This pattern persists, indicating that DBSCAN struggles to identify distinct clusters, possibly due to the nature of the data. The diminishing silhouette scores as epsilon increases, coupled with the large number of clusters, may imply that this clustering method is not well-suited for the dataset. To validate this observation, hierarchical clustering and visualization through T-SNE plots will be explored for further confirmation.\n\n\n\nHierarchical Clustering (Agglomerative Clustering)\n\nTheory\nHierarchical Clustering, also known as Agglomerative Clustering, is a versatile algorithm that builds a hierarchy of clusters without the need to predefine the number of clusters. It treats each data point individually and progressively merges the closest clusters until forming a single cluster, using methods like Ward’s method to calculate distances. The resulting linkage matrix constructs a dendrogram, enabling visualization of clustering hierarchy and facilitating the choice of cluster count. Normalizing input data is crucial for meaningful results, and the choice of linkage method, such as ward linkage, influences cluster formation.  Hierarchical Clustering focuses on finding clusters based on distance, repeatedly finding the two closest points and forming clusters until all points are assigned. The algorithm is sensitive to distance, requiring multiple runs with different distance values, and scaling the dataset to mitigate outlier effects. This method’s adaptability and visualization through a dendrogram make it valuable for exploring data structures.\n\n\nImplementation\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward') #chose 3 as that is the number of species. We could have changed it.\nlabels = hierarchical_cluster.fit_predict(df)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\nCluster Labels total:\n[0, 1, 2]\n\n\nLet’s generates a dendrogram for Agglomerative Clustering, visualizing the hierarchical linkage between data points.\n\n\nCode\n# create linkage for agglomerative clustering, and the dendrogram for the linkage. Suggest the optimal number of clusters based on the dendrogram.\nlinkage_matrix = linkage(df, method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', labels=labels, distance_sort='ascending', show_leaf_counts=True)\nplt.show()\n\n\n\n\n\nThe below code defines a function that performs hierarchical clustering on input data, varying the number of clusters. It calculates silhouette scores for each clustering iteration and outputs the optimal number of clusters that maximizes the silhouette score, in addition to plotting a graph showing how the silhouette score changes with different cluster numbers. The last lines of code apply this function to the data.\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.cluster\n\ndef maximize_silhouette(X, algo=\"ag\", nmax=None, i_plot=False):\n    # PARAM\n    i_print = False\n\n    # FORCE CONTIGUOUS\n    X = np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params = []\n    sil_scores = []\n    sil_max = -10\n\n    for param in range(2, nmax + 1):\n        if algo == \"ag\":\n            model = AgglomerativeClustering(n_clusters=param).fit(X)\n            labels = model.labels_\n            \n            try:\n                sil_scores.append(silhouette_score(X, labels))\n                params.append(param)\n            except ValueError:\n                continue\n\n            if i_print:\n                print(param, sil_scores[-1])\n\n            if sil_scores[-1] &gt; sil_max:\n                opt_param = param\n                sil_max = sil_scores[-1]\n                opt_labels = labels\n\n    print(\"Maximum Silhouette score =\", sil_max)\n    print(\"OPTIMAL CLUSTERS (btwn 2-10) =\", opt_param)\n\n    if i_plot:\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n# Example usage:\nopt_labels = maximize_silhouette(df, algo=\"ag\", nmax=10, i_plot=True)\n\n\nMaximum Silhouette score = 0.3971383731982046\nOPTIMAL CLUSTERS (btwn 2-10) = 3\n\n\n\n\n\nThe results from the code suggest that, within the considered range of 2 to 10 clusters, the maximum silhouette score for agglomerative clustering is achieved at 3 clusters, indicating it as the optimal number of clusters. This aligns with the observed trend in K-Means clustering, where both methods highlight 3 clusters as optimal based on the silhouette score.\n\n\n\nConclusions\nThe analysis using K-Means and hierarchical clustering both implied the presence of three clusters within the dataset, providing a consistent pattern. However, DBSCAN did not seem well-suited for this dataset, suggesting varying performance across clustering methods. This indicates the potential for meaningful clustering, and further exploration in the Dimensionality Reduction tab may unveil clearer insights into the distinctive patterns within the data.\n\n\nExtra Joke\nMovie Pitch: It’s a movie about high school girls trying to figure out what clique they belong in. They move from clique to clique and eventually stop when they minimize their differences. It’s called K-Means girls."
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Introduction\nOn this page, our attention turns to the implementation of Decision Trees and Random Forest using the same NCAA shot data from the 2021-22 Villanova season that was utilized in the clustering and dimensionality reduction tabs. We’ll start by loading relevant libraries and reading in the data.\n\n\nCode\n# import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# read in the data\ndf = pd.read_csv('./data/modified_data/nova_final.csv')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2764 entries, 0 to 2763\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   shot_value             2764 non-null   int64  \n 1   field_goal_percentage  2764 non-null   float64\n 2   lag1                   2764 non-null   float64\n 3   home_crowd             2764 non-null   int64  \n 4   score_diff             2764 non-null   int64  \n 5   game_num               2764 non-null   int64  \n 6   shot_outcome_numeric   2764 non-null   int64  \ndtypes: float64(2), int64(5)\nmemory usage: 151.3 KB\n\n\n\n\nBaseline Comparisons\nLet’s start by examining the distribution of class labels, specifically distinguishing between made and missed shots:\n\n\nCode\n# Compute the distribution of class labels\nclass_distribution = df['shot_outcome_numeric'].value_counts()\n\n# Print the distribution\nprint(\"Class Distribution:\")\nprint(class_distribution)\n\n\nClass Distribution:\n1    1448\n0    1316\nName: shot_outcome_numeric, dtype: int64\n\n\n\n\nCode\n# Plot the class distribution\nplt.figure(figsize=(8, 6))\nsns.countplot(x='shot_outcome_numeric', data=df, palette='viridis')\nplt.title('Class Distribution of shot_outcome_numeric')\nplt.xlabel('Class Label')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\nThere is a slight imbalance in class distribution for the shot_outcome_numeric target variable (class label). There are 1448 instances of ‘made shot’ (Class Label ‘1’) and 1316 instances of ‘missed shot’ (Class Label ‘0’). This class imbalance can influence the performance of the classification model. It’s essential to consider metrics beyond accuracy, such as precision and recall of the data, to gain a comprehensive understanding. Additionally, it’s worth noting that a model predicting ‘1’ for every instance would achieve 52.4% accuracy, so any model with an accuracy below this threshold does not provide meaningful predictions. Below is code that runs a random classifier on the data to see if it can beat the 52.4% accuracy threshold.\n\n\nCode\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Random Classifier function\ndef random_classifier(y_data):\n    ypred = np.random.randint(2, size=len(y_data))  # Random predictions (0 or 1)\n    \n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"Count of Predictions:\", Counter(ypred).values())\n    print(\"Probability of Predictions:\", np.fromiter(Counter(ypred).values(), dtype=float) / len(y_data))\n    \n    accuracy = accuracy_score(y_data, ypred)\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_data, ypred)\n    \n    print(\"Accuracy:\", accuracy)\n    print(\"Precision (Class 0, Class 1):\", precision)\n    print(\"Recall (Class 0, Class 1):\", recall)\n    print(\"F1-score (Class 0, Class 1):\", fscore)\n\n# Using the 'shot_outcome_numeric' column as labels\ny = df['shot_outcome_numeric']\n\n# Running the random classifier\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\nCount of Predictions: dict_values([1357, 1407])\nProbability of Predictions: [0.49095514 0.50904486]\nAccuracy: 0.5032561505065123\nPrecision (Class 0, Class 1): [0.47974414 0.52763449]\nRecall (Class 0, Class 1): [0.51291793 0.49447514]\nF1-score (Class 0, Class 1): [0.49577672 0.51051693]\n\n\nThe random classifier acts as a rudimentary model that makes predictions through random guessing. Its primary role is to provide a fundamental benchmark for assessing the efficacy of more sophisticated models. Specifically applied to predicting basketball shot outcomes, this classifier yields an accuracy of approximately 50%, falling short of consistently predicting ‘1’ (as established earlier). When evaluating advanced models, the goal is for them to surpass this random guessing baseline (and the 52.4% baseline from always guessing the most common class), demonstrating their ability to discern meaningful patterns within the data.\n\n\nDecision Tree\n\nTheory\nDecision Trees are powerful machine learning algorithms used for classification and regression tasks, providing a hierarchical structure whereby nodes represent decisions based on specific features. The dataset is recursively split at decision nodes using criteria like Gini impurity for classification. Each leaf node corresponds to a class label in classification or a numerical value in regression, making the final predictions.  The simplicity and interpretability of Decision Trees make them widely applicable, but their susceptibility to overfitting necessitates techniques like pruning. Pruning involves removing non-contributive parts of the tree. Decision Trees offer insights into feature importance, with features near the top of the tree contributing more to predictive power.  Applying Decision Trees to real-world scenarios involves defining rules for classification based on the dataset’s features. These rules provide a transparent decision-making process, making Decision Trees particularly useful for explaining predictions to non-experts. In the context of the basketball data I have been using, Decision Trees could form rules like “if the shot value is less than 2, classify it as a made shot.” Feature selection and hyperparameter tuning further refine these trees to better fit the data, exemplifying their adaptability to various applications. The Gini Index is a common measure used in Decision Trees to determine optimal splits, minimizing impurity and enhancing predictive accuracy. Despite their advantages, Decision Trees’ potential overfitting is mitigated by minimizing the number of layers or employing advanced techniques.\n\n\nImplementation\nIn the following code, we begin by importing the necessary libraries. Subsequently, we partition the data into subsets, distinguishing features and the target variable. Further, we perform another split on the data into training and test sets, allowing us to train a Decision Tree model on the training data and assess its performance using the test data. We then print the accuracy value of this model on the test data.\n\n\nCode\n# laod in relevant libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics \n\n# split data in features and target variable\nfeature_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num']\nX = df[feature_columns].copy()\n\ntarget_column = ['shot_outcome_numeric']\nY = df[target_column].copy()\n\n# Split data into training and test set\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # 70% training and 30% test\n\n# Create Decision Tree classifer object\nmodel = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nmoodel = model.fit(X_train,Y_train)\n\n#Predict the response for test dataset\nY_pred = model.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n\n\nAccuracy: 0.5590361445783133\n\n\nThis decision tree did better than both baseline comparisons (both the random classifier and always guessing the most common class label). However, it is still not a great model; the accuracy only slightly improved to 56%. Let’s visualize the model below.\n\n\nCode\nfrom sklearn.tree import plot_tree\n\ndef custom_plot_tree(model, X, Y):\n    plt.figure(figsize=(22.5, 15))\n    plot_tree(model, feature_names=X.columns, filled=True)\n    plt.show()\n\ncustom_plot_tree(model, X_train, Y_train)\n\n\n\n\n\nThe initial decision tree above seems excessively large, indicating potential overfitting to the data. To address this, we’ll adjust the hyperparameters below.\n\n\nHyper-Parameter Tuning\nThe provided code iterates through different values for the hyperparameter (number of layers) and generates three plots to visualize the model’s performance. The plots show the accuracy and recall for both training and test datasets, with varying numbers of layers in the Decision Tree (controlled by the max_depth hyperparameter). The blue lines represent training results, while the red lines depict test results.\n\n\nCode\nfrom sklearn.metrics import accuracy_score, recall_score\n\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train, Y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(Y_test, yp_test),recall_score(Y_test, yp_test,pos_label=0),recall_score(Y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(Y_train, yp_train),recall_score(Y_train, yp_train,pos_label=0),recall_score(Y_train, yp_train,pos_label=1)])\n\n# Extracting data\nnum_layers = [result[0] for result in test_results]\ntrain_accuracy_values = [result[1] for result in train_results]\ntest_accuracy_values = [result[1] for result in test_results]\ntrain_recall_0_values = [result[2] for result in train_results]\ntest_recall_0_values = [result[2] for result in test_results]\ntrain_recall_1_values = [result[3] for result in train_results]\ntest_recall_1_values = [result[3] for result in test_results]\n\n# Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_accuracy_values, label='Train Accuracy', marker='o', color='blue')\nplt.plot(num_layers, test_accuracy_values, label='Test Accuracy', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Recall Y = 0\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_0_values, label='Train Recall y=0', marker='o', color='blue')\nplt.plot(num_layers, test_recall_0_values, label='Test Recall y=0', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall Y = 0')\nplt.legend()\nplt.show()\n\n# Recall Y = 1\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_1_values, label='Train Recall y=1', marker='o', color='blue')\nplt.plot(num_layers, test_recall_1_values, label='Test Recall y=1', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall Y = 1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Optimal Model\nBased on the graphs, it appears that an optimal model would have three layers (a depth of 3). Consequently, we will re-train the decision tree using this optimal hyperparameter obtained from the plot above in the code below. Additionally, in the code chunk we will define and use a function that generates a confusion matrix plot, display metrics, and decision tree.\n\n\nCode\nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(X_train, Y_train)\n\nyp_train=model.predict(X_train)\nyp_test=model.predict(X_test)\n\n#function which generates a confusion matrix plot and prints information\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndef confusion_plot(y_true, y_pred):\n    # metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, _, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n\n    # plot\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    matrix = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_true))\n    matrix.plot(cmap=plt.cm.Blues)\n\n    # formatting\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {recall[0]}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {precision[0]}\")\n    print(f\"POSITIVE RECALL (Y=1): {recall[1]}\")\n    print(f\"POSITIVE PRECISION (Y=1): {precision[1]}\")\n    print(cm)\n\nconfusion_plot(Y_test,yp_test)\nplt.show()\ncustom_plot_tree(model,X,Y)\n\n\nACCURACY: 0.655421686746988\nNEGATIVE RECALL (Y=0): 0.7300771208226221\nNEGATIVE PRECISION (Y=0): 0.610752688172043\nPOSITIVE RECALL (Y=1): 0.5895691609977324\nPOSITIVE PRECISION (Y=1): 0.7123287671232876\n[[284 105]\n [181 260]]\n\n\n\n\n\n\n\n\n\n\nCode\ntree_summary = tree.export_text(model, feature_names=X_train.columns.tolist())\nprint(tree_summary)\n\n\n|--- shot_value &lt;= 1.50\n|   |--- score_diff &lt;= 10.50\n|   |   |--- game_num &lt;= 11.50\n|   |   |   |--- class: 1\n|   |   |--- game_num &gt;  11.50\n|   |   |   |--- class: 1\n|   |--- score_diff &gt;  10.50\n|   |   |--- game_num &lt;= 17.50\n|   |   |   |--- class: 1\n|   |   |--- game_num &gt;  17.50\n|   |   |   |--- class: 1\n|--- shot_value &gt;  1.50\n|   |--- shot_value &lt;= 2.50\n|   |   |--- field_goal_percentage &lt;= 0.54\n|   |   |   |--- class: 0\n|   |   |--- field_goal_percentage &gt;  0.54\n|   |   |   |--- class: 1\n|   |--- shot_value &gt;  2.50\n|   |   |--- score_diff &lt;= 8.50\n|   |   |   |--- class: 0\n|   |   |--- score_diff &gt;  8.50\n|   |   |   |--- class: 0\n\n\n\n\n\nConclusion\nAfter tuning the hyperparameter, the model’s accuracy improved to 65.5%, marking a notable 13% enhancement compared to the baseline and a 9.5% improvement over the untuned version. Delving into the confusion matrix, it becomes evident that the model accurately predicted made shots (class 1) in 71.2% of cases, while achieving a 61.1% accuracy in identifying missed shots (class 0). This nuanced evaluation provides insights into the model’s strengths and areas for potential refinement. Notably, the absence of the ‘lag’ variable in the decision tree suggests its limited influence on the model, aligning with the initial hypothesis of its lower predictive power.\nIn summary, the hyperparameter-tuned decision tree exhibits improved accuracy and provides valuable insights into feature importance. The visualization underscores the significance of ‘shot_value’ and ‘field_goal_percentage’ in predicting shot outcomes, while the negligible role of the ‘lag’ variable aligns with expectations, emphasizing the model’s capacity to discern key predictors in the dataset.\n\n\n\nRandom Forest\nDisclaimer: Although the preceding code and analysis successfully meet the assignment requirements, I also wanted to explore the application of random forests. This section will be more concise, but includes the necessary code and some analysis for random forests.\n\nTheory\nA random forest differs from a single decision tree in that it is an ensemble or a collection of decision trees. Instead of relying on the prediction of a single tree, a random forest aggregates the predictions of multiple trees to make a more robust and accurate prediction. Every tree in the ‘random forest’ is trained on a random subset of the data and features, introducing diversity in the models. During predictions, the random forest averages or takes a vote of the individual tree predictions, reducing the risk of overfitting and improving generalization performance. This ensemble approach makes random forests particularly effective in handling complex datasets and enhancing the stability and reliability of the overall model.\n\n\nImplementation\nThe following code imports necessary libraries, splits the data into training and test sets, creates a Random Forest Classifier model, trains it on the training data, predicts outcomes on the test data, and then prints the accuracy of the model’s predictions.\n\n\nCode\n# load relevant libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint\n\n# Split the data into training and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n# Create the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)\n\nY_pred = model.predict(X_test)\n\naccuracy = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.6365280289330922\n\n\n/var/folders/lb/dk54cbx965z7nj61zps2fzr00000gn/T/ipykernel_50519/4200927063.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, Y_train)\n\n\nWe can see that without any tuning, the random forest almost matches the accuracy of the hyperparameter-tuned decision tree. Let’s visualize the first three trees in the random forest below.\n\n\nCode\n# Export the first three decision trees from the forest\nfor i in range(3):\n    tree = model.estimators_[i]\n\n    # Plot the tree\n    plt.figure(figsize=(10, 8))\n    plot_tree(tree, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True, proportion=True)\n    plt.title(f'Decision Tree {i + 1}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyper-Parameter Tuning\nNext, we will utilize a random search with cross-validation to find the best hyperparameters within a specified range. The best model is then stored, and the optimal hyperparameters are printed.\n\n\nCode\nparam_dist = {'n_estimators': randint(50,500),\n              'max_depth': randint(1,20)}\n\n# Convert Y_train to a one-dimensional array\nY_train = Y_train.values.ravel()\n\n# Create a random forest classifier\nmodel = RandomForestClassifier()\n\n# Use random search to find the best hyperparameters\nrand_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=5)\n\n# Fit the random search object to the data\nrand_search.fit(X_train, Y_train)\n\n# Create a variable for the best model\nbest_rf = rand_search.best_estimator_\n\n# Print the best hyperparameters\nprint('Best hyperparameters:', rand_search.best_params_)\n\n\nBest hyperparameters: {'max_depth': 4, 'n_estimators': 417}\n\n\n\n\nTraining Optimal Model\nUsing the above hyperparameters, let’s train the optimal random forest model.\n\n\nCode\n# Predictions on the test set\nyp_test = best_rf.predict(X_test)\n\n\n\n\nCode\nconfusion_plot(Y_test, yp_test)\n\n\nACCURACY: 0.6672694394213382\nNEGATIVE RECALL (Y=0): 0.861003861003861\nNEGATIVE PRECISION (Y=0): 0.601078167115903\nPOSITIVE RECALL (Y=1): 0.4965986394557823\nPOSITIVE PRECISION (Y=1): 0.8021978021978022\n[[223  36]\n [148 146]]\n\n\n\n\n\n\n\nCode\n# Create a series containing feature importances from the model and feature names from the training data\nfeature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\n# bar chart\nfeature_importances.plot.bar()\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nConclusion\nThe tuned Random Forest model exhibits improved accuracy compared to the tuned Decision Tree, achieving an accuracy of 66.7%. Notably, the Random Forest model demonstrates higher positive recall (Y=1) and precision, indicating enhanced performance in correctly identifying instances of made shots.\nThe above code calculates the feature importance from the tuned Random Forest model and creates a bar chart to visualize the importance of each feature in predicting shot outcomes. The resulting chart helps identify which features have the most significant impact on the model’s decision-making process. The bar chart reveals, expectedly, that shot value has the most significant impact by a large margin. At the same time, lag1 exhibits a meager impact, aligning with similar results from the tuned decision tree.\n\n\n\nExtra Joke\nWhich dating app do trees use?\nTimber"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM",
    "section": "",
    "text": "Why did the arm apply for a job? Because it wanted to lend a helping hand!  Why did the naive Bayesian suddenly feel patriotic when he heard fireworks? He assumed independence.  I was surprised when my niece said she learned R at school yesterday, and then I remembered she’s 4 and she meant the letter. My priors are all too skewed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello!\nHello and welcome to my website! My name is Billy and I am a graduate student at Georgetown University. I am currently pursuing a Master’s in Data Science and Analytics and will use this website as a place to display my work. I hope you find my work insightful and enjoy your time on my website!\n\n\nGrowing Up\nI grew up in Scotch Plains, New Jersey along with my two sisters, Melissa and Jessica. We are triplets! Below is a picture of me with my family and another of our dog Roxie.\n\n \n\n\n\nEducation\nI graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics along with a PPE (Politics, Philosophy, & Economics) Concentration and Political Science minor. I absolutely loved my time at Villanova between the academics, friendships, the school winning the NCAA Championship and more!. As an undergrad, I was involved in a number of student organizations, including the Blue Key Society, where I served as a tour guide for undergraduate admissions. I also organized accepted students days for the university.\nOne of the best experiences I had during college was studying abroad. In 2019, I participated in the Institute of Politics and Economics Program at the University of Cambridge where I took a number of economic classes and study international relations and history. Seeing how these subject matters interact with analytics truly fascinated me, which is one reason why I am pursuing an advanced degree in data science. I am extremely excited to continue my education and pursue a masters degree in data science!\n\n\nTop Shelf Designs, LLC\nDuring my senior year at Villanova, I had the opportunity to start my own business with my sisters! Our business, Top Shelf Designs, LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. Being an entrepreneur has been an incredibly rewarding journey thus far, and I am eager to see where it takes us. Here is a link to our website!\n\n\nHobbies\nI’m an avid sports fan, and love watching the New York Yankees, Villanova basketball, and NY Giants. Unfortunately, none of my teams have been doing too well lately, but I’m hoping the Giants can turn it around this season! I’m fascinated by how analytics is being used to transform the sports landscape - even across college sports! While at Villanova, I researched the Houston Astros Cheating Scandal. I would love to go back and take an even deeper dive into the subject one day!\nI love music - you can often find me with headphones or earbuds in while doing work or exploring the DC area! Additionally, I find playing instruments to be a great creative outlet after a long day of work. I grew up taking piano lessons and during COVID started to teach myself how to play the guitar!\nWhenever I have the chance, I try to travel. Wile abroad, I was lucky enough to travel to a few places in Europe. My favorite spot has to be Italy - from the history and culture to the food, wine, and sports, I am counting down the days until I can go back.\nFinally, I thoroughly enjoy books, movies, video games in addition to fun, in-person experiences. Lately, I’ve been exploring the DC area which I am loving so far! If you have any recommendations - please let me know! I’m alwyas willing to try something new!\n\n\nGeneral Info (inc. netID)\n\n\n\nname\nBilly McGloin\n\n\nGU netID\nwtm30\n\n\nLinkedIn\nlink\n\n\n\n\n\nJokes\nI always like to have a laugh so below are some jokes I hope you’ll enjoy!\n\nWebsite Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki\n\n\n\nData Science Jokes\n\nWhy should you take a data scientist with you into the jungle? They can take care of python problems.\nA SQL query walks into a bar, walks up to two tables, and asks, “Can I join you?“\nYou are so mean that your standard deviation is zero.\n\n\n\nAssorted Jokes\n\nWhy didn’t the bell work at the gym? It was a dumb bell!\nWhat do you call it when a caveman farts? A blast from the past.\nWho is Bruce Lee’s vegan brother? Broco Lee\n\n\n\n\nHoya Saxa!\n\n\n\nme and a bunch of rocks!"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Why did the linear regression model break up with the logistic regression model? Because it wanted a more ‘linear’ relationship!"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page shows the raw data, the code used to clean it, and the modified data. It’s a journal of my data cleaning process. Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nncaahoopR\n\n2021-22 Season\nLet’s clean the Villanova 2021-22 data with R:\nHere is a screen shot of the first few rows and columns of the raw data:  \n\n\nCode\nlibrary(tidyverse)\n\n\nAfter we load in relevant libraries, we can read in the data and check its shape.\n\n\nCode\nnova2122 &lt;- read.csv('./data/raw_data/villanova2122.csv')\n\ndim(nova2122)\n\n\n\n1140539\n\n\nWhat columns does the NCAA data have?\n\n\nCode\n# what are the column names?\ncolnames(nova2122)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\nThe dataset appears fairly clean; however, to focus specifically on shooting data, we’ll exclude rows without a designated shooter (e.g., turnovers, steals, rebounds, or blocks) where the shooter is marked as NA. After this refinement, we’ll reassess the shape of the data.\n\n\nCode\nnova2122 &lt;- nova2122 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n539939\n\n\nWe can see that about 5,000 rows were removed and we are left with a little over half of the initial data. In the below chunk, we create a new column, “shooter_team,” based on the action team. We create another column “shot_sequence” that tracks consecutive made or missed shots by a shooter in each game half, and then use that column to create a “previous_shots” column that indicates the number of consecutive shots (made or missed) by the shooter before the current shot. This modified dataset is then saved as a CSV file.\n\n\nCode\n# only taking the columns I want from this dataset\nsample &lt;- nova2122 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova2122.csv\", row.names = FALSE)\n\n\nHere is a screen shot of the modified data:  \n\n\n2019-20 Season\nWe can replicate the process for the 2019-20 NCAA data. The following code mirrors the one used above but is tailored to the 2019-20 season.\n\n\nCode\n# let's load in the data\nnova1920 &lt;- read.csv('./data/raw_data/villanova1920.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n958139\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova1920)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova1920 &lt;- nova1920 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n454339\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova1920 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova1920.csv\", row.names = FALSE)\n\n\n\n\n\nNews API\nLet’s clean this textual data using python:  Here is a picture of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nThis code reads sentiment scores from a JSON file, extracts titles and descriptions, assigns sentiment labels based on compound scores, forms a DataFrame, and saves the output, including titles, to a CSV file.\n\n\nCode\n# Load the sentiment scores from the JSON file\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\n# Create lists to store data\ntitles = []  # List to store document titles\ndescriptions = []   # List to store document descriptions\nsentiment_labels = []  # List to store sentiment labels\n\n# Extract the scores, titles, descriptions, and labels\nfor idx, item in enumerate(sentiment_scores, start=1):\n    titles.append(item.get('title', ''))  # Get the title of the document\n    descriptions.append(item.get('description', ''))    # Get the description of the document\n    sentiment_score = item.get('sentiment_score', {})\n    \n    # Determine the sentiment label based on the compound score\n    if sentiment_score.get('compound', 0) &gt; 0:\n        sentiment_labels.append('positive')\n    elif sentiment_score.get('compound', 0) == 0:\n        sentiment_labels.append('neutral')\n    else:\n        sentiment_labels.append('negative')\n\n# Create a DataFrame\ndata = {\n    'Title': titles,\n    'Description': descriptions,\n    'Sentiment Label': sentiment_labels\n}\n\ndf_with_labels = pd.DataFrame(data)\n\n# Save to CSV\ndf_with_labels.to_csv('./data/modified_data/sentiment_scores_with_titles.csv', index=False)\n\n\n\n\nIndividual Player Data\nLet’s clean the Aaron Judge game data with python:\nHere is a screen shot of the first few rows of the raw data:  \nAfter loading in relevant libraries and reading in the data, let’s check the shape of the dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n#reading in the file\naaronjudge = pd.read_csv('./data/raw_data/AaronJudgeData.csv')\n\n#how many rows are in this dataset?\naaronjudge.shape\n\n\n(111, 37)\n\n\nWhat are the column names?\n\n\nCode\n#what are the column names?\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1',\n       'Events', 'EV.1', 'maxEV', 'LA', 'Barrels', 'Barrel%', 'HardHit',\n       'HardHit%'],\n      dtype='object')\n\n\nLet’s remove repeated columns and check the column names again.\n\n\nCode\n#removing the repeated columns\ncolumns_to_remove = ['Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1']\naaronjudge.drop(columns=columns_to_remove, inplace=True)\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Events', 'EV.1', 'maxEV', 'LA', 'Barrels',\n       'Barrel%', 'HardHit', 'HardHit%'],\n      dtype='object')\n\n\nI believe that the intiial row with the column names is repeated throughout the data, let’s check if that is indeed the case.\n\n\nCode\n# i belive the initial row with the column names is repeated throughou the data. let's check\nprint((aaronjudge['Date'] == 'Date').sum())\n\n\n5\n\n\nLet’s remove these rows and check the shape again.\n\n\nCode\n# let's remove these rows and then check the shape again\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Date'].index, inplace=True)\naaronjudge.shape\n\n\n(106, 32)\n\n\nThere is a total row at the bottom of the data, let’s remove that as well.\n\n\nCode\n# there is also a total row which I want to remove as well. let's do that now\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Total'].index, inplace=True)\naaronjudge.shape\n\n\n(105, 32)\n\n\nSo far, I have removed 6 rows and 5 columns. I want to create a location column (home or away) based on the “@” in the “Opp” column. Let’s do that now and check the values of the new column.\n\n\nCode\n# so far, I have removed 6 rows and 5 columns. \n\n# I want to create a \"location\" column based on the \"@\" in the \"Opp\" column\naaronjudge['location'] = aaronjudge['Opp'].apply(lambda x: 'away' if '@' in x else 'home')\n\n# Remove the \"@\" symbol from the values in the \"Opp\" column\naaronjudge['Opp'] = aaronjudge['Opp'].str.replace('@', '')\n\n# check value counts of the new \"location\" column\nprint(aaronjudge['location'].value_counts()) #this seems accurate\n\n\nhome    53\naway    52\nName: location, dtype: int64\n\n\nI want to create two new columns: the number of at-bats per each game and the number of hard hits for each game. For this project, we are going to calculate at-bats as the number of plate appearances minus the number of walks (sacrifices and HBP are not included in this dataset). Let’s check the data types of the columns we’ll be using to create these new columns.\n\n\nCode\nprint(aaronjudge['PA'].dtype)\nprint(aaronjudge['BB%'].dtype)\n\n\nobject\nobject\n\n\nWe need to remove the ‘%’ symbol and convert ‘BB%’ to a float, rounding it to three decimal places. PA must also be converted to a float which allows us to create the new at_bats column. Let’s do a sanity check and look at the mean at_bats and PA (at_bats should be slightly less).\n\n\nCode\n#first i have to remove the '%' symbol and convert 'BB%' to a float\naaronjudge['BB%'] = aaronjudge['BB%'].astype(str)\naaronjudge['BB%'] = aaronjudge['BB%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'BB%' column to three decimal places\naaronjudge['BB%'] = aaronjudge['BB%'].round(3)\n\n#print(aaronjudge['BB%'].mean())\n\n#convert 'PA' to a float\naaronjudge['PA'] = aaronjudge['PA'].astype(float)\n\n# now I can create the new at_bats column\naaronjudge['at_bats'] = aaronjudge['PA'] * (1 - aaronjudge['BB%'])\n\n#now lets see the average number of at bats vs the average number of plate appearances\nprint(aaronjudge['at_bats'].mean())\nprint(aaronjudge['PA'].mean())\n\n\n3.4857333333333336\n4.314285714285714\n\n\nLet’s repeat the above process (with a few minor changes) to create the hard_hits column. First, we check the data type of the columns we will be using to calculate hard_hits.\n\n\nCode\n# now I want to create a new column for hard hits per game\n# we can do this by multiplying the hard hit percentage by the events column (these columns were part of a different table that was merged with the original table)\n\nprint(aaronjudge['HardHit%'].dtype)\nprint(aaronjudge['Events'].dtype)\n\n\nobject\nobject\n\n\nAfter we remove the ‘%’ symbol and convert ‘HardHit%’ into a float (rounding to three decimal places), we must also convert ‘Events’ to a float. Events specifies the number of balls put in play. By multiplying the two columns together, we can calculate the number of hard_hits per game.\n\n\nCode\n# this code is very similar to what we just did\n\n#first i have to remove the '%' symbol and convert 'HardHit%' to a float\n\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].astype(str)\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'HardHit%' column to three decimal places\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].round(3)\n\n#print(aaronjudge['HardHit%'].mean())\n\n#convert 'Events' to a float\naaronjudge['Events'] = aaronjudge['Events'].astype(float)\n\n# now I can create the new hard_hits column\naaronjudge['hard_hits'] = (aaronjudge['Events'] * aaronjudge['HardHit%']).round(0)\n\n#now lets see the average number of hard_hits per game\nprint(aaronjudge['hard_hits'].mean())\n\n\n1.52\n\n\nWith the number of hard_hits and at_bats per game, we can calculate the correct hard hit percentage per game. Let’s do that now.\n\n\nCode\n# finally, let's create a correct hardhit% column that is based on the number of at-bats, not the number of times a player puts the ball in play\naaronjudge['correct_hardhit%'] = (aaronjudge['hard_hits'] / aaronjudge['at_bats']).round(2)\n\n# now let's see the average correct hardhit% for Aaron Judge\nprint(aaronjudge['correct_hardhit%'].mean())\n\n\n0.42829999999999996\n\n\nSometimes in certain stadiums or based on the weather, the HardHit% data is missing. This causes the value of the newly created correct_hardhit% column to be NaN, so let’s remove those few rows and check the shape again. Finally, we will save this modified data to a csv file.\n\n\nCode\naaronjudge.dropna(subset=['correct_hardhit%'], inplace=True)\n\n#let's check the shape again\naaronjudge.shape #loss of 5 rows\n\n\n(100, 36)\n\n\n\n\nCode\n# now we can save this to a csv file\naaronjudge.to_csv('./data/modified_data/aaronjudge.csv', index=False)\n\n\nHere is a screenshot of the first couple rows of the modified csv file:  \n\n\nBaseballr\nAfter loading in relevant libraries and reading in the data, let’s take a brief look at what the data looks like:\n\n\nCode\nlibrary(tidyverse)\nbaseball &lt;- read.csv(\"./data/raw_data/baseballr_six_games.csv\")\nhead(baseball)\n\n\n\nA data.frame: 6 x 160\n\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n1\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n4\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n5\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n6\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nWhat columns does this data have?\n\n\nCode\n# what are the column names?\ncolnames(baseball)\n\n\n\n'game_pk''game_date''index''startTime''endTime''isPitch''type''playId''pitchNumber''details.description''details.event''details.awayScore''details.homeScore''details.isScoringPlay''details.hasReview''details.code''details.ballColor''details.isInPlay''details.isStrike''details.isBall''details.call.code''details.call.description''count.balls.start''count.strikes.start''count.outs.start''player.id''player.link''pitchData.strikeZoneTop''pitchData.strikeZoneBottom''details.fromCatcher''pitchData.coordinates.x''pitchData.coordinates.y''hitData.trajectory''hitData.hardness''hitData.location''hitData.coordinates.coordX''hitData.coordinates.coordY''actionPlayId''details.eventType''details.runnerGoing''position.code''position.name''position.type''position.abbreviation''battingOrder''atBatIndex''result.type''result.event''result.eventType''result.description''result.rbi''result.awayScore''result.homeScore''about.atBatIndex''about.halfInning''about.inning''about.startTime''about.endTime''about.isComplete''about.isScoringPlay''about.hasReview''about.hasOut''about.captivatingIndex''count.balls.end''count.strikes.end''count.outs.end''matchup.batter.id''matchup.batter.fullName''matchup.batter.link''matchup.batSide.code''matchup.batSide.description''matchup.pitcher.id''matchup.pitcher.fullName''matchup.pitcher.link''matchup.pitchHand.code''matchup.pitchHand.description''matchup.splits.batter''matchup.splits.pitcher''matchup.splits.menOnBase''batted.ball.result''home_team''home_level_id''home_level_name''home_parentOrg_id''home_parentOrg_name''home_league_id''home_league_name''away_team''away_level_id''away_level_name''away_parentOrg_id''away_parentOrg_name''away_league_id''away_league_name''batting_team''fielding_team''last.pitch.of.ab''pfxId''details.trailColor''details.type.code''details.type.description''pitchData.startSpeed''pitchData.endSpeed''pitchData.zone''pitchData.typeConfidence''pitchData.plateTime''pitchData.extension''pitchData.coordinates.aY''pitchData.coordinates.aZ''pitchData.coordinates.pfxX''pitchData.coordinates.pfxZ''pitchData.coordinates.pX''pitchData.coordinates.pZ''pitchData.coordinates.vX0''pitchData.coordinates.vY0''pitchData.coordinates.vZ0''pitchData.coordinates.x0''pitchData.coordinates.y0''pitchData.coordinates.z0''pitchData.coordinates.aX''pitchData.breaks.breakAngle''pitchData.breaks.breakLength''pitchData.breaks.breakY''pitchData.breaks.spinRate''pitchData.breaks.spinDirection''hitData.launchSpeed''hitData.launchAngle''hitData.totalDistance''injuryType''umpire.id''umpire.link''details.isOut''pitchData.breaks.breakVertical''pitchData.breaks.breakVerticalInduced''pitchData.breaks.breakHorizontal''isBaseRunningPlay''details.disengagementNum''isSubstitution''replacedPlayer.id''replacedPlayer.link''result.isOut''about.isTopInning''matchup.postOnFirst.id''matchup.postOnFirst.fullName''matchup.postOnFirst.link''matchup.postOnSecond.id''matchup.postOnSecond.fullName''matchup.postOnSecond.link''matchup.postOnThird.id''matchup.postOnThird.fullName''matchup.postOnThird.link''reviewDetails.isOverturned''reviewDetails.inProgress''reviewDetails.reviewType''reviewDetails.challengeTeamId''base''details.violation.type''details.violation.description''details.violation.player.id''details.violation.player.fullName'\n\n\nAre there any missing data?\n\n\nCode\n# missing data?\ndata.frame(colSums(is.na(baseball)))\n\n\n\nA data.frame: 160 x 1\n\n\n\ncolSums.is.na.baseball..\n\n\n\n&lt;dbl&gt;\n\n\n\n\ngame_pk\n0\n\n\ngame_date\n0\n\n\nindex\n0\n\n\nstartTime\n0\n\n\nendTime\n0\n\n\nisPitch\n0\n\n\ntype\n0\n\n\nplayId\n215\n\n\npitchNumber\n242\n\n\ndetails.description\n0\n\n\ndetails.event\n1755\n\n\ndetails.awayScore\n1755\n\n\ndetails.homeScore\n1755\n\n\ndetails.isScoringPlay\n1755\n\n\ndetails.hasReview\n0\n\n\ndetails.code\n215\n\n\ndetails.ballColor\n243\n\n\ndetails.isInPlay\n242\n\n\ndetails.isStrike\n242\n\n\ndetails.isBall\n242\n\n\ndetails.call.code\n242\n\n\ndetails.call.description\n242\n\n\ncount.balls.start\n0\n\n\ncount.strikes.start\n0\n\n\ncount.outs.start\n0\n\n\nplayer.id\n1790\n\n\nplayer.link\n1790\n\n\npitchData.strikeZoneTop\n243\n\n\npitchData.strikeZoneBottom\n243\n\n\ndetails.fromCatcher\n1943\n\n\n...\n...\n\n\numpire.link\n1970\n\n\ndetails.isOut\n0\n\n\npitchData.breaks.breakVertical\n243\n\n\npitchData.breaks.breakVerticalInduced\n243\n\n\npitchData.breaks.breakHorizontal\n243\n\n\nisBaseRunningPlay\n1951\n\n\ndetails.disengagementNum\n1904\n\n\nisSubstitution\n1908\n\n\nreplacedPlayer.id\n1948\n\n\nreplacedPlayer.link\n1948\n\n\nresult.isOut\n0\n\n\nabout.isTopInning\n0\n\n\nmatchup.postOnFirst.id\n1837\n\n\nmatchup.postOnFirst.fullName\n1837\n\n\nmatchup.postOnFirst.link\n1837\n\n\nmatchup.postOnSecond.id\n1903\n\n\nmatchup.postOnSecond.fullName\n1903\n\n\nmatchup.postOnSecond.link\n1903\n\n\nmatchup.postOnThird.id\n1930\n\n\nmatchup.postOnThird.fullName\n1930\n\n\nmatchup.postOnThird.link\n1930\n\n\nreviewDetails.isOverturned\n1965\n\n\nreviewDetails.inProgress\n1965\n\n\nreviewDetails.reviewType\n1965\n\n\nreviewDetails.challengeTeamId\n1965\n\n\nbase\n1965\n\n\ndetails.violation.type\n1969\n\n\ndetails.violation.description\n1969\n\n\ndetails.violation.player.id\n1969\n\n\ndetails.violation.player.fullName\n1969\n\n\n\n\n\nUpon examination of the data, it seems insufficient for analyzing the hot hand phenomenon for this study. The preceding individual player data appears to be more appropriate for a comprehensive analysis of this topic, as it includes relevant metrics such as hard-hit percentage.\n\n\nExtra Joke\nWhat did the broom say to the vacuum?\n“I’m so tired of people pushing us around.”"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction to the Hot Hand",
    "section": "",
    "text": "Within the world of sports, the terms “hot hand” or being “in the zone” are frequently used to describe a streak of exceptional performance by a player. The “hot hand” term originated in the sport of basketball, suggesting that a player who has made numerous successive shots is more likely to make their next basket, as opposed to the same person who had missed their last few shots. For example, a basketball player may make an exceptional number of shots in a short period of time, such as Klay Thompson scoring 37 points in a quarter. Another example is Joe DiMaggio’s 56 game hitting streak in 1941, one of the most incredible hitting runs in baseball history. Ordinarily, these types of “runs” or “streaks” are rare. There is “a belief that the performance of a player during this particular period is significantly better than expected on the basis of the player’s overall record” 1. Often, this heightened performance can be attributed to increased confidence by the player.   This belief is shared by a majority of players, coaches, and fans, yet there is little statistical evidence to support this phenomenon. In fact, a majority of studies suggest that the “hot hand” is a fallacy and advise coaches not to consider it when selecting plays. The “hot hand” phenomenon has been studied by many psychologists and statisticians who still debate this issue to this day.   While the term “hot hand” may be most commonly associated with the world of sports, “studies have been done in other academic fields outside the sports domain, such as economics and cognitive science” 2. Through this project, I plan to explore the mystery of streaks both within and outside the world of sports."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction to the Hot Hand",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilovich, Vallone, and Tversky (1985)↩︎\nBar-Eli, Avugos, and Raab (2006)↩︎"
  },
  {
    "objectID": "5100.html",
    "href": "5100.html",
    "title": "Political Colors and Quality of Life: Analyzing How State Political Leanings Influence Citizen Well-Being",
    "section": "",
    "text": "The interplay between political leanings and quality of life in the United States forms a complex tapestry. This study aims to dissect this interplay, focusing on how a state’s political orientations impact various facets of citizen well-being. This project not only seeks to highlight the correlations between political inclinations and quality of life indicators but also strives to understand the causative relationships within. The primary research question guiding this study is: How do state political leanings influence the quality of life of their residents?"
  },
  {
    "objectID": "5100.html#data-collection",
    "href": "5100.html#data-collection",
    "title": "Political Colors and Quality of Life: Analyzing How State Political Leanings Influence Citizen Well-Being",
    "section": "Data Collection",
    "text": "Data Collection\n\nNational Voting Data\nNational voting data was collected from The Federal Election Commission (FEC). They host elections and voting information for every federal election since 1982. For our analysis we used the 2020 data. The data is available in Portable Document Format (PDF) and Excel, .xlsx, formats. We used the .xlsx version and converted it to a Comma-Separated Values (CSV) file. The data we used for our project can be found on tab 9, 2020 Pres General Results, in the Excel file (FEC). This data is used to determine the states’ political leanings. (2020)  FEC | Election and voting information\n\n\nPopulation Data\nPopulation data was taken from the United States Census Bureau. To match the voting data we used the State Population Totals and Components of Change: 2020-2022. The data is provided in a CSV file that is easy to import into the data cleaning workflow. This data was used to normalize other data we gathered. (2022)  State Population Totals: 2020-2022 (census.gov)\n\n\nIncome Data\nIncome data was also obtained from the U.S. Census Bureau, which collects and maintains numerous sets of economic and income data for the United States. We used the Table H-8, median household income by state, as the information was the cleanest for our use case. Table H-8 is an Excel document with numerous header rows that do not contain any data. (2022)  Historical Income Tables: Households (census.gov)\n\n\nCrime Data\nWe scraped crime data from the FBI’s Crime Data Explorer (CDE). The CDE permits the query of the number of crimes by state, year, and the type of crime (property crime, larceny, ect.). For our analysis, we chose to retrieve crime data from 2020-2022 for each state, constructing two seperate CSV files, one for violent crimes and one for non-violent crimes. This data coupled with the state population data reveals the crime rates for each state for a given year. (2022)  FBI Crime Data API (cde.ucr.cjis.gov)\n\n\nHealth Data\nHeart disease deaths data was collected from the Centers for Disease Control and Prevention (CDC). The CSV obtained from their site lists the year, state, number of deaths from heart disease, had the rate of heart disease deaths per 100k people per state, and a url to provide more information for the state. Number of deaths from heart disease was one of the features we used to determine the health of a state. Since the data was taken in as deaths per state it was normalized with the states population during cleaning. Heart disease was chosen since it is a leading cause of death in America and is a good determinator for how people take care of themselves. (2022)  Stats of the States - Heart Disease Mortality (cdc.gov)  Covid-19 data was gathered from the New York Times GitHub Repository. The repository host multiple datasets with COVID-19 data. We used the us-states.csv file that contains date (single day the information is for), state, fips, cases, and deaths. The repository contains a README.md that shows their methods for collection and organizing the data. The file was a CSV and easy to import into the workflow during data cleaning. Both COVID-19 cases and deaths were used as features for determining the health of a state. These variables show how a state reacts to rising health risks. (2023)  NYTimes/covid-19-data: A repository of data on coronavirus cases and deaths in the U.S. (github.com)\n\n\nEducation Data\nEducation data was acquired through the United States Census Bureau API. The data comes from the American Community Survey (ACS), which is an ongoing survey that provides data every year about social, economic,demographic, and housing characteristics of the U.S. population. We focused on the number of people with a bachelor’s degree, associate’s degree, GED or equivalent, and some college for each state from 2021-2022. (2022)  State Education Totals: 2021-2022 (census.gov)\n\n\nRace Data\nWe retrieved data on the number of white, black, asian, and hispanic people per state from the Census API and ACS similar to the education data. Again, we only chose to use the data from 2021-2022. (2022)  State Race Totals: 2021-2022 (census.gov)"
  },
  {
    "objectID": "5100.html#data-cleaning",
    "href": "5100.html#data-cleaning",
    "title": "Political Colors and Quality of Life: Analyzing How State Political Leanings Influence Citizen Well-Being",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThis section of the report will act as a journal of our data cleaning process. We will show the raw data, the code used to clean it, and then the modified data.\n\nNational Voting Data\nAfter loading in the relevant libraries and read in the data, let’s take a look at what the national voting data looks like.\n\n\nCode\n#load in relevant libraries\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\n# read in the national voting data\ndf &lt;- read.csv('voting.csv')\n\n#what does the national voting data look like?\ncat(\"Shape of Data:\", dim(df))\nhead(df)\n\n\nShape of Data: 681 13\n\n\n\nA data.frame: 6 x 13\n\n\n\nX...1\nFEC.ID\nSTATE\nSTATE.ABBREVIATION\nGENERAL.ELECTION.DATE\nFIRST.NAME\nLAST.NAME\nLAST.NAME...FIRST\nTOTAL.VOTES\nPARTY\nGENERAL.RESULTS\nGENERAL..\nTOTAL.VOTES..\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n3\nP80001571\nAlabama\nAL\n11/3/20\nDonald J.\nTrump\nTrump, Donald J.\n\nR\n1,441,170\n62.03%\n\n\n\n3\n4\nP80000722\nAlabama\nAL\n11/3/20\nJoseph R.\nBiden\nBiden, Joseph R.\n\nD\n849,624\n36.57%\n\n\n\n4\n5\nP00013524\nAlabama\nAL\n11/3/20\nJo\nJorgensen\nJorgensen, Jo\n\nIND\n25,176\n1.08%\n\n\n\n5\n6\nn/a\nAlabama\nAL\n11/3/20\n\nScattered\nScattered\n\nW\n7,312\n0.31%\n\n\n\n6\n7\nn/a\nAlabama\nAL\n11/3/20\n\n\n\nTotal State Votes:\n\n\n\n2,323,282\n\n\n\n\n\nWe can see that the above data has multiple rows per state, with rows for each candidate and total state votes. Let’s clean this data. We can start by only keeping votes for the two major parties and then create two new columns (one for Candidate and another for Total Votes for that candidate). The data then needs to be converted from long to wide so that each row/observation represents a state (including DC). Finally, let’s create some new columns that will be useful for our analysis: Total Votes Overall, Vote Outcome, and Percentage Democratic.\n\n\nCode\n#VOTING DATA\n# remove all votes not for trump or biden\ndf &lt;- df[df$PARTY %in% c(\"D\", \"R\"), ]\n\n# create columns for biden and trump votes for each state\ndf &lt;- df %&gt;%\n  mutate(VoteTotal = as.numeric(gsub(\",\", \"\", GENERAL.RESULTS)),\n         Candidate = ifelse(PARTY == \"R\", \"Trump\", ifelse(PARTY == \"D\", \"Biden\", NA))) %&gt;%\n  group_by(STATE, Candidate) %&gt;%\n  summarize(TotalVotes = sum(VoteTotal))\n  \n# convert from long to wide\ndf_wide &lt;- df %&gt;%\n  spread(key = Candidate, value = TotalVotes, fill = 0)\n\n# create columns for total votes, outcome, and percentage democratic\ndf_result &lt;- df_wide %&gt;%\n  mutate(TotalVotes = Biden + Trump,\n         Outcome = ifelse(Biden &gt; Trump, \"Democratic\", \"Republican\"),\n         Percentage_Democratic = round((Biden / TotalVotes) * 100, 2))\n\ncat(\"Shape of Data:\", dim(df_result))\nhead(df_result)\n\n\n`summarise()` has grouped output by 'STATE'. You can override using the\n`.groups` argument.\n\n\nShape of Data: 51 6\n\n\n\nA grouped_df: 6 x 6\n\n\nSTATE\nBiden\nTrump\nTotalVotes\nOutcome\nPercentage_Democratic\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n\n\n\n\n\n\n\nPopulation Data\nNow we can read in the population data that was acquired from the United States Census Bureau and see what it looks like.\n\n\nCode\n# read in the population data\npop &lt;- read.csv('population.csv')\n\n# what does the data look like?\ncat(\"Shape of Data:\", dim(pop))\nhead(pop)\n\n\nShape of Data: 51 2\n\n\n\nA data.frame: 6 x 2\n\n\n\nX...State\nPopulation.Estimate\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\n.Alabama\n5,031,362\n\n\n2\n.Alaska\n732,923\n\n\n3\n.Arizona\n7,179,943\n\n\n4\n.Arkansas\n3,014,195\n\n\n5\n.California\n39,501,653\n\n\n6\n.Colorado\n5,784,865\n\n\n\n\n\nLet’s remove the period before the state name, convert the population column to numeric, and check the sum of all populations to check the accuracy of the data. Let’s merged the clean data together by STATE.\n\n\nCode\n#POPULATION DATA\n# Remove the dot before the state name\npop &lt;- pop %&gt;%\n  mutate(STATE = gsub(\"\\\\.\", \"\", X...State))\n\n# Convert Population.Estimate to numeric after removing commas\npop$Population.Estimate &lt;- as.numeric(gsub(\",\", \"\", pop$Population.Estimate))\n\npop &lt;- select(pop, -X...State)\n\nsum(pop$Population.Estimate)\n\n# Merge the two datasets\ndf_merged &lt;- left_join(df_result, pop, by = \"STATE\")\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\n331511512\n\n\nShape of Data: 51 7\n\n\n\nA grouped_df: 6 x 7\n\n\nSTATE\nBiden\nTrump\nTotalVotes\nOutcome\nPercentage_Democratic\nPopulation.Estimate\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n\n\n\n\n\n\n\nIncome Data\nLet’s read in the income data that was acquired from the U.S. Census Bureau and see what it looks like.\n\n\nCode\n# read in the income data\nincome &lt;- read.csv('median_income.csv')\n\n# what does the data look like?\ncat(\"Shape of Data:\", dim(income))\nhead(income)\n\n\nShape of Data: 52 2\n\n\n\nA data.frame: 6 x 2\n\n\n\nX...State\nMedian.income\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n1\nUnited States\n68,010\n\n\n2\nAlabama\n54,690\n\n\n3\nAlaska\n74,750\n\n\n4\nArizona\n67,090\n\n\n5\nArkansas\n50,780\n\n\n6\nCalifornia\n77,650\n\n\n\n\n\nWe need to remove the United States row and rename the state column so that it matches the merged data. Below is the newly merged data.\n\n\nCode\n#INCOME DATA\n# remove united states row and rename the state column\nincome &lt;- income %&gt;%\n  filter(X...State != \"United States\") %&gt;%\n  rename(STATE = X...State)\n\n# merge the income data with the merged data\ndf_merged &lt;- left_join(df_merged, income, by = \"STATE\")\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 8\n\n\n\nA grouped_df: 6 x 8\n\n\nSTATE\nBiden\nTrump\nTotalVotes\nOutcome\nPercentage_Democratic\nPopulation.Estimate\nMedian.income\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n\n\n\n\n\n\n\nCrime Data\nLet’s read in the crime data and see what it looks like.\n\n\nCode\ncrimes &lt;- read.csv('crimes.csv')\n\ncat(\"Shape of Data\", dim(crimes))\nhead(crimes)\n\n\nShape of Data 153 4\n\n\n\nA data.frame: 6 x 4\n\n\n\nX...State\nYear\nViolent_Crimes\nProperty_Crimes\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\nAL\n2020\n22322\n105161\n\n\n2\nAL\n2021\n17590\n74271\n\n\n3\nAL\n2022\n20759\n88240\n\n\n4\nAK\n2020\n6126\n16528\n\n\n5\nAK\n2021\n5573\n13456\n\n\n6\nAK\n2022\n5567\n13124\n\n\n\n\n\nAgain we must rename the state column to match the rest of the data. Additionally, the state column is now abbreviations so we must map it to the full state name that can be done with the follwing code. Once done, we can drop the original state column. Similar to before, the data is in long format and we must convert it to wide so that each row (unit of observation) is a state and there is a column for each crime type for different years. Once done, we can merge the crime data with the rest of the data.\n\n\nCode\n#CRIME DATA\n# rename the state column \ncolnames(crimes)[colnames(crimes) == \"X...State\"] &lt;- \"STATE\"\n\n# Create a mapping between state abbreviations and full state names\nstate_mapping &lt;- data.frame(\n  Abbreviation = c(\n    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\",\n    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n  ),\n  StateName = c(\n    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",\n    \"Connecticut\", \"District of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\",\n    \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n    \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n    \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\",\n    \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\",\n    \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\",\n    \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n  )\n)\n\n# convert STATE column to character\ncrimes$STATE &lt;- as.character(crimes$STATE)\n\n# Merge crime_data with state_mapping to get full state names\ncrimes &lt;- left_join(crimes, state_mapping, by = c(\"STATE\" = \"Abbreviation\"))\n\n# Drop the original 'STATE' column\ncrimes &lt;- select(crimes, -STATE)\n\n# change column name\ncolnames(crimes)[colnames(crimes) == \"StateName\"] &lt;- \"STATE\"\n\ncrimes &lt;- distinct(crimes, STATE, Year, .keep_all = TRUE)\n\n# convert from long to wide\ncrimes_wide &lt;- crimes %&gt;%\n  pivot_wider(\n    names_from = Year,\n    values_from = c(Violent_Crimes, Property_Crimes)\n  )\n\n# merge the crimes data with the merged data\ndf_merged &lt;- left_join(df_merged, crimes_wide, by = \"STATE\")\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 14\n\n\n\nA grouped_df: 6 x 14\n\n\nSTATE\nBiden\nTrump\nTotalVotes\nOutcome\nPercentage_Democratic\nPopulation.Estimate\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\nViolent_Crimes_2022\nProperty_Crimes_2020\nProperty_Crimes_2021\nProperty_Crimes_2022\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n20759\n105161\n74271\n88240\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n5567\n16528\n13456\n13124\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n31754\n165323\n153641\n151421\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n19654\n79200\n76580\n74664\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n194935\n842054\n847567\n914517\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n28759\n164582\n182850\n183816\n\n\n\n\n\n\n\nHealth Data\nLet’s read in the health data and see what it looks like. We’ll start with data on heart disease that was acquired from the CDC.\n\n\nCode\nhearts &lt;- read.csv('heart_disease_deaths.csv')\n\ncat(\"Shape of Data:\", dim(hearts))\nhead(hearts)\n\n\nShape of Data: 450 4\n\n\n\nA data.frame: 6 x 4\n\n\n\nYEAR\nSTATE\nRATE\nDEATHS\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n2021\nAL\n247.5\n15173\n\n\n2\n2021\nAK\n154.7\n1011\n\n\n3\n2021\nAZ\n158.3\n14550\n\n\n4\n2021\nAR\n231.0\n8547\n\n\n5\n2021\nCA\n147.8\n65471\n\n\n6\n2021\nCO\n135.1\n8081\n\n\n\n\n\nSimilar to before, we must alter the state column to match the rest of the data. As the data uses abbreviations instead of state names, we must once again map it to the full state name. We will also rename some of the other columns for clarity. Furthermore, we only want to keep years 2020 and onwards. As done before, we must convert the data from long to wide with each row represeting one unit of observation (a state). Let’s then merge the heart disease deaths and rates with the rest of the data. Finally we will rename some of the columns in the merged dataset for clarity.\n\n\nCode\n#HEART DISEASE DATA\n# convert STATE column to character\nhearts$STATE &lt;- as.character(hearts$STATE)\n\n# Merge crime_data with state_mapping to get full state names\nhearts &lt;- left_join(hearts, state_mapping, by = c(\"STATE\" = \"Abbreviation\"))\n\n# Drop the original 'STATE' column\nhearts &lt;- select(hearts, -STATE)\n\n# renaming columnss\nhearts &lt;- hearts %&gt;%\n  rename(STATE = StateName, DeathsPer100k = RATE)\n\n# filter out only for 2020 and 2021\nhearts &lt;- hearts %&gt;%\n  filter(YEAR &gt;= 2020)\n  \n# converting hearts from long to wide\nhearts_wide &lt;- hearts %&gt;%\n  pivot_wider(\n    names_from = YEAR,\n    values_from = c(DEATHS, DeathsPer100k),\n    names_glue = \"{.value}{YEAR}\"\n  )\n\n# merge the heart disease mortality data with the merged data\ndf_merged &lt;- left_join(df_merged, hearts_wide, by = \"STATE\")\n\n# renaming columns for clarity\ndf_merged &lt;- df_merged %&gt;%\n  rename(BidenVotes = Biden, TrumpVotes = Trump, VoteOutcome = Outcome, DemPercentage = Percentage_Democratic, Population = Population.Estimate, \n  HeartDeaths2021 = DEATHS2021, HeartDeaths2020 = DEATHS2020, HeartDeathsPer100k2021 = DeathsPer100k2021, HeartDeathsPer100k2020 = DeathsPer100k2020)\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 18\n\n\n\nA grouped_df: 6 x 18\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\nViolent_Crimes_2022\nProperty_Crimes_2020\nProperty_Crimes_2021\nProperty_Crimes_2022\nHeartDeaths2021\nHeartDeaths2020\nHeartDeathsPer100k2021\nHeartDeathsPer100k2020\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n20759\n105161\n74271\n88240\n15173\n14739\n247.5\n237.5\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n5567\n16528\n13456\n13124\n1011\n915\n154.7\n139.8\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n31754\n165323\n153641\n151421\n14550\n14196\n158.3\n144.8\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n19654\n79200\n76580\n74664\n8547\n8621\n231.0\n222.5\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n194935\n842054\n847567\n914517\n65471\n66538\n147.8\n144.0\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n28759\n164582\n182850\n183816\n8081\n8023\n135.1\n128.1\n\n\n\n\n\nLet’s read in the COVID-19 data (acquired from the nytimes) and see what it looks like.\n\n\nCode\ncovid &lt;- read.csv('covid.csv')\n\ncat(\"Shape of Data:\", dim(covid))\nhead(covid)\n\n\nShape of Data: 61942 5\n\n\n\nA data.frame: 6 x 5\n\n\n\ndate\nstate\nfips\ncases\ndeaths\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1/21/2020\nWashington\n53\n1\n0\n\n\n2\n1/22/2020\nWashington\n53\n1\n0\n\n\n3\n1/23/2020\nWashington\n53\n1\n0\n\n\n4\n1/24/2020\nIllinois\n17\n1\n0\n\n\n5\n1/24/2020\nWashington\n53\n1\n0\n\n\n6\n1/25/2020\nCalifornia\n6\n1\n0\n\n\n\n\n\nThe above data counts the number of total cumulative cases and deaths per each state. Thus, we can take the maximum value for each state to find the total cases and deaths per state. This is done via the code below. We can then merge this dataset with the rest of the data. It is important to use a left join as we only want to keep the states that are in the merged dataframe. Addtionally, we will convert relevant columns to numeric and then create a new column that calculates COVID cases and deaths as percentages of the state population. Below is the newly merged data.\n\n\nCode\n#COVID DATA\n# getting total cases and deaths per state\ncovid &lt;- covid %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    covid_cases = max(cases, na.rm = TRUE),\n    covid_deaths = max(deaths, na.rm = TRUE)\n  )\n\n#rename the state column\ncovid &lt;- covid %&gt;%\n  rename(STATE = state)\n\n# merge the covid data with the merged data\ndf_merged &lt;- left_join(df_merged, covid, by = \"STATE\")\n\n# Convert relevant columns to numeric\ndf_merged$covid_cases &lt;- as.numeric(df_merged$covid_cases)\ndf_merged$covid_deaths &lt;- as.numeric(df_merged$covid_deaths)\n\n# Create new columns for COVID cases and deaths as percentages\ndf_merged$covid_cases_percentage &lt;- round((df_merged$covid_cases / df_merged$Population) * 100, 2)\ndf_merged$covid_deaths_percentage &lt;- round((df_merged$covid_deaths / df_merged$Population) * 100, 2)\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 22\n\n\n\nA grouped_df: 6 x 22\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\n...\nProperty_Crimes_2021\nProperty_Crimes_2022\nHeartDeaths2021\nHeartDeaths2020\nHeartDeathsPer100k2021\nHeartDeathsPer100k2020\ncovid_cases\ncovid_deaths\ncovid_cases_percentage\ncovid_deaths_percentage\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n...\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n...\n74271\n88240\n15173\n14739\n247.5\n237.5\n1648385\n21631\n32.76\n0.43\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n...\n13456\n13124\n1011\n915\n154.7\n139.8\n308893\n1438\n42.15\n0.20\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n...\n153641\n151421\n14550\n14196\n158.3\n144.8\n2451062\n33190\n34.14\n0.46\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n...\n76580\n74664\n8547\n8621\n231.0\n222.5\n1008303\n13068\n33.45\n0.43\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n...\n847567\n914517\n65471\n66538\n147.8\n144.0\n12169158\n104277\n30.81\n0.26\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n...\n182850\n183816\n8081\n8023\n135.1\n128.1\n1771010\n14245\n30.61\n0.25\n\n\n\n\n\n\n\nEducation Data\nLet’s read in the education data that was acquired from _____ and take a look at it.\n\n\nCode\neducation &lt;- read.csv('education.csv') #keep in mind that this data is from 2021\n\ncat(\"Shape of Data:\", dim(education))\nhead(education)\n\n\nShape of Data: 51 1235\n\n\n\nA data.frame: 6 x 1235\n\n\n\nX...DP02_0001E\nDP02_0001EA\nDP02_0001M\nDP02_0001MA\nDP02_0001PE\nDP02_0001PEA\nDP02_0001PM\nDP02_0001PMA\nDP02_0002E\nDP02_0002EA\n...\nDP02_0154EA\nDP02_0154M\nDP02_0154MA\nDP02_0154PE\nDP02_0154PEA\nDP02_0154PM\nDP02_0154PMA\nGEO_ID\nNAME\nstate\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\n1967559\nNA\n10527\nNA\n1967559\nNA\n-888888888\n(X)\n904392\nNA\n...\nNA\n12886\nNA\n85.0\nNA\n0.5\nNA\n0400000US01\nAlabama\n1\n\n\n2\n2817723\nNA\n10850\nNA\n2817723\nNA\n-888888888\n(X)\n1344242\nNA\n...\nNA\n12165\nNA\n91.4\nNA\n0.3\nNA\n0400000US04\nArizona\n4\n\n\n3\n1183675\nNA\n7882\nNA\n1183675\nNA\n-888888888\n(X)\n565893\nNA\n...\nNA\n10272\nNA\n85.5\nNA\n0.6\nNA\n0400000US05\nArkansas\n5\n\n\n4\n13429063\nNA\n19170\nNA\n13429063\nNA\n-888888888\n(X)\n6517082\nNA\n...\nNA\n23412\nNA\n92.9\nNA\n0.1\nNA\n0400000US06\nCalifornia\n6\n\n\n5\n2313042\nNA\n8099\nNA\n2313042\nNA\n-888888888\n(X)\n1124072\nNA\n...\nNA\n10722\nNA\n93.0\nNA\n0.3\nNA\n0400000US08\nColorado\n8\n\n\n6\n1428313\nNA\n5900\nNA\n1428313\nNA\n-888888888\n(X)\n664848\nNA\n...\nNA\n8523\nNA\n92.2\nNA\n0.4\nNA\n0400000US09\nConnecticut\n9\n\n\n\n\n\nUsing the code below, we can only select the specific columns that we want and rename them for clarity. Then we can merge this education data with the larger dataframe.\n\n\nCode\n#EDUCATION DATA\n#rename and select relevant columns\neducation &lt;- education %&gt;%\n  rename(STATE = NAME, HighSchoolGraduates = DP02_0062E, BachelorsDegree = DP02_0065E, AssociatesDegree = DP02_0064E, SomeCollege = DP02_0063E)\n\neducation2 &lt;- education %&gt;% select(STATE, HighSchoolGraduates, BachelorsDegree, AssociatesDegree, SomeCollege)\n\n# merge the education data with the merged data\ndf_merged &lt;- left_join(df_merged, education2, by = \"STATE\")\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 26\n\n\n\nA grouped_df: 6 x 26\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\n...\nHeartDeathsPer100k2021\nHeartDeathsPer100k2020\ncovid_cases\ncovid_deaths\ncovid_cases_percentage\ncovid_deaths_percentage\nHighSchoolGraduates\nBachelorsDegree\nAssociatesDegree\nSomeCollege\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n...\n247.5\n237.5\n1648385\n21631\n32.76\n0.43\n1079285\n572276\n303028\n705662\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n...\n154.7\n139.8\n308893\n1438\n42.15\n0.20\n135473\n101422\n40276\n118250\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n...\n158.3\n144.8\n2451062\n33190\n34.14\n0.46\n1168057\n985673\n467305\n1180491\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n...\n231.0\n222.5\n1008303\n13068\n33.45\n0.43\n695062\n324137\n161542\n435235\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n...\n147.8\n144.0\n12169158\n104277\n30.81\n0.26\n5578997\n5958030\n2120275\n5287901\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n...\n135.1\n128.1\n1771010\n14245\n30.61\n0.25\n814373\n1107309\n334157\n793438\n\n\n\n\n\n\n\nRace Data\nWe can add in race data using the same process as above.\n\n\nCode\nrace &lt;- read.csv('Race.csv')\n\ncat(\"Shape of Data:\", dim(race))\nhead(race)\n\n\nShape of Data: 52 731\n\n\n\nA data.frame: 6 x 731\n\n\n\nDP05_0001E\nDP05_0001EA\nDP05_0001M\nDP05_0001MA\nDP05_0001PE\nDP05_0001PEA\nDP05_0001PM\nDP05_0001PMA\nDP05_0002E\nDP05_0002EA\n...\nDP05_0091EA\nDP05_0091M\nDP05_0091MA\nDP05_0091PE\nDP05_0091PEA\nDP05_0091PM\nDP05_0091PMA\nGEO_ID\nNAME\nstate\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\n1\n5074296\nNA\n-555555555\n*****\n5074296\nNA\n-888888888\n(X)\n2461248\nNA\n...\nNA\n4729\nNA\n52.4\nNA\n0.1\nNA\n0400000US01\nAlabama\n1\n\n\n2\n733583\nNA\n-555555555\n*****\n733583\nNA\n-888888888\n(X)\n385667\nNA\n...\nNA\n2046\nNA\n46.8\nNA\n0.3\nNA\n0400000US02\nAlaska\n2\n\n\n3\n7359197\nNA\n-555555555\n*****\n7359197\nNA\n-888888888\n(X)\n3678381\nNA\n...\nNA\n8862\nNA\n50.4\nNA\n0.1\nNA\n0400000US04\nArizona\n4\n\n\n4\n3045637\nNA\n-555555555\n*****\n3045637\nNA\n-888888888\n(X)\n1504488\nNA\n...\nNA\n3663\nNA\n51.3\nNA\n0.1\nNA\n0400000US05\nArkansas\n5\n\n\n5\n39029342\nNA\n-555555555\n*****\n39029342\nNA\n-888888888\n(X)\n19536425\nNA\n...\nNA\n28487\nNA\n50.6\nNA\n0.1\nNA\n0400000US06\nCalifornia\n6\n\n\n6\n5839926\nNA\n-555555555\n*****\n5839926\nNA\n-888888888\n(X)\n2960896\nNA\n...\nNA\n7496\nNA\n49.4\nNA\n0.1\nNA\n0400000US08\nColorado\n8\n\n\n\n\n\n\n\nCode\n#RACE DATA\n# rename and select relevant columns\nrace &lt;- race %&gt;%\n  rename(STATE = NAME, White = DP05_0037E, Black = DP05_0038E, Asian = DP05_0044E, Hispanic_Latino = DP05_0073E)\n\nrace2 &lt;- race %&gt;% select(STATE, White, Black, Asian, Hispanic_Latino)\n\n# merge the race data with the merged data\ndf_merged &lt;- left_join(df_merged, race2, by = \"STATE\")\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n\nShape of Data: 51 30\n\n\n\nA grouped_df: 6 x 30\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\n...\ncovid_cases_percentage\ncovid_deaths_percentage\nHighSchoolGraduates\nBachelorsDegree\nAssociatesDegree\nSomeCollege\nWhite\nBlack\nAsian\nHispanic_Latino\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n...\n32.76\n0.43\n1079285\n572276\n303028\n705662\n3302528\n1302035\n78893\n246477\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n...\n42.15\n0.20\n135473\n101422\n40276\n118250\n437533\n22202\n46184\n56491\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n...\n34.14\n0.46\n1168057\n985673\n467305\n1180491\n4254015\n340760\n266441\n2388520\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n...\n33.45\n0.43\n695062\n324137\n161542\n435235\n2103849\n437331\n48921\n255416\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n...\n30.81\n0.26\n5578997\n5958030\n2120275\n5287901\n15175598\n2121422\n6054038\n15732184\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n...\n30.61\n0.25\n814373\n1107309\n334157\n793438\n4106707\n235519\n190181\n1314962\n\n\n\n\n\n\n\nNormalizing Data\nNormalizing data is a vital step in the data science life cycle as it ensures that all variables contribute equitably to analysis, irrespective of their initial scale. This process not only prevents biases and inaccuracies in statistical models or machine learning algorithms but also enhances clarity in understanding differences between units of observation. By placing these units on a comparable scale, normalization facilitates more meaningful comparisons and analyses.\nThe COVID data is already normalized. Thus, we will now normalize the crime, education, and race data per state. For crime data, we take the average violent crimes and property crimes for 2020-22 and then divide that by the state population. For the education and race data, we simply take the data and divide it by the state population. Finally, for the heart death data, we take the average of heart deaths per 100k for 2020 and 2021. Below is a an example of the newly normalized data. This data is then saved to a csv file for future use.\n\n\nCode\n#normalize the crime data per state\ndf_merged$normalized_violent_crimes &lt;- round(apply(df_merged[, c(\"Violent_Crimes_2020\", \"Violent_Crimes_2021\", \"Violent_Crimes_2022\")], 1, mean, na.rm = TRUE) / df_merged$Population, 5)\ndf_merged$normalized_property_crimes &lt;- round(apply(df_merged[, c(\"Property_Crimes_2020\", \"Property_Crimes_2021\", \"Property_Crimes_2022\")], 1, mean, na.rm = TRUE) / df_merged$Population, 5)\n\n# normalize education data per state\ndf_merged$normalized_high_school &lt;- round(df_merged$HighSchoolGraduates  / df_merged$Population, 5)\ndf_merged$normalized_bachelors &lt;- round(df_merged$BachelorsDegree    / df_merged$Population, 5)\ndf_merged$normalized_associates &lt;- round(df_merged$AssociatesDegree  / df_merged$Population, 5)\ndf_merged$normalized_some_college &lt;- round(df_merged$SomeCollege     / df_merged$Population, 5)\n\n# finally, let's normalize the race data per state\ndf_merged$normalized_white &lt;- round(df_merged$White  / df_merged$Population, 5)\ndf_merged$normalized_black &lt;- round(df_merged$Black  / df_merged$Population, 5)\ndf_merged$normalized_asian &lt;- round(df_merged$Asian  / df_merged$Population, 5)\ndf_merged$normalized_hispanic_latino &lt;- round(df_merged$Hispanic_Latino  / df_merged$Population, 5)\n\n#Averaged 2020 and 2021 heart death death data\ndf_merged$HeartDeathsPer100k &lt;- ((df_merged$HeartDeathsPer100k2020 + df_merged$HeartDeathsPer100k2021 )/2)\n\ncat(\"Shape of Data:\", dim(df_merged))\nhead(df_merged)\n\n#save to csv\nwrite.csv(df_merged, file = 'cleaned.csv', row.names = FALSE)\n\n\nShape of Data: 51 41\n\n\n\nA grouped_df: 6 x 41\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\n...\nnormalized_property_crimes\nnormalized_high_school\nnormalized_bachelors\nnormalized_associates\nnormalized_some_college\nnormalized_white\nnormalized_black\nnormalized_asian\nnormalized_hispanic_latino\nHeartDeathsPer100k\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54,690\n22322\n17590\n...\n0.01773\n0.21451\n0.11374\n0.06023\n0.14025\n0.65639\n0.25878\n0.01568\n0.04899\n242.50\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74,750\n6126\n5573\n...\n0.01961\n0.18484\n0.13838\n0.05495\n0.16134\n0.59697\n0.03029\n0.06301\n0.07708\n147.25\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67,090\n35980\n30922\n...\n0.02184\n0.16268\n0.13728\n0.06508\n0.16442\n0.59249\n0.04746\n0.03711\n0.33267\n151.55\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50,780\n20363\n21271\n...\n0.02548\n0.23060\n0.10754\n0.05359\n0.14440\n0.69798\n0.14509\n0.01623\n0.08474\n226.75\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77,650\n174026\n188343\n...\n0.02197\n0.14123\n0.15083\n0.05368\n0.13387\n0.38418\n0.05370\n0.15326\n0.39827\n145.90\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83,780\n24570\n27916\n...\n0.03061\n0.14078\n0.19141\n0.05776\n0.13716\n0.70991\n0.04071\n0.03288\n0.22731\n131.60"
  },
  {
    "objectID": "5100.html#eda",
    "href": "5100.html#eda",
    "title": "Political Colors and Quality of Life: Analyzing How State Political Leanings Influence Citizen Well-Being",
    "section": "EDA",
    "text": "EDA\nExploratory Data Analysis (EDA) is a fundamental starting point in data analysis, helping grasp the data’s characteristics, patterns, and possible outliers. It provides essential insights for making informed modeling decisions.\nBy analyzing the below data, we hope to gain an understanding of overall trends that can aid in refining our hypotheses.\n\n\nCode\n#rename to df for ease of use\ndf &lt;- df_merged\n\n\n\nIncome\n\n\nCode\ndf$Median.income = as.numeric(gsub(\",\", \"\", df$Median.income))\n#show summary stats\ncat(\"BASIC STATISTICS:\\n\")\ncat(\"----------------------\\n\")\nsummary(df$Median.income)\n\nggplot(df, aes(y = Median.income)) +\n  geom_boxplot(fill = \"pink\", alpha = 0.7) +\n  labs(title = \"Median Income\", y = \"Median Income\")\n\n\nBASIC STATISTICS:\n----------------------\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  45130   60360   67410   68660   78200   94790 \n\n\n\n\n\nThis box plot shows the distribution of median income. The plot has a wide interquartile range and the median value is indicated by the line inside the box at 67410.\n\n\nCode\nggplot(df, aes(x = reorder(STATE, Median.income), y = Median.income, fill = VoteOutcome)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n  coord_flip() +\n  xlab(\"State\") +\n  ylab(\"Median Income\") +\n  ggtitle(\"Median Income by State (Colored by Political Affiliation)\")\n\n\n\n\n\nThis is a bar chart showing median income by state, with the color indicating vote outcome. The states at the top, which have higher median incomes, tend to have voted Democratic, while states at the bottom with lower median incomes tend to have voted Republican.\n\n\nCode\nggplot(df, aes(x = VoteOutcome, y = Median.income, fill = VoteOutcome)) +\n    geom_boxplot() +\n    scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n    xlab(\"Political Affiliation\") +\n    ylab(\"Median Income\") +\n    ggtitle(\"Median Income by Political Affiliation\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\nThis is a boxplot comparing median income distributions between Democratic and Republican vote outcomes. We can see that the median income for Democratic areas is higher than for Republican areas, as indicated by the position of the median line within each box. Additionally, the interquartile range for Democratic areas seems slightly broader, suggesting more variability in income.\n\n\nCode\nggplot(df, aes(x = DemPercentage, y = Median.income, color = VoteOutcome)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") +\n    scale_color_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n    xlab(\"Percentage of Vote for Winning Party\") +\n    ylab(\"Median Income\") +\n    ggtitle(\"Median Income vs Percentage of Vote\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis is a scatter plot with a trend line comparing median income to the percentage of vote for the winning party, again distinguished by vote outcome. This plot suggests a positive correlation for both Democratic and Republican areas, meaning that as the percentage of vote for the winning party increases, the median income also tends to increase. The shaded areas around the trend lines are the confidence intervals, which seem to be broader for Republican areas, suggesting more variability or less certainty in the trend compared to the Democratic areas.\n\n\nCrime\n\n\nCode\n#show summary stats\ncat(\"BASIC STATISTICS:\\n\")\ncat(\"----------------------\\n\")\ncat(\"Violet Crimes:\")\nsummary(df$normalized_violent_crimes)\n\ncat(\"\\nProperty Crimes:\")\nsummary(df$normalized_property_crimes)\n\nggplot(df, aes(x = VoteOutcome, y = normalized_violent_crimes, fill = VoteOutcome)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n  labs(title = \"Violent Crime Rates by Political Affiliation\", x = \"Political Affiliation\", y = \"Normalized Violent Crimes\") +\n  theme(legend.position = \"none\")\n\n\nBASIC STATISTICS:\n----------------------\nViolet Crimes:\nProperty Crimes:\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.001090 0.002755 0.003360 0.003835 0.004560 0.009410 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.01021 0.01517 0.01866 0.01937 0.02239 0.03801 \n\n\n\n\n\nThe box plot displays that the median violent crime rate for Republican states is higher than Democratic states. Two Democratic points are significant outliers as well, which drove the Democratic mean higher even though it is still lower than the Republican states.\n\n\nCode\nggplot(df, aes(x = VoteOutcome, y = normalized_property_crimes, fill = VoteOutcome)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n  labs(title = \"Property Crime Rates by Political Affiliation\", x = \"Political Affiliation\", y = \"Normalized Property Crimes\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nThe second box plot shows the distribution for non-violent crime rates. Again, we see that Democratic states overall have lower crime rates, with the property crimes making the difference even more obvious.\n\n\nHealth\n\n\nCode\nhealth &lt;- df[c(\"STATE\", \"BidenVotes\", \"TrumpVotes\", \"VoteOutcome\", \"Population\", \n                'HeartDeathsPer100k2020', \"HeartDeathsPer100k2021\", \"covid_cases_percentage\", \"covid_deaths_percentage\")]\n\n#show summary stats\ncat(\"BASIC STATISTICS:\\n\")\ncat(\"----------------------\\n\")\nprint(summary(health))\n\n\nBASIC STATISTICS:\n----------------------\n    STATE             BidenVotes         TrumpVotes      VoteOutcome       \n Length:51          Min.   :   73491   Min.   :  18586   Length:51         \n Class :character   1st Qu.:  399258   1st Qu.: 473638   Class :character  \n Mode  :character   Median :  856034   Median :1020280   Mode  :character  \n                    Mean   : 1586213   Mean   :1449562                     \n                    3rd Qu.: 2375907   3rd Qu.:1791166                     \n                    Max.   :11110639   Max.   :6006518                     \n                                                                           \n   Population       HeartDeathsPer100k2020 HeartDeathsPer100k2021\n Min.   :  577605   Min.   :118.1          Min.   :123.9         \n 1st Qu.: 1820311   1st Qu.:146.3          1st Qu.:155.2         \n Median : 4507445   Median :162.4          Median :167.8         \n Mean   : 6500226   Mean   :169.0          Mean   :177.1         \n 3rd Qu.: 7451987   3rd Qu.:183.9          3rd Qu.:194.2         \n Max.   :39501653   Max.   :245.6          Max.   :264.2         \n                    NA's   :1              NA's   :1             \n covid_cases_percentage covid_deaths_percentage\n Min.   :22.19          Min.   :0.1300         \n 1st Qu.:28.36          1st Qu.:0.2650         \n Median :31.60          Median :0.3500         \n Mean   :31.11          Mean   :0.3308         \n 3rd Qu.:33.56          3rd Qu.:0.3950         \n Max.   :42.15          Max.   :0.4600         \n                                               \n\n\n\n\nCode\nggplot(df, aes(x = DemPercentage, y = covid_cases_percentage, color = VoteOutcome)) +\n    geom_point() +\n    #geom_smooth(method = \"lm\") +\n    scale_color_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n    xlab(\"Percentage of Vote for Democratic Party\") +\n    ylab(\"COVID-19 Cases (percent of population)\") +\n    ggtitle(\"Covid Case Rates vs Percentage of Vote\")\n    \ndf$covid_deaths_percentage &lt;- df$covid_deaths_percentage * 100\n\nggplot(df, aes(x = DemPercentage, y = covid_deaths_percentage, color = VoteOutcome)) +\n    geom_point() +\n    #geom_smooth(method = \"lm\") +\n    scale_color_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n    xlab(\"Percentage of Vote for Democratic Party\") +\n    ylab(\"COVID-19 Deaths (percent of population)\") +\n    ggtitle(\"Covid Death Rates vs Percentage of Vote\")\n\nggplot(df, aes(x = DemPercentage, y = HeartDeathsPer100k, color = VoteOutcome)) +\n    geom_point() +\n    #geom_smooth(method = \"lm\") +\n    scale_color_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\") +\n    xlab(\"Percentage of Vote for Democratic Party\") +\n    ylab(\"Heart Disease Deaths (per 100k)\") +\n    ggtitle(\"Heart Disease Death Rates per 100K vs Percentage of Vote\",\n    subtitle = \"Average of 2020 and 2021 Data\")\n\n\n\n\n\nWarning message:\n\"Removed 1 rows containing missing values (`geom_point()`).\"\n\n\n\n\n\n\n\n\nIt seems there may be a difference in the disease/sickness death rates for Democrat and Republican states examining the scatterplots. There are more Republican states with higher rates of Covid, and more Democratic states with lower rates of Covid. The heart disease deaths per 100k people for each state shows a similar distribution. When looking at the Covid death rate though, the difference between Democratic and Republican states becomes less evident, if a difference exists at all.\n\n\nCode\nggplot(df, aes(x = VoteOutcome, y = covid_cases_percentage, fill = VoteOutcome)) +\n    geom_boxplot() +\n    scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n    xlab(\"Political Affiliation\") +\n    ylab(\"COVID-19 Cases (percent of population)\") +\n    ggtitle(\"Covid Case Rates by Political Affiliation\") +\n    theme(legend.position = \"none\")\n\ndf$covid_deaths_percentage &lt;- df$covid_deaths_percentage * 100\n\nggplot(df, aes(x = VoteOutcome, y = covid_deaths_percentage, fill = VoteOutcome)) +\n    geom_boxplot() +\n    scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n    xlab(\"Political Affiliation\") +\n    ylab(\"COVID-19 Deaths (percent of population)\") +\n    ggtitle(\"Covid Death Rates by Political Affiliation\") +\n    theme(legend.position = \"none\")\n\nggplot(df, aes(x = VoteOutcome, y = HeartDeathsPer100k, fill = VoteOutcome)) +\n    geom_boxplot() +\n    scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n    xlab(\"Political Affiliation\") +\n    ylab(\"Heart Disease Deaths (per 100k)\") +\n    ggtitle(\"Heart Disease Death Rates per 100K by Political Affiliation\",\n    subtitle = \"Average of 2020 and 2021 Data\") + \n    theme(legend.position = \"none\")\n\n\n\n\n\nWarning message:\n\"Removed 1 rows containing non-finite values (`stat_boxplot()`).\"\n\n\n\n\n\n\n\n\nThe box plots now clearly reveal that states that voted republican have higher rates of covid cases,covid deaths, and heart disease deaths.\n\n\nCode\nggplot(df, aes(x = reorder(STATE, covid_cases_percentage), y = covid_cases_percentage, fill = VoteOutcome)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n  coord_flip() +\n  xlab(\"State\") +\n  ylab(\"COVID-19 Cases (percent of population)\") +\n  ggtitle(\"COVID-19 Cases by State (Colored by Political Affiliation)\") +\n  theme(legend.position = \"none\")\n\ndf$covid_deaths_percentage &lt;- df$covid_deaths_percentage * 100\n\nggplot(df, aes(x = reorder(STATE, covid_deaths_percentage), y = covid_deaths_percentage, fill = VoteOutcome)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n  coord_flip() +\n  xlab(\"State\") +\n  ylab(\"COVID-19 Deaths (percent of population)\") +\n  ggtitle(\"COVID-19 Deaths by State (Colored by Political Affiliation)\") +\n  theme(legend.position = \"none\")\n\nggplot(df, aes(x = reorder(STATE, HeartDeathsPer100k), y = HeartDeathsPer100k, fill = VoteOutcome)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\")) +\n  coord_flip() +\n  xlab(\"State\") +\n  ylab(\"Heart Disease Deaths (per 100k)\") +\n  ggtitle(\"Heart Disease Deaths by State (Colored by Political Affiliation)\",\n           subtitle = \"Average of 2020 and 2021 Data\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nWarning message:\n\"Removed 1 rows containing missing values (`position_stack()`).\"\n\n\n\n\n\n\n\n\nThese sideways bar charts prove valuable in helping us realize which states had the highest disease rates and what their political affiliation was. For Covid case, Covid death, and heart disease death rate, there are mostly Republican states in the top ten of each category.\n\n\nEducation\n\n\nCode\ndf1 &lt;- df %&gt;%\n  rename(\n    `High School Graduate` = normalized_high_school,\n    `Bachelors Degree` = normalized_bachelors,\n    `Some College` = normalized_some_college,\n    `Associates Degree` = normalized_associates\n  )\n\neducation &lt;- gather(df1, key = \"EducationLevel\", value = \"EducationCount\", \n                    'High School Graduate', 'Bachelors Degree', 'Some College', 'Associates Degree')\n\nggplot(education, aes(x = EducationLevel, y = EducationCount, fill = VoteOutcome)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democratic\" = \"blue\"), name = \"Political Affiliation\" ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Education Level\") +\n  ylab(\"Normalized Education Percentage of Individuals\") +\n  ggtitle(\"Normalized Education Levels by Political Affiliation\")\n\n\n\n\n\n\n\nCode\nhead(education)\n\n\n\nA grouped_df: 6 x 39\n\n\nSTATE\nBidenVotes\nTrumpVotes\nTotalVotes\nVoteOutcome\nDemPercentage\nPopulation\nMedian.income\nViolent_Crimes_2020\nViolent_Crimes_2021\n...\nHispanic_Latino\nnormalized_violent_crimes\nnormalized_property_crimes\nnormalized_white\nnormalized_black\nnormalized_asian\nnormalized_hispanic_latino\nHeartDeathsPer100k\nEducationLevel\nEducationCount\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n...\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nAlabama\n849624\n1441170\n2290794\nRepublican\n37.09\n5031362\n54690\n22322\n17590\n...\n246477\n0.00402\n0.01773\n0.65639\n0.25878\n0.01568\n0.04899\n242.50\nnormalized_high_school\n0.21451\n\n\nAlaska\n153778\n189951\n343729\nRepublican\n44.74\n732923\n74750\n6126\n5573\n...\n56491\n0.00785\n0.01961\n0.59697\n0.03029\n0.06301\n0.07708\n147.25\nnormalized_high_school\n0.18484\n\n\nArizona\n1672143\n1661686\n3333829\nDemocratic\n50.16\n7179943\n67090\n35980\n30922\n...\n2388520\n0.00458\n0.02184\n0.59249\n0.04746\n0.03711\n0.33267\n151.55\nnormalized_high_school\n0.16268\n\n\nArkansas\n423932\n760647\n1184579\nRepublican\n35.79\n3014195\n50780\n20363\n21271\n...\n255416\n0.00678\n0.02548\n0.69798\n0.14509\n0.01623\n0.08474\n226.75\nnormalized_high_school\n0.23060\n\n\nCalifornia\n11110639\n6006518\n17117157\nDemocratic\n64.91\n39501653\n77650\n174026\n188343\n...\n15732184\n0.00470\n0.02197\n0.38418\n0.05370\n0.15326\n0.39827\n145.90\nnormalized_high_school\n0.14123\n\n\nColorado\n1804352\n1364607\n3168959\nDemocratic\n56.94\n5784865\n83780\n24570\n27916\n...\n1314962\n0.00468\n0.03061\n0.70991\n0.04071\n0.03288\n0.22731\n131.60\nnormalized_high_school\n0.14078\n\n\n\n\n\nIn this boxplot, we can see the distribution of normalized counts across different education levels, separated by vote outcome. For those with an associate’s level of education, the distribution is fairly similar for both Democratic and Republican areas, as the median line is nearly at the same level. However, the interquartile range is slightly broader for Democratic areas, indicating more variability. For those at the bachelor’s level, the median count is higher in Democratic areas than in Republican areas. The range is also broader for Democratic areas, showing a wider spread of counts. For those with high school level, the median count again appears to be higher for Democratic areas compared to Republican areas. The interquartile ranges are quite similar, suggesting similar variability between the two vote outcomes. For those with some college, we can see a higher median count for Democratic areas, with the interquartile range being wider for Democratic areas as well. Overall, the plot suggests that there is a trend where Democratic areas have a higher normalized count of individuals with higher education levels (bachelor’s and some college), while the counts are more similar for lower levels of education (high school and associates)."
  },
  {
    "objectID": "5100.html#statistical-methods",
    "href": "5100.html#statistical-methods",
    "title": "Political Colors and Quality of Life: Analyzing How State Political Leanings Influence Citizen Well-Being",
    "section": "Statistical Methods",
    "text": "Statistical Methods\n\nWelch’s T-Test\nWelch’s t-test, also called unequal variances t-test, compares the means of two populations to test the null hypothesis. Unlike the Student’s t-test, which assumes the sample means of the two populations are normally distributed and have equal variances, Welch’s t-test was designed for cases where the populations have different variances, but still assumes the samples are normally distributed. It seems easy to assume that you should use Welch’s t-test only when variances are different but it was been argued that you should always use Welch’s t-test over the Student t-test because, it has better control of type I and type II errors (2006). Welch’s t-test is calculated by dividing the difference of the two means by the square root of the sum of the square of their standard error: \\(\\frac{\\bar{X_1} - \\bar{X_1}}{\\sqrt{s_{\\bar{X_1}}^2} + {s_{\\bar{X_1}}^2}}\\) Where \\(\\bar{X_i}\\) is the mean and \\(s_{\\bar{X_i}}^2\\) is the standard error\n\n\nHotelling’s T-Squared Test\nHotelling’s T-Squared Test is used to compare the means of two groups across multiple variables simultaneously. It’s an extension of the Student’s t-test to the multivariate case, where each observation is a vector of values rather than a single scalar value. This test is useful when the variables are correlated, and it assumes that the data from both groups are drawn from multivariate normally distributed populations with equal covariance matrices. The fundamental assumptions for the test are that the samples are randomly drawn from normally distributed populations and that the samples are independent of each other. The test statistic T^2 is derived from the means, variances, and covariances of the variables, and it’s often transformed into an F-statistic for determining statistical significance.\n\n\nBootstrapped Difference of Means\nA bootstrap approach samples each observed population n times with replacement, where n is the number of entities of the population and uses the newly created datasets to find a mean. The bootstrapped mean for population two is subtracted from population one. This process of creating mean from samples and finding the difference is repeated many times creating a list of many differences of means. Net the quantiles are looked at a predetermined percentage, also referred to as the confidence interval. Common confidence intervals are 90 and 95 percent. With a confidence level of 90 percent we would look at the 5 and 95 percent quantiles. Bootstrap approaches are useful when one or both of the populations for the observed samples are small."
  }
]