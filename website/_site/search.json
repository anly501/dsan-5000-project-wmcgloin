[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "I will replace this later with my actual data"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction to the Hot Hand",
    "section": "",
    "text": "Within the world of sports, the terms “hot hand” or being “in the zone” are frequently used to describe a streak of exceptional performance by a player. The “hot hand” term originated in the sport of basketball, suggesting that a player who has made numerous successive shots is more likely to make their next basket, as opposed to the same person who had missed their last few shots. For example, a basketball player may make an exceptional number of shots in a short period of time, such as Klay Thompson scoring 37 points in a quarter. Another example is Joe DiMaggio’s 56 game hitting streak in 1941, one of the most incredible hitting runs in baseball history. Ordinarily, these types of “runs” or “streaks” are rare. There is “a belief that the performance of a player during this particular period is significantly better than expected on the basis of the player’s overall record” 1. Often, this heightened performance can be attributed to increased confidence by the player.   This belief is shared by a majority of players, coaches, and fans, yet there is little statistical evidence to support this phenomenon. In fact, a majority of studies suggest that the “hot hand” is a fallacy and advise coaches not to consider it when selecting plays. The “hot hand” phenomenon has been studied by many psychologists and statisticians who still debate this issue to this day.   While the term “hot hand” may be most commonly associated with the world of sports, “studies have been done in other academic fields outside the sports domain, such as economics and cognitive science” 2. Through this project, I plan to explore the mystery of streaks both within and outside the world of sports."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction to the Hot Hand",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilovich, Vallone, and Tversky (1985)↩︎\nBar-Eli, Avugos, and Raab (2006)↩︎"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page shows the raw data, the code used to clean it, and the modified data. It’s a journal of my data cleaning process. Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nncaahoopR\nlet’s clean the Villanova 2021-22 data with R:\nhere is a screen shot of the first few rows and columns of the raw data:  \n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\n# let's load in the data\nnova2122 &lt;- read.csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n1140539\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova2122)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova2122 &lt;- nova2122 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n539939\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova2122 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova2122.csv\", row.names = FALSE)\n\n\nHere is a screen shot of the modified data:  \n\n2019-20 Season cleaning\n\n\nCode\n# let's load in the data\nnova1920 &lt;- read.csv('./data/raw_data/villanova1920.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n958139\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova1920)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova1920 &lt;- nova1920 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n454339\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova1920 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova1920.csv\", row.names = FALSE)\n\n\n\n\n\nnewsapi\nlet’s clean this using python:  here is a picture of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n# Load the sentiment scores from the JSON file\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\n# Create lists to store data\ntitles = []  # List to store document titles\ndescriptions = []   # List to store document descriptions\nsentiment_labels = []  # List to store sentiment labels\n\n# Extract the scores, titles, descriptions, and labels\nfor idx, item in enumerate(sentiment_scores, start=1):\n    titles.append(item.get('title', ''))  # Get the title of the document\n    descriptions.append(item.get('description', ''))    # Get the description of the document\n    sentiment_score = item.get('sentiment_score', {})\n    \n    # Determine the sentiment label based on the compound score\n    if sentiment_score.get('compound', 0) &gt; 0:\n        sentiment_labels.append('positive')\n    elif sentiment_score.get('compound', 0) == 0:\n        sentiment_labels.append('neutral')\n    else:\n        sentiment_labels.append('negative')\n\n# Create a DataFrame\ndata = {\n    'Title': titles,\n    'Description': descriptions,\n    'Sentiment Label': sentiment_labels\n}\n\ndf_with_labels = pd.DataFrame(data)\n\n# Save to CSV\ndf_with_labels.to_csv('./data/modified_data/sentiment_scores_with_titles.csv', index=False)\n\n\n\n\nindividual player data\nlet’s clean the aaron judge game data with python:\nhere is a screen shot of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\n#reading in the file\naaronjudge = pd.read_csv('./data/raw_data/AaronJudgeData.csv')\n\n\n\n\nCode\n#how many rows are in this dataset?\naaronjudge.shape\n\n\n(111, 37)\n\n\n\n\nCode\n#what are the column names?\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1',\n       'Events', 'EV.1', 'maxEV', 'LA', 'Barrels', 'Barrel%', 'HardHit',\n       'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n#removing the repeated columns\ncolumns_to_remove = ['Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1']\naaronjudge.drop(columns=columns_to_remove, inplace=True)\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Events', 'EV.1', 'maxEV', 'LA', 'Barrels',\n       'Barrel%', 'HardHit', 'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n# i belive the initial row with the column names is repeated throughou the data. let's check\nprint((aaronjudge['Date'] == 'Date').sum())\n\n\n5\n\n\n\n\nCode\n# let's remove these rows and then check the shape again\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Date'].index, inplace=True)\naaronjudge.shape\n\n\n(106, 32)\n\n\n\n\nCode\n# there is also a total row which I want to remove as well. let's do that now\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Total'].index, inplace=True)\naaronjudge.shape\n\n\n(105, 32)\n\n\n\n\nCode\n# so far, I have removed 6 rows and 5 columns. \n\n# I want to create a \"location\" column based on the \"@\" in the \"Opp\" column\naaronjudge['location'] = aaronjudge['Opp'].apply(lambda x: 'away' if '@' in x else 'home')\n\n# Remove the \"@\" symbol from the values in the \"Opp\" column\naaronjudge['Opp'] = aaronjudge['Opp'].str.replace('@', '')\n\n# check value counts of the new \"location\" column\nprint(aaronjudge['location'].value_counts()) #this seems accurate\n\n\nhome    53\naway    52\nName: location, dtype: int64\n\n\n\n\nCode\nprint(aaronjudge['PA'].dtype)\nprint(aaronjudge['BB%'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# I want to create two new columns. The number of at bats per each game and the number of hard hits in each game. \n# for this project, we are going to calculate at-bats as should be the number of plate appearances minus walks (sacrifices and HBP are not included in this dataset)\n\n#first i have to remove the '%' symbol and convert 'BB%' to a float\n\naaronjudge['BB%'] = aaronjudge['BB%'].astype(str)\naaronjudge['BB%'] = aaronjudge['BB%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'BB%' column to three decimal places\naaronjudge['BB%'] = aaronjudge['BB%'].round(3)\n\n#print(aaronjudge['BB%'].mean())\n\n#convert 'PA' to a float\naaronjudge['PA'] = aaronjudge['PA'].astype(float)\n\n# now I can create the new at_bats column\naaronjudge['at_bats'] = aaronjudge['PA'] * (1 - aaronjudge['BB%'])\n\n#now lets see the average number of at bats vs the average number of plate appearances\nprint(aaronjudge['at_bats'].mean())\nprint(aaronjudge['PA'].mean())\n\n\n3.4857333333333336\n4.314285714285714\n\n\n\n\nCode\n# now I want to create a new column for hard hits per game\n# we can do this by multiplying the hard hit percentage by the events column (these columns were part of a different table that was merged with the original table)\n\nprint(aaronjudge['HardHit%'].dtype)\nprint(aaronjudge['Events'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# this code is very similar to what we just did\n\n#first i have to remove the '%' symbol and convert 'HardHit%' to a float\n\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].astype(str)\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'HardHit%' column to three decimal places\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].round(3)\n\n#print(aaronjudge['HardHit%'].mean())\n\n#convert 'Events' to a float\naaronjudge['Events'] = aaronjudge['Events'].astype(float)\n\n# now I can create the new hard_hits column\naaronjudge['hard_hits'] = (aaronjudge['Events'] * aaronjudge['HardHit%']).round(0)\n\n#now lets see the average number of hard_hits per game\nprint(aaronjudge['hard_hits'].mean())\n\n\n1.52\n\n\n\n\nCode\n# finally, let's create a correct hardhit% column that is based on the number of at-bats, not the number of times a player puts the ball in play\naaronjudge['correct_hardhit%'] = (aaronjudge['hard_hits'] / aaronjudge['at_bats']).round(2)\n\n# now let's see the average correct hardhit% for Aaron Judge\nprint(aaronjudge['correct_hardhit%'].mean())\n\n\n0.42829999999999996\n\n\n\n\nCode\n# sometimes in certain stadiums or based on the weather, the HardHit% data is missing\n# this causes the value of the new correct_hardhit% column to be NaN, so let's remove those few rows\naaronjudge.dropna(subset=['correct_hardhit%'], inplace=True)\n\n#let's check the shape again\naaronjudge.shape #loss of 5 rows\n\n\n(100, 36)\n\n\n\n\nCode\n# now we can save this to a csv file\naaronjudge.to_csv('./data/modified_data/aaronjudge.csv', index=False)\n\n\nhere is a screenshot of the first couple rows of the modified csv file:  \n\n\nBaseballr\n\n\nCode\nlibrary(tidyverse)\nbaseball &lt;- read.csv(\"./data/raw_data/baseballr_six_games.csv\")\nhead(baseball)\n\n\n\nA data.frame: 6 x 160\n\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n\n&lt;int&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n1\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n4\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n5\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n6\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\n# what are the column names?\ncolnames(baseball)\n\n\n\n'game_pk''game_date''index''startTime''endTime''isPitch''type''playId''pitchNumber''details.description''details.event''details.awayScore''details.homeScore''details.isScoringPlay''details.hasReview''details.code''details.ballColor''details.isInPlay''details.isStrike''details.isBall''details.call.code''details.call.description''count.balls.start''count.strikes.start''count.outs.start''player.id''player.link''pitchData.strikeZoneTop''pitchData.strikeZoneBottom''details.fromCatcher''pitchData.coordinates.x''pitchData.coordinates.y''hitData.trajectory''hitData.hardness''hitData.location''hitData.coordinates.coordX''hitData.coordinates.coordY''actionPlayId''details.eventType''details.runnerGoing''position.code''position.name''position.type''position.abbreviation''battingOrder''atBatIndex''result.type''result.event''result.eventType''result.description''result.rbi''result.awayScore''result.homeScore''about.atBatIndex''about.halfInning''about.inning''about.startTime''about.endTime''about.isComplete''about.isScoringPlay''about.hasReview''about.hasOut''about.captivatingIndex''count.balls.end''count.strikes.end''count.outs.end''matchup.batter.id''matchup.batter.fullName''matchup.batter.link''matchup.batSide.code''matchup.batSide.description''matchup.pitcher.id''matchup.pitcher.fullName''matchup.pitcher.link''matchup.pitchHand.code''matchup.pitchHand.description''matchup.splits.batter''matchup.splits.pitcher''matchup.splits.menOnBase''batted.ball.result''home_team''home_level_id''home_level_name''home_parentOrg_id''home_parentOrg_name''home_league_id''home_league_name''away_team''away_level_id''away_level_name''away_parentOrg_id''away_parentOrg_name''away_league_id''away_league_name''batting_team''fielding_team''last.pitch.of.ab''pfxId''details.trailColor''details.type.code''details.type.description''pitchData.startSpeed''pitchData.endSpeed''pitchData.zone''pitchData.typeConfidence''pitchData.plateTime''pitchData.extension''pitchData.coordinates.aY''pitchData.coordinates.aZ''pitchData.coordinates.pfxX''pitchData.coordinates.pfxZ''pitchData.coordinates.pX''pitchData.coordinates.pZ''pitchData.coordinates.vX0''pitchData.coordinates.vY0''pitchData.coordinates.vZ0''pitchData.coordinates.x0''pitchData.coordinates.y0''pitchData.coordinates.z0''pitchData.coordinates.aX''pitchData.breaks.breakAngle''pitchData.breaks.breakLength''pitchData.breaks.breakY''pitchData.breaks.spinRate''pitchData.breaks.spinDirection''hitData.launchSpeed''hitData.launchAngle''hitData.totalDistance''injuryType''umpire.id''umpire.link''details.isOut''pitchData.breaks.breakVertical''pitchData.breaks.breakVerticalInduced''pitchData.breaks.breakHorizontal''isBaseRunningPlay''details.disengagementNum''isSubstitution''replacedPlayer.id''replacedPlayer.link''result.isOut''about.isTopInning''matchup.postOnFirst.id''matchup.postOnFirst.fullName''matchup.postOnFirst.link''matchup.postOnSecond.id''matchup.postOnSecond.fullName''matchup.postOnSecond.link''matchup.postOnThird.id''matchup.postOnThird.fullName''matchup.postOnThird.link''reviewDetails.isOverturned''reviewDetails.inProgress''reviewDetails.reviewType''reviewDetails.challengeTeamId''base''details.violation.type''details.violation.description''details.violation.player.id''details.violation.player.fullName'\n\n\n\n\nCode\n# missing data?\ndata.frame(colSums(is.na(baseball)))\n\n\n\nA data.frame: 160 x 1\n\n\n\ncolSums.is.na.baseball..\n\n\n\n&lt;dbl&gt;\n\n\n\n\ngame_pk\n0\n\n\ngame_date\n0\n\n\nindex\n0\n\n\nstartTime\n0\n\n\nendTime\n0\n\n\nisPitch\n0\n\n\ntype\n0\n\n\nplayId\n215\n\n\npitchNumber\n242\n\n\ndetails.description\n0\n\n\ndetails.event\n1755\n\n\ndetails.awayScore\n1755\n\n\ndetails.homeScore\n1755\n\n\ndetails.isScoringPlay\n1755\n\n\ndetails.hasReview\n0\n\n\ndetails.code\n215\n\n\ndetails.ballColor\n243\n\n\ndetails.isInPlay\n242\n\n\ndetails.isStrike\n242\n\n\ndetails.isBall\n242\n\n\ndetails.call.code\n242\n\n\ndetails.call.description\n242\n\n\ncount.balls.start\n0\n\n\ncount.strikes.start\n0\n\n\ncount.outs.start\n0\n\n\nplayer.id\n1790\n\n\nplayer.link\n1790\n\n\npitchData.strikeZoneTop\n243\n\n\npitchData.strikeZoneBottom\n243\n\n\ndetails.fromCatcher\n1943\n\n\n...\n...\n\n\numpire.link\n1970\n\n\ndetails.isOut\n0\n\n\npitchData.breaks.breakVertical\n243\n\n\npitchData.breaks.breakVerticalInduced\n243\n\n\npitchData.breaks.breakHorizontal\n243\n\n\nisBaseRunningPlay\n1951\n\n\ndetails.disengagementNum\n1904\n\n\nisSubstitution\n1908\n\n\nreplacedPlayer.id\n1948\n\n\nreplacedPlayer.link\n1948\n\n\nresult.isOut\n0\n\n\nabout.isTopInning\n0\n\n\nmatchup.postOnFirst.id\n1837\n\n\nmatchup.postOnFirst.fullName\n1837\n\n\nmatchup.postOnFirst.link\n1837\n\n\nmatchup.postOnSecond.id\n1903\n\n\nmatchup.postOnSecond.fullName\n1903\n\n\nmatchup.postOnSecond.link\n1903\n\n\nmatchup.postOnThird.id\n1930\n\n\nmatchup.postOnThird.fullName\n1930\n\n\nmatchup.postOnThird.link\n1930\n\n\nreviewDetails.isOverturned\n1965\n\n\nreviewDetails.inProgress\n1965\n\n\nreviewDetails.reviewType\n1965\n\n\nreviewDetails.challengeTeamId\n1965\n\n\nbase\n1965\n\n\ndetails.violation.type\n1969\n\n\ndetails.violation.description\n1969\n\n\ndetails.violation.player.id\n1969\n\n\ndetails.violation.player.fullName\n1969\n\n\n\n\n\nUpon examination of the data, it seems insufficient for analyzing the hot hand phenomenon for this study. The preceding individual player data appears to be more appropriate for a comprehensive analysis of this topic, as it includes relevant metrics such as hard-hit percentage.\n\n\nExtra Joke\nWhat did the broom say to the vacuum?\n“I’m so tired of people pushing us around.”"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Why did the linear regression model break up with the logistic regression model? Because it wanted a more ‘linear’ relationship!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello!\nHello and welcome to my website! My name is Billy and I am a graduate student at Georgetown University. I am currently pursuing a Master’s in Data Science and Analytics and will use this website as a place to display my work. I hope you find my work insightful and enjoy your time on my website!\n\n\nGrowing Up\nI grew up in Scotch Plains, New Jersey along with my two sisters, Melissa and Jessica. We are triplets! Below is a picture of me with my family and another of our dog Roxie.\n\n \n\n\n\nEducation\nI graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics along with a PPE (Politics, Philosophy, & Economics) Concentration and Political Science minor. I absolutely loved my time at Villanova between the academics, friendships, the school winning the NCAA Championship and more!. As an undergrad, I was involved in a number of student organizations, including the Blue Key Society, where I served as a tour guide for undergraduate admissions. I also organized accepted students days for the university.\nOne of the best experiences I had during college was studying abroad. In 2019, I participated in the Institute of Politics and Economics Program at the University of Cambridge where I took a number of economic classes and study international relations and history. Seeing how these subject matters interact with analytics truly fascinated me, which is one reason why I am pursuing an advanced degree in data science. I am extremely excited to continue my education and pursue a masters degree in data science!\n\n\nTop Shelf Designs, LLC\nDuring my senior year at Villanova, I had the opportunity to start my own business with my sisters! Our business, Top Shelf Designs, LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. Being an entrepreneur has been an incredibly rewarding journey thus far, and I am eager to see where it takes us. Here is a link to our website!\n\n\nHobbies\nI’m an avid sports fan, and love watching the New York Yankees, Villanova basketball, and NY Giants. Unfortunately, none of my teams have been doing too well lately, but I’m hoping the Giants can turn it around this season! I’m fascinated by how analytics is being used to transform the sports landscape - even across college sports! While at Villanova, I researched the Houston Astros Cheating Scandal. I would love to go back and take an even deeper dive into the subject one day!\nI love music - you can often find me with headphones or earbuds in while doing work or exploring the DC area! Additionally, I find playing instruments to be a great creative outlet after a long day of work. I grew up taking piano lessons and during COVID started to teach myself how to play the guitar!\nWhenever I have the chance, I try to travel. Wile abroad, I was lucky enough to travel to a few places in Europe. My favorite spot has to be Italy - from the history and culture to the food, wine, and sports, I am counting down the days until I can go back.\nFinally, I thoroughly enjoy books, movies, video games in addition to fun, in-person experiences. Lately, I’ve been exploring the DC area which I am loving so far! If you have any recommendations - please let me know! I’m alwyas willing to try something new!\n\n\nGeneral Info (inc. netID)\n\n\n\nname\nBilly McGloin\n\n\nGU netID\nwtm30\n\n\nLinkedIn\nlink\n\n\n\n\n\nJokes\nI always like to have a laugh so below are some jokes I hope you’ll enjoy!\n\nWebsite Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki\n\n\n\nAssorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!\n\n\n\n\nHoya Saxa!\n\n\n\nme and a bunch of rocks!"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM",
    "section": "",
    "text": "Why did the arm apply for a job? Because it wanted to lend a helping hand!  Why did the naive Bayesian suddenly feel patriotic when he heard fireworks? He assumed independence.  I was surprised when my niece said she learned R at school yesterday, and then I remembered she’s 4 and she meant the letter. My priors are all too skewed."
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Introduction\nOn this page, our attention turns to the implementation of Decision Trees and Random Forest using the same NCAA shot data from the 2021-22 Villanova season that was utilized in the clustering and dimensionality reduction tabs. We’ll start by loading relevant libraries and reading in the data.\n\n\nCode\n# import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# read in the data\ndf = pd.read_csv('./data/modified_data/nova_final.csv')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2764 entries, 0 to 2763\nData columns (total 7 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   shot_value             2764 non-null   int64  \n 1   field_goal_percentage  2764 non-null   float64\n 2   lag1                   2764 non-null   float64\n 3   home_crowd             2764 non-null   int64  \n 4   score_diff             2764 non-null   int64  \n 5   game_num               2764 non-null   int64  \n 6   shot_outcome_numeric   2764 non-null   int64  \ndtypes: float64(2), int64(5)\nmemory usage: 151.3 KB\n\n\n\n\nBaseline Comparisons\nLet’s start by examining the distribution of class labels, specifically distinguishing between made and missed shots:\n\n\nCode\n# Compute the distribution of class labels\nclass_distribution = df['shot_outcome_numeric'].value_counts()\n\n# Print the distribution\nprint(\"Class Distribution:\")\nprint(class_distribution)\n\n\nClass Distribution:\n1    1448\n0    1316\nName: shot_outcome_numeric, dtype: int64\n\n\n\n\nCode\n# Plot the class distribution\nplt.figure(figsize=(8, 6))\nsns.countplot(x='shot_outcome_numeric', data=df, palette='viridis')\nplt.title('Class Distribution of shot_outcome_numeric')\nplt.xlabel('Class Label')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\nThere is a slight imbalance in class distribution for the shot_outcome_numeric target variable (class label). There are 1448 instances of ‘made shot’ (Class Label ‘1’) and 1316 instances of ‘missed shot’ (Class Label ‘0’). This class imbalance can influence the performance of the classification model. It’s essential to consider metrics beyond accuracy, such as precision and recall of the data, to gain a comprehensive understanding. Additionally, it’s worth noting that a model predicting ‘1’ for every instance would achieve 52.4% accuracy, so any model with an accuracy below this threshold does not provide meaningful predictions. Below is code that runs a random classifier on the data to see if it can beat the 52.4% accuracy threshold.\n\n\nCode\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Random Classifier function\ndef random_classifier(y_data):\n    ypred = np.random.randint(2, size=len(y_data))  # Random predictions (0 or 1)\n    \n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"Count of Predictions:\", Counter(ypred).values())\n    print(\"Probability of Predictions:\", np.fromiter(Counter(ypred).values(), dtype=float) / len(y_data))\n    \n    accuracy = accuracy_score(y_data, ypred)\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_data, ypred)\n    \n    print(\"Accuracy:\", accuracy)\n    print(\"Precision (Class 0, Class 1):\", precision)\n    print(\"Recall (Class 0, Class 1):\", recall)\n    print(\"F1-score (Class 0, Class 1):\", fscore)\n\n# Using the 'shot_outcome_numeric' column as labels\ny = df['shot_outcome_numeric']\n\n# Running the random classifier\nrandom_classifier(y)\n\n\n-----RANDOM CLASSIFIER-----\nCount of Predictions: dict_values([1357, 1407])\nProbability of Predictions: [0.49095514 0.50904486]\nAccuracy: 0.5032561505065123\nPrecision (Class 0, Class 1): [0.47974414 0.52763449]\nRecall (Class 0, Class 1): [0.51291793 0.49447514]\nF1-score (Class 0, Class 1): [0.49577672 0.51051693]\n\n\nThe random classifier acts as a rudimentary model that makes predictions through random guessing. Its primary role is to provide a fundamental benchmark for assessing the efficacy of more sophisticated models. Specifically applied to predicting basketball shot outcomes, this classifier yields an accuracy of approximately 50%, falling short of consistently predicting ‘1’ (as established earlier). When evaluating advanced models, the goal is for them to surpass this random guessing baseline (and the 52.4% baseline from always guessing the most common class), demonstrating their ability to discern meaningful patterns within the data.\n\n\nDecision Tree\n\nTheory\nDecision Trees are powerful machine learning algorithms used for classification and regression tasks, providing a hierarchical structure whereby nodes represent decisions based on specific features. The dataset is recursively split at decision nodes using criteria like Gini impurity for classification. Each leaf node corresponds to a class label in classification or a numerical value in regression, making the final predictions.  The simplicity and interpretability of Decision Trees make them widely applicable, but their susceptibility to overfitting necessitates techniques like pruning. Pruning involves removing non-contributive parts of the tree. Decision Trees offer insights into feature importance, with features near the top of the tree contributing more to predictive power.  Applying Decision Trees to real-world scenarios involves defining rules for classification based on the dataset’s features. These rules provide a transparent decision-making process, making Decision Trees particularly useful for explaining predictions to non-experts. In the context of the basketball data I have been using, Decision Trees could form rules like “if the shot value is less than 2, classify it as a made shot.” Feature selection and hyperparameter tuning further refine these trees to better fit the data, exemplifying their adaptability to various applications. The Gini Index is a common measure used in Decision Trees to determine optimal splits, minimizing impurity and enhancing predictive accuracy. Despite their advantages, Decision Trees’ potential overfitting is mitigated by minimizing the number of layers or employing advanced techniques.\n\n\nImplementation\nIn the following code, we begin by importing the necessary libraries. Subsequently, we partition the data into subsets, distinguishing features and the target variable. Further, we perform another split on the data into training and test sets, allowing us to train a Decision Tree model on the training data and assess its performance using the test data. We then print the accuracy value of this model on the test data.\n\n\nCode\n# laod in relevant libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics \n\n# split data in features and target variable\nfeature_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num']\nX = df[feature_columns].copy()\n\ntarget_column = ['shot_outcome_numeric']\nY = df[target_column].copy()\n\n# Split data into training and test set\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # 70% training and 30% test\n\n# Create Decision Tree classifer object\nmodel = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nmoodel = model.fit(X_train,Y_train)\n\n#Predict the response for test dataset\nY_pred = model.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n\n\nAccuracy: 0.5590361445783133\n\n\nThis decision tree did better than both baseline comparisons (both the random classifier and always guessing the most common class label). However, it is still not a great model; the accuracy only slightly improved to 56%. Let’s visualize the model below.\n\n\nCode\nfrom sklearn.tree import plot_tree\n\ndef custom_plot_tree(model, X, Y):\n    plt.figure(figsize=(22.5, 15))\n    plot_tree(model, feature_names=X.columns, filled=True)\n    plt.show()\n\ncustom_plot_tree(model, X_train, Y_train)\n\n\n\n\n\nThe initial decision tree above seems excessively large, indicating potential overfitting to the data. To address this, we’ll adjust the hyperparameters below.\n\n\nHyper-Parameter Tuning\nThe provided code iterates through different values for the hyperparameter (number of layers) and generates three plots to visualize the model’s performance. The plots show the accuracy and recall for both training and test datasets, with varying numbers of layers in the Decision Tree (controlled by the max_depth hyperparameter). The blue lines represent training results, while the red lines depict test results.\n\n\nCode\nfrom sklearn.metrics import accuracy_score, recall_score\n\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train, Y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(Y_test, yp_test),recall_score(Y_test, yp_test,pos_label=0),recall_score(Y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(Y_train, yp_train),recall_score(Y_train, yp_train,pos_label=0),recall_score(Y_train, yp_train,pos_label=1)])\n\n# Extracting data\nnum_layers = [result[0] for result in test_results]\ntrain_accuracy_values = [result[1] for result in train_results]\ntest_accuracy_values = [result[1] for result in test_results]\ntrain_recall_0_values = [result[2] for result in train_results]\ntest_recall_0_values = [result[2] for result in test_results]\ntrain_recall_1_values = [result[3] for result in train_results]\ntest_recall_1_values = [result[3] for result in test_results]\n\n# Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_accuracy_values, label='Train Accuracy', marker='o', color='blue')\nplt.plot(num_layers, test_accuracy_values, label='Test Accuracy', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Recall Y = 0\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_0_values, label='Train Recall y=0', marker='o', color='blue')\nplt.plot(num_layers, test_recall_0_values, label='Test Recall y=0', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall Y = 0')\nplt.legend()\nplt.show()\n\n# Recall Y = 1\nplt.figure(figsize=(10, 6))\nplt.plot(num_layers, train_recall_1_values, label='Train Recall y=1', marker='o', color='blue')\nplt.plot(num_layers, test_recall_1_values, label='Test Recall y=1', marker='o', color='red')\nplt.xlabel('Number of Layers in Decision Tree (max_depth)')\nplt.ylabel('Recall Y = 1')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Optimal Model\nBased on the graphs, it appears that an optimal model would have three layers (a depth of 3). Consequently, we will re-train the decision tree using this optimal hyperparameter obtained from the plot above in the code below. Additionally, in the code chunk we will define and use a function that generates a confusion matrix plot, display metrics, and decision tree.\n\n\nCode\nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(X_train, Y_train)\n\nyp_train=model.predict(X_train)\nyp_test=model.predict(X_test)\n\n#function which generates a confusion matrix plot and prints information\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndef confusion_plot(y_true, y_pred):\n    # metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, _, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n\n    # plot\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    matrix = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_true))\n    matrix.plot(cmap=plt.cm.Blues)\n\n    # formatting\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {recall[0]}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {precision[0]}\")\n    print(f\"POSITIVE RECALL (Y=1): {recall[1]}\")\n    print(f\"POSITIVE PRECISION (Y=1): {precision[1]}\")\n    print(cm)\n\nconfusion_plot(Y_test,yp_test)\nplt.show()\ncustom_plot_tree(model,X,Y)\n\n\nACCURACY: 0.655421686746988\nNEGATIVE RECALL (Y=0): 0.7300771208226221\nNEGATIVE PRECISION (Y=0): 0.610752688172043\nPOSITIVE RECALL (Y=1): 0.5895691609977324\nPOSITIVE PRECISION (Y=1): 0.7123287671232876\n[[284 105]\n [181 260]]\n\n\n\n\n\n\n\n\n\n\nCode\ntree_summary = tree.export_text(model, feature_names=X_train.columns.tolist())\nprint(tree_summary)\n\n\n|--- shot_value &lt;= 1.50\n|   |--- score_diff &lt;= 10.50\n|   |   |--- game_num &lt;= 11.50\n|   |   |   |--- class: 1\n|   |   |--- game_num &gt;  11.50\n|   |   |   |--- class: 1\n|   |--- score_diff &gt;  10.50\n|   |   |--- game_num &lt;= 17.50\n|   |   |   |--- class: 1\n|   |   |--- game_num &gt;  17.50\n|   |   |   |--- class: 1\n|--- shot_value &gt;  1.50\n|   |--- shot_value &lt;= 2.50\n|   |   |--- field_goal_percentage &lt;= 0.54\n|   |   |   |--- class: 0\n|   |   |--- field_goal_percentage &gt;  0.54\n|   |   |   |--- class: 1\n|   |--- shot_value &gt;  2.50\n|   |   |--- score_diff &lt;= 8.50\n|   |   |   |--- class: 0\n|   |   |--- score_diff &gt;  8.50\n|   |   |   |--- class: 0\n\n\n\n\n\nConclusion\nAfter tuning the hyperparameter, the model’s accuracy improved to 65.5%, marking a notable 13% enhancement compared to the baseline and a 9.5% improvement over the untuned version. Delving into the confusion matrix, it becomes evident that the model accurately predicted made shots (class 1) in 71.2% of cases, while achieving a 61.1% accuracy in identifying missed shots (class 0). This nuanced evaluation provides insights into the model’s strengths and areas for potential refinement. Notably, the absence of the ‘lag’ variable in the decision tree suggests its limited influence on the model, aligning with the initial hypothesis of its lower predictive power.\nIn summary, the hyperparameter-tuned decision tree exhibits improved accuracy and provides valuable insights into feature importance. The visualization underscores the significance of ‘shot_value’ and ‘field_goal_percentage’ in predicting shot outcomes, while the negligible role of the ‘lag’ variable aligns with expectations, emphasizing the model’s capacity to discern key predictors in the dataset.\n\n\n\nRandom Forest\nDisclaimer: Although the preceding code and analysis successfully meet the assignment requirements, I also wanted to explore the application of random forests. This section will be more concise, but includes the necessary code and some analysis for random forests.\n\nTheory\nA random forest differs from a single decision tree in that it is an ensemble or a collection of decision trees. Instead of relying on the prediction of a single tree, a random forest aggregates the predictions of multiple trees to make a more robust and accurate prediction. Every tree in the ‘random forest’ is trained on a random subset of the data and features, introducing diversity in the models. During predictions, the random forest averages or takes a vote of the individual tree predictions, reducing the risk of overfitting and improving generalization performance. This ensemble approach makes random forests particularly effective in handling complex datasets and enhancing the stability and reliability of the overall model.\n\n\nImplementation\nThe following code imports necessary libraries, splits the data into training and test sets, creates a Random Forest Classifier model, trains it on the training data, predicts outcomes on the test data, and then prints the accuracy of the model’s predictions.\n\n\nCode\n# load relevant libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint\n\n# Split the data into training and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n# Create the model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)\n\nY_pred = model.predict(X_test)\n\naccuracy = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.6365280289330922\n\n\n/var/folders/lb/dk54cbx965z7nj61zps2fzr00000gn/T/ipykernel_50519/4200927063.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, Y_train)\n\n\nWe can see that without any tuning, the random forest almost matches the accuracy of the hyperparameter-tuned decision tree. Let’s visualize the first three trees in the random forest below.\n\n\nCode\n# Export the first three decision trees from the forest\nfor i in range(3):\n    tree = model.estimators_[i]\n\n    # Plot the tree\n    plt.figure(figsize=(10, 8))\n    plot_tree(tree, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True, proportion=True)\n    plt.title(f'Decision Tree {i + 1}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyper-Parameter Tuning\nNext, we will utilize a random search with cross-validation to find the best hyperparameters within a specified range. The best model is then stored, and the optimal hyperparameters are printed.\n\n\nCode\nparam_dist = {'n_estimators': randint(50,500),\n              'max_depth': randint(1,20)}\n\n# Convert Y_train to a one-dimensional array\nY_train = Y_train.values.ravel()\n\n# Create a random forest classifier\nmodel = RandomForestClassifier()\n\n# Use random search to find the best hyperparameters\nrand_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=5)\n\n# Fit the random search object to the data\nrand_search.fit(X_train, Y_train)\n\n# Create a variable for the best model\nbest_rf = rand_search.best_estimator_\n\n# Print the best hyperparameters\nprint('Best hyperparameters:', rand_search.best_params_)\n\n\nBest hyperparameters: {'max_depth': 4, 'n_estimators': 417}\n\n\n\n\nTraining Optimal Model\nUsing the above hyperparameters, let’s train the optimal random forest model.\n\n\nCode\n# Predictions on the test set\nyp_test = best_rf.predict(X_test)\n\n\n\n\nCode\nconfusion_plot(Y_test, yp_test)\n\n\nACCURACY: 0.6672694394213382\nNEGATIVE RECALL (Y=0): 0.861003861003861\nNEGATIVE PRECISION (Y=0): 0.601078167115903\nPOSITIVE RECALL (Y=1): 0.4965986394557823\nPOSITIVE PRECISION (Y=1): 0.8021978021978022\n[[223  36]\n [148 146]]\n\n\n\n\n\n\n\nCode\n# Create a series containing feature importances from the model and feature names from the training data\nfeature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\n# bar chart\nfeature_importances.plot.bar()\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nConclusion\nThe tuned Random Forest model exhibits improved accuracy compared to the tuned Decision Tree, achieving an accuracy of 66.7%. Notably, the Random Forest model demonstrates higher positive recall (Y=1) and precision, indicating enhanced performance in correctly identifying instances of made shots.\nThe above code calculates the feature importance from the tuned Random Forest model and creates a bar chart to visualize the importance of each feature in predicting shot outcomes. The resulting chart helps identify which features have the most significant impact on the model’s decision-making process. The bar chart reveals, expectedly, that shot value has the most significant impact by a large margin. At the same time, lag1 exhibits a meager impact, aligning with similar results from the tuned decision tree.\n\n\n\nExtra Joke\nWhich dating app do trees use?\nTimber"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction\nOn this page, I’ll employ three clustering techniques to analyze Villanova’s 2021-22 NCAA shot data (the same data from the dimensionality-reduction tab), considering all six features: lag1 (previous shot), shot_value, field_goal_percentage, game_num, home_crowd, and score_diff. The primary objective is to uncover patterns within the dataset through k-means, DBSCAN, and hierarchical clustering. Each step of the process will be explained in a straightfowrad manner with interpretations of the results.\n\nImport libraries and load the dataset\n\n\nCode\n# import the necessary packages...\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# read in the feature dataset\ndf = pd.read_csv('./data/modified_data/nova_features.csv')\n\n\n\n\n\nKMeans\n\nTheory\nK-Means clustering, a widely recognized algorithm, is valued for its simplicity and effectiveness, making it particularly appealing in various applications. This technique involves grouping data points into ‘k’ clusters, where ‘k’ is a user-specified parameter. The algorithm starts by assigning random points to these clusters, with centroids acting as their centers. Distances, usually calculated using Euclidean distance, guide the process. Iteratively, points shift between clusters, and new centroids emerge through Lloyd’s algorithm until no better cluster assignments are feasible.  K-Means’ core lies in optimizing the sum of squared distances between data points and their assigned cluster mean. The choice of ‘k’ dictates the number of centroids, which represent cluster centers. The algorithm strategically redistributes points to minimize the in-cluster sum of squares. Its simplicity and widespread usage make it a fundamental tool in unsupervised machine learning, showcasing its ability to uncover inherent patterns in data.  In a broader context, K-Means clustering serves as a technique to group data points based on their similarity to an average grouping. Distance metrics, such as Euclidean or Manhattan distance, play a pivotal role, and normalizing input data becomes crucial for robust performance. Centroids, serving as the algorithm’s starting point, evolve through successive iterations, refining cluster assignments and centroids’ positions. The algorithm’s convergence reveals meaningful clusters, transforming data chaos into structured insights. Implementation through sklearn’s KMeans algorithm enhances efficiency, and feature selection enables practitioners to customize the clustering process to their data’s nuances.\n\n\nImplementation\n\n\nCode\n# import relevent libraries for clustering. we will use KMeans, AgglomerativeClustering, MeanShift, Birch, and DBSCAN\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.cluster import MeanShift, Birch\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport random\n\n\nThe below code performs K-Means clustering for different numbers of clusters (ranging from 2 to 10). For each cluster number, it calculates and stores the distortion, which measures the average Euclidean distance between each data point and its assigned cluster center. Inertia is also computed and stored, representing the sum of squared distances of data points to their closest cluster center. Additionally, the silhouette score is determined and recorded, offering insights into how well-defined and separated the clusters are. The results, encompassing these crucial metrics—distortion, inertia, and silhouette score—are then organized into a DataFrame for detailed analysis and printed. This is in an attempt to identify the optimal number of clusters based on a comprehensive evaluation of these key clustering indicators.\n\n\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create empty lists to store the results\nclus = []\ndistortions = []\ninertias = []\nsilhouette_scores = []\n\n# Loop through the range of clusters\nfor i in range(2, 11):  # Silhouette score is not defined for a single cluster\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(df)  \n    clus.append(i)\n    centers = kmeans.cluster_centers_\n    distortions.append(sum(np.min(pairwise_distances(df, centers, metric='euclidean'), axis=1)) / df.shape[0])\n    inertias.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(df, kmeans.labels_))\n\n# Create a DataFrame from the lists\nresults = pd.DataFrame({'Cluster': clus, 'Distortion': distortions, 'Inertia': inertias, 'Silhouette Score': silhouette_scores})\n\nprint(results)\n\n\n   Cluster  Distortion        Inertia  Silhouette Score\n0        2   10.622510  447410.233953          0.372238\n1        3    8.828917  272970.775541          0.408870\n2        4    7.974259  215311.617146          0.390174\n3        5    6.985968  163526.599625          0.380241\n4        6    6.394550  134984.307974          0.368484\n5        7    5.753101  113388.811614          0.384348\n6        8    5.309537   94219.154891          0.403127\n7        9    5.020084   83855.750340          0.403238\n8       10    4.698719   75156.752577          0.386414\n\n\nAmong the tested cluster counts (2 to 10), the silhouette score, a measure of cluster quality, is highest when there are three clusters. A silhouette score close to 1 indicates well-separated clusters. In this case, the silhouette score peaks at three clusters, suggesting that the data is most naturally organized into this number of distinct groups. This finding signifies a meaningful and clear grouping in the data, enabling better understanding and interpretation of underlying patterns.  The subsequent block of code generates three plots: one for distortion, one for inertia, and one for the silhouette score. Each plot has the number of clusters on the x-axis and the corresponding metric on the y-axis. These visualizations help interpret the results, allowing for the identification of trends and patterns.\n\n\nCode\n# Create subplots with 1 row and 3 columns\nfig, ax = plt.subplots(1, 3, figsize=(18, 4))\n\n# Plot Distortion\nax[0].plot(results['Cluster'], results['Distortion'], marker='o')\nax[0].set_title('Distortion')\nax[0].set_xlabel('Cluster')\nax[0].set_ylabel('Distortion')\n\n# Plot Inertia\nax[1].plot(results['Cluster'], results['Inertia'], marker='o')\nax[1].set_title('Inertia')\nax[1].set_xlabel('Cluster')\nax[1].set_ylabel('Inertia')\n\n# Plot Silhouette Score\nax[2].plot(results['Cluster'], results['Silhouette Score'], marker='o')\nax[2].set_title('Silhouette Score')\nax[2].set_xlabel('Cluster')\nax[2].set_ylabel('Silhouette Score')\n\n# Display the side-by-side plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe analysis of the graphs and results indicates that opting for 3 clusters is appropriate. The elbow and silhouette score methods suggest that this choice effectively captures meaningful patterns within the data. Having 3 clusters strikes a balance between simplicity and preserving relevant information. The subsequent 3D scatter plot visually represents the clustered data points in a three-dimensional space, offering a comprehensive view of the results.\n\n\nCode\npca_result = pd.read_csv('./data/modified_data/nova_pca.csv')\n\nimport plotly.express as px\n\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(df)\n\n# Get cluster labels\nlabels = kmeans.labels_\n\n# Assuming pca_result has columns '0', '1', and '2'\nfig = px.scatter_3d(pca_result, x='0', y='1', z='2', color=labels, symbol=labels, opacity=0.7,\n                    size_max=10, title='3D Scatter Plot of PC1, PC2, and PC3 with Cluster Labels',\n                    labels={'0': 'Principal Component 1 (PC1)',\n                            '1': 'Principal Component 2 (PC2)',\n                            '2': 'Principal Component 3 (PC3)',\n                            'color': 'Cluster'},\n                    )\nfig.show()\n\n\n\n                                                \n\n\n\n\n\nDBSCAN\n\nTheory\nDBSCAN, or Density-Based Spatial Clustering of Applications with Noise, is a robust clustering algorithm that distinguishes clusters based on the density of data points, incorporating both distance metrics and a minimum number of points. Unlike K-Means, DBSCAN doesn’t require users to specify the number of clusters (‘k’) in advance; instead, it dynamically identifies clusters by expanding neighborhoods around data points.  The algorithm starts by randomly selecting and expanding a neighborhood around a data point. If the density within this neighborhood is sufficient, a cluster is formed, and the process iterates until no more data points can be added. Outliers are identified as points in low-density regions. DBSCAN’s strength lies in its ability to uncover clusters of arbitrary shapes and sizes, making it particularly valuable for datasets where the number of clusters is unknown.  DBSCAN requires two key parameters: epsilon (ε) and minimum samples. Epsilon defines the maximum distance between two points for them to be considered in the same cluster, while minimum samples specify the minimum number of points required in each cluster. Experimentation is often needed to find optimal parameters, as the algorithm’s outcome is sensitive to these choices.  In summary, DBSCAN offers a unique approach to clustering, emphasizing data density over distances to centroids. Its flexibility in identifying clusters of varying shapes and the ability to mark outliers enhances its effectiveness in exploring complex datasets. Implementation through sklearn’s DBSCAN module provides a practical means to apply this algorithm to diverse datasets.\n\n\nImplementation\nThe below code performs an exhaustive search for optimal parameters (epsilon and minimum samples) for the DBSCAN algorithm. It calculates silhouette scores for different combinations of epsilon values (z1) and minimum sample sizes (z2), aiming to identify the best configuration that yields the highest silhouette score and the corresponding number of clusters. The resulting dataframe, df1, is then printed and visualized with a line plot.\n\n\nCode\nbest_scores = []\neps = []\nclus = []\nz1 = [i / 10 for i in range(5, 20)]\nz2 = range(2, 10) # explain why 2 to 10 or just do 1 to 10 but then u have to fix smth in the code if i dont remember wrong. i suggest explaining is a common assumption to do here\n\nfor i in z1:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in z2:\n        model = DBSCAN(eps=i, min_samples=j)\n        predics = model.fit_predict(df)\n        num_clusters = len(pd.Series(predics).unique())\n        if num_clusters &gt; 1:\n            score = silhouette_score(df, predics)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    clus.append(best_cluster)\n    eps.append(best_eps)\n\ndf1 = pd.DataFrame({'Epsilons': eps, 'Best_Clusters': clus, 'Best_Silhouette': best_scores})\nprint(df1.sort_values(by=\"Best_Silhouette\", ascending=False))\nsns.lineplot(data=df1, x='Best_Clusters',y='Best_Silhouette')\nplt.show()\n\n\n    Epsilons  Best_Clusters  Best_Silhouette\n0        0.5            595         0.065783\n1        0.6            595         0.065783\n2        0.7            595         0.065783\n3        0.8            595         0.065783\n4        0.9            595         0.065783\n5        1.0            456         0.020257\n10       1.5             33        -0.029439\n11       1.6             33        -0.029439\n12       1.7             33        -0.029439\n6        1.1            216        -0.040136\n7        1.2            216        -0.040136\n8        1.3            216        -0.040136\n9        1.4            216        -0.040136\n13       1.8             22        -0.117656\n14       1.9             22        -0.117656\n\n\n\n\n\nThe results suggest that varying epsilon values from 0.5 to 0.9 consistently yield a high number of clusters (595) with a low silhouette score (0.065783). This pattern persists, indicating that DBSCAN struggles to identify distinct clusters, possibly due to the nature of the data. The diminishing silhouette scores as epsilon increases, coupled with the large number of clusters, may imply that this clustering method is not well-suited for the dataset. To validate this observation, hierarchical clustering and visualization through T-SNE plots will be explored for further confirmation.\n\n\n\nHierarchical Clustering (Agglomerative Clustering)\n\nTheory\nHierarchical Clustering, also known as Agglomerative Clustering, is a versatile algorithm that builds a hierarchy of clusters without the need to predefine the number of clusters. It treats each data point individually and progressively merges the closest clusters until forming a single cluster, using methods like Ward’s method to calculate distances. The resulting linkage matrix constructs a dendrogram, enabling visualization of clustering hierarchy and facilitating the choice of cluster count. Normalizing input data is crucial for meaningful results, and the choice of linkage method, such as ward linkage, influences cluster formation.  Hierarchical Clustering focuses on finding clusters based on distance, repeatedly finding the two closest points and forming clusters until all points are assigned. The algorithm is sensitive to distance, requiring multiple runs with different distance values, and scaling the dataset to mitigate outlier effects. This method’s adaptability and visualization through a dendrogram make it valuable for exploring data structures.\n\n\nImplementation\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward') #chose 3 as that is the number of species. We could have changed it.\nlabels = hierarchical_cluster.fit_predict(df)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\nCluster Labels total:\n[0, 1, 2]\n\n\nLet’s generates a dendrogram for Agglomerative Clustering, visualizing the hierarchical linkage between data points.\n\n\nCode\n# create linkage for agglomerative clustering, and the dendrogram for the linkage. Suggest the optimal number of clusters based on the dendrogram.\nlinkage_matrix = linkage(df, method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', labels=labels, distance_sort='ascending', show_leaf_counts=True)\nplt.show()\n\n\n\n\n\nThe below code defines a function that performs hierarchical clustering on input data, varying the number of clusters. It calculates silhouette scores for each clustering iteration and outputs the optimal number of clusters that maximizes the silhouette score, in addition to plotting a graph showing how the silhouette score changes with different cluster numbers. The last lines of code apply this function to the data.\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.cluster\n\ndef maximize_silhouette(X, algo=\"ag\", nmax=None, i_plot=False):\n    # PARAM\n    i_print = False\n\n    # FORCE CONTIGUOUS\n    X = np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params = []\n    sil_scores = []\n    sil_max = -10\n\n    for param in range(2, nmax + 1):\n        if algo == \"ag\":\n            model = AgglomerativeClustering(n_clusters=param).fit(X)\n            labels = model.labels_\n            \n            try:\n                sil_scores.append(silhouette_score(X, labels))\n                params.append(param)\n            except ValueError:\n                continue\n\n            if i_print:\n                print(param, sil_scores[-1])\n\n            if sil_scores[-1] &gt; sil_max:\n                opt_param = param\n                sil_max = sil_scores[-1]\n                opt_labels = labels\n\n    print(\"Maximum Silhouette score =\", sil_max)\n    print(\"OPTIMAL CLUSTERS (btwn 2-10) =\", opt_param)\n\n    if i_plot:\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n# Example usage:\nopt_labels = maximize_silhouette(df, algo=\"ag\", nmax=10, i_plot=True)\n\n\nMaximum Silhouette score = 0.3971383731982046\nOPTIMAL CLUSTERS (btwn 2-10) = 3\n\n\n\n\n\nThe results from the code suggest that, within the considered range of 2 to 10 clusters, the maximum silhouette score for agglomerative clustering is achieved at 3 clusters, indicating it as the optimal number of clusters. This aligns with the observed trend in K-Means clustering, where both methods highlight 3 clusters as optimal based on the silhouette score.\n\n\n\nConclusions\nThe analysis using K-Means and hierarchical clustering both implied the presence of three clusters within the dataset, providing a consistent pattern. However, DBSCAN did not seem well-suited for this dataset, suggesting varying performance across clustering methods. This indicates the potential for meaningful clustering, and further exploration in the Dimensionality Reduction tab may unveil clearer insights into the distinctive patterns within the data.\n\n\nExtra Joke\nMovie Pitch: It’s a movie about high school girls trying to figure out what clique they belong in. They move from clique to clique and eventually stop when they minimize their differences. It’s called K-Means girls."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "three more to go!"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Introduction\nThis study delves into Villanova’s 2021-22 season NCAA shot data, spotlighting six key features. Using Python and sklearn, we employ Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. This approach trims features while preserving variance, simplifying data for improved model comprehension and visualization.\n\n\nDimensionality Reduction with PCA\nPrincipal Component Analysis (PCA) is a valuable machine learning technique used to simplify large datasets by reducing their dimensionality. The primary goal is to decrease the number of variables while retaining crucial information. Explore the PCA process as I walk you through my code and showcase the corresponding output below. \n\nLoad in relevant libraries and data\n\n\nCode\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nnova = pd.read_csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# only keeping the shot data\nnova = nova.dropna(subset=['shooter'])\n\n# Creating a new column to specify the team of the shooter\nnova['shooter_team'] = np.where(nova['action_team'] == \"home\", nova['home'], nova['away'])\n\n# only keeping the villanova shots\nnova = nova[nova['shooter_team'] == 'Villanova']\n\n# changing shot outcome to numeric\nnova['shot_outcome_numeric'] = nova['shot_outcome'].apply(lambda x: 1 if x == 'made' else 0)\n\n\n\n\nCode\n#creating a new column called shot value\nnova['shot_value'] = 2  # Default value for shots that are not free throws or three-pointers\nnova.loc[nova['free_throw'], 'shot_value'] = 1\nnova.loc[nova['three_pt'], 'shot_value'] = 3\n\n# Calculate the mean of shot_outcome for each player (field goal percentage)\nmean_and_count_data = nova.groupby('shooter').agg(\n    shots=('shot_outcome', 'count'),\n    field_goal_percentage=('shot_outcome_numeric', lambda x: x[x == 1].count() / len(x) if len(x) &gt; 0 else 0)\n).sort_values(by='shots', ascending=False)\n\n# Add the calculated field goal percentage to the original DataFrame\nnova = nova.merge(mean_and_count_data[['field_goal_percentage']], left_on='shooter', right_index=True, how='left').round(4)\n\n# create a lag variable for the previous shot (1 indicates made shot, -1 indicates miss, 0 indicates no previous shot in half\nnova = nova.sort_values(by=['shooter', 'game_id', 'play_id'])  # Arrange the data by shooter, game_id, and play_id\nnova['lag1'] = nova.groupby(['shooter', 'game_id'])['shot_outcome_numeric'].shift(1)\nnova['lag1'] = nova['lag1'].replace({0: -1}).fillna(0)  # Replace initial 0 values with -1, and NaN values with 0\nnova = nova.sort_values(by=['game_id', 'play_id'])\n\n# reset the index\nnova = nova.reset_index(drop=True)\n\n# create a new column for the home crowd\nnova['home_crowd'] = (nova['home'] == 'Villanova').astype(int)\n\n# create a new column for the game number in the season\nnova['game_num'] = nova['game_id'].astype('category').cat.codes + 1\n\nnova.head()\n\n\n\n\n\n\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\npossession_before\npossession_after\nwrong_time\nshooter_team\nshot_outcome_numeric\nshot_value\nfield_goal_percentage\nlag1\nhome_crowd\ngame_num\n\n\n\n\n0\n401365747\n2021-11-28\nLa Salle\nVillanova\n4\n1\n19:22\n2362\n2362\nJustin Moore missed Three Point Jumper.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n3\n0.4721\n0.0\n0\n1\n\n\n1\n401365747\n2021-11-28\nLa Salle\nVillanova\n13\n1\n18:32\n2312\n2312\nEric Dixon missed Dunk.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n2\n0.5794\n0.0\n0\n1\n\n\n2\n401365747\n2021-11-28\nLa Salle\nVillanova\n16\n1\n18:18\n2298\n2298\nCollin Gillespie made Three Point Jumper.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n3\n0.5337\n0.0\n0\n1\n\n\n3\n401365747\n2021-11-28\nLa Salle\nVillanova\n18\n1\n17:35\n2255\n2255\nEric Dixon made Layup.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5794\n-1.0\n0\n1\n\n\n4\n401365747\n2021-11-28\nLa Salle\nVillanova\n21\n1\n16:59\n2219\n2219\nJermaine Samuels made Layup. Assisted by Eric ...\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5535\n0.0\n0\n1\n\n\n\n\n5 rows × 46 columns\n\n\n\n\n\nCode\n# subsetting my data into feature varaibles and target variable\nfeature_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num']\nnova_features = nova[feature_columns].copy()\n\ntarget_column = ['shot_outcome_numeric']\nnova_target = nova[target_column].copy()\n\nall_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num', 'shot_outcome_numeric']\nnova_final = nova[all_columns].copy()\n\n# save feature_columns to csv for clustering\nnova_features.to_csv('./data/modified_data/nova_features.csv', index=False)\n\n# save nova_final to csv for decision trees\nnova_final.to_csv('./data/modified_data/nova_final.csv', index=False)\n\n\nnova_features.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2764 entries, 0 to 2763\nData columns (total 6 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   shot_value             2764 non-null   int64  \n 1   field_goal_percentage  2764 non-null   float64\n 2   lag1                   2764 non-null   float64\n 3   home_crowd             2764 non-null   int64  \n 4   score_diff             2764 non-null   int64  \n 5   game_num               2764 non-null   int8   \ndtypes: float64(2), int64(3), int8(1)\nmemory usage: 110.8 KB\n\n\n\n\nStandardization\nNormalize data to have a mean of 0 and a standard deviation of 1.\n\n\nCode\n# Standardization\nscaler = StandardScaler()\nnova_features_standardized = scaler.fit_transform(nova_features)\n\n\n\n\nCovariance Matrix Computation\nCalculate the covariance matrix to understand variable relationships.\n\n\nCode\n#covariance matrix\nco_ma = np.cov(nova_features_standardized, rowvar=False)\nprint(co_ma)\n\n\n[[ 1.00036193 -0.1413293  -0.16465368  0.00400472  0.0018319   0.00327538]\n [-0.1413293   1.00036193  0.08216798 -0.05311615 -0.04587765 -0.01289989]\n [-0.16465368  0.08216798  1.00036193  0.04351646  0.04402943 -0.00288759]\n [ 0.00400472 -0.05311615  0.04351646  1.00036193  0.49188691  0.31830859]\n [ 0.0018319  -0.04587765  0.04402943  0.49188691  1.00036193  0.12171352]\n [ 0.00327538 -0.01289989 -0.00288759  0.31830859  0.12171352  1.00036193]]\n\n\n\n\nComputing Eigenvectors and Eigenvalues\nIdentify principal components using eigenvectors and eigenvalues.\n\n\nCode\n#eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(co_ma)\nprint(\"Eigenvalues\\n\",\"----------------------\")\nprint(eigenvalues)\nprint(\"\\nEigenvectors\\n\",\"----------------------\")\nprint(eigenvectors)\n\n\nEigenvalues\n ----------------------\n[1.65668919 1.26200685 0.46449209 0.81824262 0.8669812  0.9337596 ]\n\nEigenvectors\n ----------------------\n[[-0.01233438  0.63776031  0.00198086 -0.75043735 -0.17142614  0.02371907]\n [ 0.09828166 -0.51666678 -0.01569294 -0.30589464 -0.50573557  0.61139994]\n [-0.06667471 -0.57041517  0.01469836 -0.56821948  0.30014023 -0.50695901]\n [-0.66744252 -0.01515272 -0.73678     0.0201096  -0.1050718  -0.00127727]\n [-0.5923083  -0.02488482  0.60566041  0.08768672 -0.46111118 -0.24781971]\n [-0.43524065  0.00974191  0.29977402 -0.11093025  0.63332208  0.55425972]]\n\n\n\n\nFeature Vectors\nSelect eigenvectors as new feature vectors.\n\n\nCode\n# choosing principal components\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigenvalues)[::-1]\nsorted_eigenvalue = eigenvalues[sorted_index]\n\n\n\n\nRecasting Data Among Principal Component Axis\nTransform data using chosen principal components.\n\n\nCode\n# PCA with components decided above\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\ndesired_variance = 0.75 \nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\n\npca = PCA(n_components=num_components)\nnova_pca = pca.fit_transform(nova_features_standardized)\n\n\n\n\nDeciding optimal number of components\nTo decide the optimal number of components, we can use both a cumulative explained variance plot and a scree plot to visualize explained variance ratio.\n\n\nCode\n# Cumulative Explained Variance Plot\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', color='#FFB6C1')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Scree Plot\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(\"Explained Variance Ratio for Each Component:\")\nprint(explained_variance_ratio)\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='#FFB6C1')\nplt.title('Scree Plot')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n\n# find the number of variables it takes to reach a variance of 0.75\ndesired_variance = 0.75\nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\nprint(f\"Number of components to capture {desired_variance * 100}% variance: {num_components}\")\n\n\nExplained Variance Ratio for Each Component:\n[0.27601497 0.21025838 0.1555703  0.14444459]\nNumber of components to capture 75.0% variance: 4\n\n\n\n\n\nAs a general guideline, the goal is to retain at least 80% of the variance. However, given the relatively small size of our dataset, we have adjusted the threshold to 75%. Therefore, we will select 4 components, ensuring the cumulative explained variance surpasses 75%.\n\n\nVisualizing reduced-dimensional data\nNow, let’s visualize the reduced-dimensional data using a scatter plot of the first two principal components.\n\n\nCode\n# pca scatter plot\nplt.scatter(nova_pca[:, 0], nova_pca[:, 1], alpha=0.5, color='#D8BFD8')\nplt.title('PCA Scatter Plot')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n# limit PCA to 4 components\npca = PCA(n_components=4)\n\n# save nova_pca to csv\nnova_pca_df = pd.DataFrame(nova_pca)\nnova_pca_df.to_csv('./data/modified_data/nova_pca.csv', index=False)\n\n\n\n\n\nThe scree plot guides us in determining that capturing 75% of the variance necessitates employing four principal components. The scatter plot, showcasing the reduced-dimensional data, visually represents patterns within the dataset. You many notice that a seperation occurs where Principal Component 1 equals 0. While PCA excels at identifying linear relationships, it’s important to acknowledge that observations with higher variability may be distant from the main cluster. These steps underscore how PCA simplifies dimensionality reduction, fostering a deeper understanding of the dataset. It’s worth noting that the sklearn library’s PCA function automates these procedures for ease of implementation.\n\n\n\nDimensionality Reduction with t-SNE\nt-SNE, or t-distributed Stochastic Neighbor Embedding, is an unsupervised non-linear dimensionality reduction technique designed to explore and visualize high-dimensional data. It transforms complex datasets into a lower-dimensional space, emphasizing preserving local relationships among data points. By finding similarity measures between pairs of instances in higher and lower dimensional spaces and optimizing these measures, t-SNE enhances our ability to interpret intricate datasets.\nAdditionally, exploring clustering in this context allows me to identify distinct groups or patterns within the NCAA shot data. By combining t-SNE, a dimensionality reduction technique, with KMeans clustering, I can uncover and visualize natural structures or associations in the dataset. The choice of three clusters is informed by the results obtained on the clustering page. Exploring different perplexity values enhances the flexibility of my analysis, helping me discover nuanced patterns at varying levels of detail.\n\n\nCode\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport pandas as pd\n\ndef explore_tsne(perplexity_value):\n    X = nova_features.iloc[:, :]\n\n    # t-SNE for 3 dimensions with different perplexity\n    tsne = TSNE(n_components=3, random_state=1, perplexity=perplexity_value)\n    X_tsne = tsne.fit_transform(X)\n\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    clusters = kmeans.fit_predict(X)\n\n    # Create a DataFrame with 3D data\n    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2', 'Dimension 3'])\n    tsne_df['Cluster'] = clusters\n\n    # Interactive 3D scatter plot with plotly\n    fig = px.scatter_3d(tsne_df, x='Dimension 1', y='Dimension 2', z='Dimension 3',\n                        color='Cluster', symbol='Cluster', opacity=0.7, size_max=10,\n                        title=f't-SNE 3D Visualization (Perplexity={perplexity_value})',\n                        labels={'Cluster': 'Cluster'})\n\n    # Show the plot\n    fig.show()\n\n# Explore t-SNE with different perplexity values\nperplexities = [5, 20, 40]  # Add more values as needed\nfor perplexity_value in perplexities:\n    explore_tsne(perplexity_value)\n\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nPerplexity in t-SNE determines the balance between capturing local and global relationships in the data’s low-dimensional representation. Lower perplexity values focus on local details, higher values emphasize global structures, while moderate values strike a balance. Experimenting with different perplexity values helps find an optimal configuration for visualizing and understanding the dataset. As I explored various perplexity values, I noticed that with larger perplexity values, clusters became more distinct, revealing clearer patterns and structures within the data. This observation underscores the importance of choosing an appropriate perplexity value for the specific characteristics of the dataset, ultimately enhancing the effectiveness of t-SNE in revealing underlying structures.\n\n\nEvaluation & Comparison\nIn summary, PCA efficiently preserves the overall structure, making it well-suited for large datasets with linear relationships. Conversely, t-SNE excels at unveiling local structures and clusters, offering enhanced visualization for smaller datasets. The decision between these techniques hinges on factors like dataset size, structure, and specific analysis goals.\nIn my analysis, it became evident that certain variables play a crucial role in explaining most of the variance in our dataset. Despite having only six feature variables, retaining four allows us to preserve over 75% of the variance, indicating limited redundancy. The application of t-SNE for cluster visualization proved insightful, revealing subtle overlaps within the clusters. This aligns with previous observations, reinforcing that identifying distinct clusters in this dataset poses challenges.\n\n\nExtra Joke\nIf we were compressed down to a single dimension… what would be the point of it all?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN_website",
    "section": "",
    "text": "Welcome to my website, where I’ll be sharing various projects from my time in the DSAN program at Georgetown University. Expect regular updates as I delve into new and exciting topics - stay tuned for more! While you’re here, try the snake game below (code altered from here)!"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "This page will take you through the data sources and methodologies employed in this specific project. Furthermore, you can find brief descriptions/images/tables of the various datasets mentioned. Data must be acquired using at least one Python API and one R API. This project will use various data formats that may include labeled data, qualitative data, text data, geo data, record-data, etc.\n\nncaahoopR\n“ncaahoopR” is an R package tailored for NCAA Basketball Play-by-Play Data analysis. It excels at retrieving play-by-play data in a tidy format. For the purposes of this project, I will start by scraping play-by-play data for the Villanova Wildcats Men’s Basketball team from both the 2019-20 and 2021-22 seasons (the 2020-21 was shortened due to COVID-19).\n\n\nCode\nlibrary(tidyverse)\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lbenz730/ncaahoopR\")\nlibrary(ncaahoopR)\n\n\n\n\nCode\nVillanova1920 &lt;- get_pbp(\"Villanova\", \"2019-20\")\nVillanova2122 &lt;- get_pbp(\"Villanova\", \"2021-22\")\nwrite.csv(Villanova1920, file = \"./data/raw_data/villanova1920.csv\", row.names = FALSE)\nwrite.csv(Villanova2122, file = \"./data/raw_data/villanova2122.csv\", row.names = FALSE)\n\n\n\n\nCode\nhead(Villanova1920)\n\n\n\nA data.frame: 6 x 39\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\nshot_y\nshot_team\nshot_outcome\nshooter\nassist\nthree_pt\nfree_throw\npossession_before\npossession_after\nwrong_time\n\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\n401169778\n2019-11-05\nVillanova\nArmy\n1\n1\n19:37\n2377\n2377\nSaddiq Bey made Jumper.\n...\nNA\nVillanova\nmade\nSaddiq Bey\nNA\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n2\n401169778\n2019-11-05\nVillanova\nArmy\n2\n1\n19:16\n2356\n2356\nTucker Blackwell made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nTucker Blackwell\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n3\n401169778\n2019-11-05\nVillanova\nArmy\n3\n1\n19:01\n2341\n2341\nFoul on Jermaine Samuels.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n4\n401169778\n2019-11-05\nVillanova\nArmy\n4\n1\n19:01\n2341\n2341\nJermaine Samuels Turnover.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n5\n401169778\n2019-11-05\nVillanova\nArmy\n5\n1\n18:42\n2322\n2322\nMatt Wilson made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nMatt Wilson\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n6\n401169778\n2019-11-05\nVillanova\nArmy\n6\n1\n18:31\n2311\n2311\nJeremiah Robinson-Earl made Jumper. Assisted by Justin Moore.\n...\nNA\nVillanova\nmade\nJeremiah Robinson-Earl\nJustin Moore\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n\n\n\n\n\nBaseballr\n“Baseballr” is a package in R that focuses on baseball analytics, also known as sabremetrics. It includes various functions that can be used for scraping data from websites like FanGraphs.com, Baseball-Reference.com, and BaseballSavant.mlb.com. It also includes functions for calculating specific baseball metrics such as wOBA (weighted on-base average) and FIP (fielding independent pitching). I will mainly use this package to gather data (which uses an API as can be seen below).\n\nSource Code\nThe below source code was pulled from the baseballr github repository. This specific code uses a mlb api to acquire play-by-play data for a specific game. I will use these functions later on through the baseballr package.\n\n\nCode\nmlb_api_call &lt;- function(url){\n  res &lt;-\n    httr::RETRY(\"GET\", url)\n  \n  json &lt;- res$content %&gt;%\n    rawToChar() %&gt;%\n    jsonlite::fromJSON(simplifyVector = T)\n  \n  return(json)\n}\n\nmlb_stats_endpoint &lt;- function(endpoint){\n  all_endpoints = c(\n    \"v1/attendance\",#\n    \"v1/conferences\",#\n    \"v1/conferences/{conferenceId}\",#\n    \"v1/awards/{awardId}/recipients\",#\n    \"v1/awards\",#\n    \"v1/baseballStats\",#\n    \"v1/eventTypes\",#\n    \"v1/fielderDetailTypes\",#\n    \"v1/gameStatus\",#\n    \"v1/gameTypes\",#\n    \"v1/highLow/types\",#\n    \"v1/hitTrajectories\",#\n    \"v1/jobTypes\",#\n    \"v1/languages\",\n    \"v1/leagueLeaderTypes\",#\n    \"v1/logicalEvents\",#\n    \"v1/metrics\",#\n    \"v1/pitchCodes\",#\n    \"v1/pitchTypes\",#\n    \"v1/playerStatusCodes\",#\n    \"v1/positions\",#\n    \"v1/reviewReasons\",#\n    \"v1/rosterTypes\",#\n    \"v1/runnerDetailTypes\",#\n    \"v1/scheduleEventTypes\",#\n    \"v1/situationCodes\",#\n    \"v1/sky\",#\n    \"v1/standingsTypes\",#\n    \"v1/statGroups\",#\n    \"v1/statTypes\",#\n    \"v1/windDirection\",#\n    \"v1/divisions\",#\n    \"v1/draft/{year}\",#\n    \"v1/draft/prospects/{year}\",#\n    \"v1/draft/{year}/latest\",#\n    \"v1.1/game/{gamePk}/feed/live\",\n    \"v1.1/game/{gamePk}/feed/live/diffPatch\",#\n    \"v1.1/game/{gamePk}/feed/live/timestamps\",#\n    \"v1/game/changes\",##x\n    \"v1/game/analytics/game\",##x\n    \"v1/game/analytics/guids\",##x\n    \"v1/game/{gamePk}/guids\",##x\n    \"v1/game/{gamePk}/{GUID}/analytics\",##x\n    \"v1/game/{gamePk}/{GUID}/contextMetricsAverages\",##x\n    \"v1/game/{gamePk}/contextMetrics\",#\n    \"v1/game/{gamePk}/winProbability\",#\n    \"v1/game/{gamePk}/boxscore\",#\n    \"v1/game/{gamePk}/content\",#\n    \"v1/game/{gamePk}/feed/color\",##x\n    \"v1/game/{gamePk}/feed/color/diffPatch\",##x\n    \"v1/game/{gamePk}/feed/color/timestamps\",##x\n    \"v1/game/{gamePk}/linescore\",#\n    \"v1/game/{gamePk}/playByPlay\",#\n    \"v1/gamePace\",#\n    \"v1/highLow/{orgType}\",#\n    \"v1/homeRunDerby/{gamePk}\",#\n    \"v1/homeRunDerby/{gamePk}/bracket\",#\n    \"v1/homeRunDerby/{gamePk}/pool\",#\n    \"v1/league\",#\n    \"v1/league/{leagueId}/allStarBallot\",#\n    \"v1/league/{leagueId}/allStarWriteIns\",#\n    \"v1/league/{leagueId}/allStarFinalVote\",#\n    \"v1/people\",#\n    \"v1/people/freeAgents\",#\n    \"v1/people/{personId}\",##U\n    \"v1/people/{personId}/stats/game/{gamePk}\",#\n    \"v1/people/{personId}/stats/game/current\",#\n    \"v1/jobs\",#\n    \"v1/jobs/umpires\",#\n    \"v1/jobs/datacasters\",#\n    \"v1/jobs/officialScorers\",#\n    \"v1/jobs/umpires/games/{umpireId}\",##x\n    \"v1/schedule/\",#\n    \"v1/schedule/games/tied\",#\n    \"v1/schedule/postseason\",#\n    \"v1/schedule/postseason/series\",#\n    \"v1/schedule/postseason/tuneIn\",##x\n    \"v1/seasons\",#\n    \"v1/seasons/all\",#\n    \"v1/seasons/{seasonId}\",#\n    \"v1/sports\",#\n    \"v1/sports/{sportId}\",#\n    \"v1/sports/{sportId}/players\",#\n    \"v1/standings\",#\n    \"v1/stats\",#\n    \"v1/stats/metrics\",##x\n    \"v1/stats/leaders\",#\n    \"v1/stats/streaks\",##404\n    \"v1/teams\",#\n    \"v1/teams/history\",#\n    \"v1/teams/stats\",#\n    \"v1/teams/stats/leaders\",#\n    \"v1/teams/affiliates\",#\n    \"v1/teams/{teamId}\",#\n    \"v1/teams/{teamId}/stats\",#\n    \"v1/teams/{teamId}/affiliates\",#\n    \"v1/teams/{teamId}/alumni\",#\n    \"v1/teams/{teamId}/coaches\",#\n    \"v1/teams/{teamId}/personnel\",#\n    \"v1/teams/{teamId}/leaders\",#\n    \"v1/teams/{teamId}/roster\",##x\n    \"v1/teams/{teamId}/roster/{rosterType}\",#\n    \"v1/venues\"#\n  )\n  base_url = glue::glue('http://statsapi.mlb.com/api/{endpoint}')\n  return(base_url)\n}\n\n\n\n\n\nCode\nx &lt;- \"http://statsapi.mlb.com/api/v1/game/575156/playByPlay\"\n\noutput &lt;- mlb_api_call(x)\n\n\n“output” is a very messy list that is extremely long. Instead of printing “output”, below are three images of part of the list.\n  \nThe below code builds on the previous code, returning a tibble that includes over 100 columns of data provided by the MLB Stats API at a pitch level. As you will see, the output is much cleaner and easier to work with.\n\n\nCode\n#' @rdname mlb_pbp\n#' @title **Acquire pitch-by-pitch data for Major and Minor League games**\n#'\n#' @param game_pk The date for which you want to find game_pk values for MLB games\n#' @importFrom jsonlite fromJSON\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#'\n#' Some data will vary depending on the\n#' park and the league level, as most sensor data is not available in\n#' minor league parks via this API. Note that the column names have mostly\n#' been left as-is and there are likely duplicate columns in terms of the\n#' information they provide. I plan to clean the output up down the road, but\n#' for now I am leaving the majority as-is.\n#'\n#' Both major and minor league pitch-by-pitch data can be pulled with this function.\n#' \n#'  |col_name                       |types     |\n#'  |:------------------------------|:---------|\n#'  |game_pk                        |numeric   |\n#'  |game_date                      |character |\n#'  |index                          |integer   |\n#'  |startTime                      |character |\n#'  |endTime                        |character |\n#'  |isPitch                        |logical   |\n#'  |type                           |character |\n#'  |playId                         |character |\n#'  |pitchNumber                    |integer   |\n#'  |details.description            |character |\n#'  |details.event                  |character |\n#'  |details.awayScore              |integer   |\n#'  |details.homeScore              |integer   |\n#'  |details.isScoringPlay          |logical   |\n#'  |details.hasReview              |logical   |\n#'  |details.code                   |character |\n#'  |details.ballColor              |character |\n#'  |details.isInPlay               |logical   |\n#'  |details.isStrike               |logical   |\n#'  |details.isBall                 |logical   |\n#'  |details.call.code              |character |\n#'  |details.call.description       |character |\n#'  |count.balls.start              |integer   |\n#'  |count.strikes.start            |integer   |\n#'  |count.outs.start               |integer   |\n#'  |player.id                      |integer   |\n#'  |player.link                    |character |\n#'  |pitchData.strikeZoneTop        |numeric   |\n#'  |pitchData.strikeZoneBottom     |numeric   |\n#'  |details.fromCatcher            |logical   |\n#'  |pitchData.coordinates.x        |numeric   |\n#'  |pitchData.coordinates.y        |numeric   |\n#'  |hitData.trajectory             |character |\n#'  |hitData.hardness               |character |\n#'  |hitData.location               |character |\n#'  |hitData.coordinates.coordX     |numeric   |\n#'  |hitData.coordinates.coordY     |numeric   |\n#'  |actionPlayId                   |character |\n#'  |details.eventType              |character |\n#'  |details.runnerGoing            |logical   |\n#'  |position.code                  |character |\n#'  |position.name                  |character |\n#'  |position.type                  |character |\n#'  |position.abbreviation          |character |\n#'  |battingOrder                   |character |\n#'  |atBatIndex                     |character |\n#'  |result.type                    |character |\n#'  |result.event                   |character |\n#'  |result.eventType               |character |\n#'  |result.description             |character |\n#'  |result.rbi                     |integer   |\n#'  |result.awayScore               |integer   |\n#'  |result.homeScore               |integer   |\n#'  |about.atBatIndex               |integer   |\n#'  |about.halfInning               |character |\n#'  |about.inning                   |integer   |\n#'  |about.startTime                |character |\n#'  |about.endTime                  |character |\n#'  |about.isComplete               |logical   |\n#'  |about.isScoringPlay            |logical   |\n#'  |about.hasReview                |logical   |\n#'  |about.hasOut                   |logical   |\n#'  |about.captivatingIndex         |integer   |\n#'  |count.balls.end                |integer   |\n#'  |count.strikes.end              |integer   |\n#'  |count.outs.end                 |integer   |\n#'  |matchup.batter.id              |integer   |\n#'  |matchup.batter.fullName        |character |\n#'  |matchup.batter.link            |character |\n#'  |matchup.batSide.code           |character |\n#'  |matchup.batSide.description    |character |\n#'  |matchup.pitcher.id             |integer   |\n#'  |matchup.pitcher.fullName       |character |\n#'  |matchup.pitcher.link           |character |\n#'  |matchup.pitchHand.code         |character |\n#'  |matchup.pitchHand.description  |character |\n#'  |matchup.splits.batter          |character |\n#'  |matchup.splits.pitcher         |character |\n#'  |matchup.splits.menOnBase       |character |\n#'  |batted.ball.result             |factor    |\n#'  |home_team                      |character |\n#'  |home_level_id                  |integer   |\n#'  |home_level_name                |character |\n#'  |home_parentOrg_id              |integer   |\n#'  |home_parentOrg_name            |character |\n#'  |home_league_id                 |integer   |\n#'  |home_league_name               |character |\n#'  |away_team                      |character |\n#'  |away_level_id                  |integer   |\n#'  |away_level_name                |character |\n#'  |away_parentOrg_id              |integer   |\n#'  |away_parentOrg_name            |character |\n#'  |away_league_id                 |integer   |\n#'  |away_league_name               |character |\n#'  |batting_team                   |character |\n#'  |fielding_team                  |character |\n#'  |last.pitch.of.ab               |character |\n#'  |pfxId                          |character |\n#'  |details.trailColor             |character |\n#'  |details.type.code              |character |\n#'  |details.type.description       |character |\n#'  |pitchData.startSpeed           |numeric   |\n#'  |pitchData.endSpeed             |numeric   |\n#'  |pitchData.zone                 |integer   |\n#'  |pitchData.typeConfidence       |numeric   |\n#'  |pitchData.plateTime            |numeric   |\n#'  |pitchData.extension            |numeric   |\n#'  |pitchData.coordinates.aY       |numeric   |\n#'  |pitchData.coordinates.aZ       |numeric   |\n#'  |pitchData.coordinates.pfxX     |numeric   |\n#'  |pitchData.coordinates.pfxZ     |numeric   |\n#'  |pitchData.coordinates.pX       |numeric   |\n#'  |pitchData.coordinates.pZ       |numeric   |\n#'  |pitchData.coordinates.vX0      |numeric   |\n#'  |pitchData.coordinates.vY0      |numeric   |\n#'  |pitchData.coordinates.vZ0      |numeric   |\n#'  |pitchData.coordinates.x0       |numeric   |\n#'  |pitchData.coordinates.y0       |numeric   |\n#'  |pitchData.coordinates.z0       |numeric   |\n#'  |pitchData.coordinates.aX       |numeric   |\n#'  |pitchData.breaks.breakAngle    |numeric   |\n#'  |pitchData.breaks.breakLength   |numeric   |\n#'  |pitchData.breaks.breakY        |numeric   |\n#'  |pitchData.breaks.spinRate      |integer   |\n#'  |pitchData.breaks.spinDirection |integer   |\n#'  |hitData.launchSpeed            |numeric   |\n#'  |hitData.launchAngle            |numeric   |\n#'  |hitData.totalDistance          |numeric   |\n#'  |injuryType                     |character |\n#'  |umpire.id                      |integer   |\n#'  |umpire.link                    |character |\n#'  |isBaseRunningPlay              |logical   |\n#'  |isSubstitution                 |logical   |\n#'  |about.isTopInning              |logical   |\n#'  |matchup.postOnFirst.id         |integer   |\n#'  |matchup.postOnFirst.fullName   |character |\n#'  |matchup.postOnFirst.link       |character |\n#'  |matchup.postOnSecond.id        |integer   |\n#'  |matchup.postOnSecond.fullName  |character |\n#'  |matchup.postOnSecond.link      |character |\n#'  |matchup.postOnThird.id         |integer   |\n#'  |matchup.postOnThird.fullName   |character |\n#'  |matchup.postOnThird.link       |character |\n#' @export\n#' @examples \\donttest{\n#'   try(mlb_pbp(game_pk = 632970))\n#' }\n\nmlb_pbp &lt;- function(game_pk) {\n  \n  mlb_endpoint &lt;- mlb_stats_endpoint(glue::glue(\"v1.1/game/{game_pk}/feed/live\"))\n  \n  tryCatch(\n    expr = {\n      payload &lt;- mlb_endpoint %&gt;% \n        mlb_api_call() %&gt;% \n        jsonlite::toJSON() %&gt;% \n        jsonlite::fromJSON(flatten = TRUE)\n      \n      plays &lt;- payload$liveData$plays$allPlays$playEvents %&gt;% \n        dplyr::bind_rows()\n      \n      at_bats &lt;- payload$liveData$plays$allPlays\n      \n      current &lt;- payload$liveData$plays$currentPlay\n      \n      game_status &lt;- payload$gameData$status$abstractGameState\n      \n      home_team &lt;- payload$gameData$teams$home$name\n      \n      home_level &lt;- payload$gameData$teams$home$sport\n      \n      home_league &lt;- payload$gameData$teams$home$league\n      \n      away_team &lt;- payload$gameData$teams$away$name\n      \n      away_level &lt;- payload$gameData$teams$away$sport\n      \n      away_league &lt;- payload$gameData$teams$away$league\n      \n      columns &lt;- lapply(at_bats, function(x) class(x)) %&gt;%\n        dplyr::bind_rows(.id = \"variable\")\n      cols &lt;- c(colnames(columns))\n      classes &lt;- c(t(unname(columns[1,])))\n      \n      df &lt;- data.frame(cols, classes)\n      list_columns &lt;- df %&gt;%\n        dplyr::filter(.data$classes == \"list\") %&gt;%\n        dplyr::pull(\"cols\")\n      \n      at_bats &lt;- at_bats %&gt;%\n        dplyr::select(-c(tidyr::one_of(list_columns)))\n      \n      pbp &lt;- plays %&gt;%\n        dplyr::left_join(at_bats, by = c(\"endTime\" = \"playEndTime\"))\n      \n      pbp &lt;- pbp %&gt;%\n        tidyr::fill(\"atBatIndex\":\"matchup.splits.menOnBase\", .direction = \"up\") %&gt;%\n        dplyr::mutate(\n          game_pk = game_pk,\n          game_date = substr(payload$gameData$datetime$dateTime, 1, 10)) %&gt;%\n        dplyr::select(\"game_pk\", \"game_date\", tidyr::everything())\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::mutate(\n          matchup.batter.fullName = factor(.data$matchup.batter.fullName),\n          matchup.pitcher.fullName = factor(.data$matchup.pitcher.fullName),\n          atBatIndex = factor(.data$atBatIndex)\n          # batted.ball.result = case_when(!result.event %in% c(\n          #   \"Single\", \"Double\", \"Triple\", \"Home Run\") ~ \"Out/Other\",\n          #   TRUE ~ result.event),\n          # batted.ball.result = factor(batted.ball.result,\n          #                             levels = c(\"Single\", \"Double\", \"Triple\", \"Home Run\", \"Out/Other\"))\n        ) %&gt;%\n        dplyr::mutate(\n          home_team = home_team,\n          home_level_id = home_level$id,\n          home_level_name = home_level$name,\n          home_parentOrg_id = payload$gameData$teams$home$parentOrgId,\n          home_parentOrg_name = payload$gameData$teams$home$parentOrgName,\n          home_league_id = home_league$id,\n          home_league_name = home_league$name,\n          away_team = away_team,\n          away_level_id = away_level$id,\n          away_level_name = away_level$name,\n          away_parentOrg_id = payload$gameData$teams$away$parentOrgId,\n          away_parentOrg_name = payload$gameData$teams$away$parentOrgName,\n          away_league_id = away_league$id,\n          away_league_name = away_league$name,\n          batting_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                       .data$home_team,\n                                       .data$away_team)),\n          fielding_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                        .data$away_team,\n                                        .data$home_team)))\n      pbp &lt;- pbp %&gt;%\n        dplyr::arrange(desc(.data$atBatIndex), desc(.data$pitchNumber))\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::group_by(.data$atBatIndex) %&gt;%\n        dplyr::mutate(\n          last.pitch.of.ab =  ifelse(.data$pitchNumber == max(.data$pitchNumber), \"true\", \"false\"),\n          last.pitch.of.ab = factor(.data$last.pitch.of.ab)) %&gt;%\n        dplyr::ungroup()\n      \n      pbp &lt;- dplyr::bind_rows(baseballr::stats_api_live_empty_df, pbp)\n      \n      check_home_level &lt;- pbp %&gt;%\n        dplyr::distinct(.data$home_level_id) %&gt;%\n        dplyr::pull()\n      \n      # this will need to be updated in the future to properly estimate X,Z coordinates at the minor league level\n      \n      # if(check_home_level != 1) {\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.pX_est = predict(x_model, pbp),\n      #                   pitchData.coordinates.pZ_est = predict(y_model, pbp))\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      # }\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::rename(\n          \"count.balls.start\" = \"count.balls.x\",\n          \"count.strikes.start\" = \"count.strikes.x\",\n          \"count.outs.start\" = \"count.outs.x\",\n          \"count.balls.end\" = \"count.balls.y\",\n          \"count.strikes.end\" = \"count.strikes.y\",\n          \"count.outs.end\" = \"count.outs.y\") %&gt;%\n        make_baseballr_data(\"MLB Play-by-Play data from MLB.com\",Sys.time())\n    },\n    error = function(e) {\n      message(glue::glue(\"{Sys.time()}: Invalid arguments provided\"))\n    },\n    finally = {\n    }\n  ) \n  return(pbp)\n}\n\n#' @rdname get_pbp_mlb\n#' @title **(legacy) Acquire pitch-by-pitch data for Major and Minor League games**\n#' @inheritParams mlb_pbp\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#' @keywords legacy\n#' @export\n# get_pbp_mlb &lt;- mlb_pbp\n\n\n\n\nExample\nHere is an example using the mlb_pbp function.\n\n\nCode\nexample &lt;- (mlb_pbp(575156))\nhead(example)\n\n\n2023-10-12 13:40:46.684707: Invalid arguments provided\n\n\n\n\nA tibble: 6 x 146\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nabout.isTopInning\nmatchup.postOnFirst.id\nmatchup.postOnFirst.fullName\nmatchup.postOnFirst.link\nmatchup.postOnSecond.id\nmatchup.postOnSecond.fullName\nmatchup.postOnSecond.link\nmatchup.postOnThird.id\nmatchup.postOnThird.fullName\nmatchup.postOnThird.link\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n575156\n2019-06-01\n5\n2019-06-01T15:38:42.000Z\n2019-06-01T19:38:07.354Z\nTRUE\npitch\n05751566-0846-0063-000c-f08cd117d70a\n6\nIn play, out(s)\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n4\n2019-06-01T15:38:19.000Z\n2019-06-01T15:38:42.000Z\nTRUE\npitch\n05751566-0846-0053-000c-f08cd117d70a\n5\nFoul\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n3\n2019-06-01T15:38:02.000Z\n2019-06-01T15:38:19.000Z\nTRUE\npitch\n05751566-0846-0043-000c-f08cd117d70a\n4\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n2\n2019-06-01T15:37:45.000Z\n2019-06-01T15:38:02.000Z\nTRUE\npitch\n05751566-0846-0033-000c-f08cd117d70a\n3\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n1\n2019-06-01T15:37:31.000Z\n2019-06-01T15:37:45.000Z\nTRUE\npitch\n05751566-0846-0023-000c-f08cd117d70a\n2\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n0\n2019-06-01T15:37:15.000Z\n2019-06-01T15:37:31.000Z\nTRUE\npitch\n05751566-0846-0013-000c-f08cd117d70a\n1\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAcquiring Data\nI will pull more data eventually, but for now I am scraping two series of games from the 2023 season.\n\n\nCode\nlibrary(baseballr)\n\n\nThe below code allows me to find the correct game_pk values that I can then use to pull play-by-play data.\n\n\nCode\n#mlb_game_pks(\"2023-06-25\")\n# mlb_game_pks(\"2023-06-24\")\n# mlb_game_pks(\"2023-06-23\")\n\n\n\n\nCode\n#game_pk values\n\n#diamondbacks/giants - 717641, 717639, 717612\n\n#mariners/orioles - 717651, 717628, 717627\n\n\n\n\nCode\nx &lt;- c(717641, 717639, 717612, 717651, 717628, 717627)\nresult &lt;- lapply(x, mlb_pbp)\ncombined_tibble &lt;- bind_rows(result)\n# Save the data to a CSV file\nwrite.csv(combined_tibble, file = \"./data/raw_data/baseballr_six_games.csv\", row.names = FALSE)\nhead(combined_tibble)\n\n\n\nA baseballr_data: 6 x 160\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nNews API\n\n\nCode\nAPI_KEY='05d7ae99b5b7455191c97c2c5c3a1f9b'\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n#updated code\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\ndef gettingdata(TOPIC):\n    URLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n    print(baseURL)\n    print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    #print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n\n    def string_cleaner(input_string):\n        try: \n            out=re.sub(r\"\"\"\n                        [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                        \\ *           # plus zero or more copies of a space,\n                        \"\"\",\n                         \" \",          # and replace it with a single space\n                        input_string, flags=re.VERBOSE)\n\n            #REPLACE SELECT CHARACTERS WITH NOTHING\n            out = re.sub('[’.]+', '', input_string)\n\n            #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n            out = re.sub(r'\\s+', ' ', out)\n\n            #CONVERT TO LOWER CASE\n            out=out.lower()\n        except:\n            print(\"ERROR\")\n            out=''\n        return out\n    \n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    #print(\"AVAILABLE KEYS:\")\n    #print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            if(verbose):\n                print(\"----------------\")\n                print(key)\n                print(article[key])\n                print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    text = cleaned_data\n    return text\n\n\n\n\nCode\n# trying stuff out\n\n# Obtain data using gettingdata function\nTOPIC = 'sports streak'\nhotstreak = gettingdata(TOPIC)\n\n\n\n\nCode\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nnltk.download('vader_lexicon')\nsia = SentimentIntensityAnalyzer()\n\nsentiment_scores = []\n\nfor text_pair in hotstreak:\n    title, description = text_pair\n    score = sia.polarity_scores(description)\n\n    sentiment_scores.append({'title': title, 'description': description, 'sentiment_score': score})\n\n\nwith open('sentiment_scores.json', 'w') as json_file:\n    json.dump(sentiment_scores, json_file, indent=4)\n\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nBelow you can see the first few rows of the newsapi.csv file:  \n\n\nIndividual Player Data\nI also want to eventually scrape specific data from fangraphs. For now, I was able to download a few tables that had game data for Aaron Judge and then merge them together. Below are screen shots of the initial csv file.   \n\n\nExtra Joke\nHow much data can be stored in a glacier? A frostbite!"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nIntroduction to Naive Bayes\nNaive Bayes, a widely acclaimed machine learning algorithm, harnesses Bayes’ Theorem to categorize data into predefined classes or categories. Praised for its simplicity, swift training capabilities, and robust performance, it stands as a foundational tool in data science. At its core, Bayes’ Theorem calculates the probability of event A given the occurrence of event B, expressed as: \\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\] Naive Bayes accomplishes classifications by leveraging feature vectors and the principles of Bayes’ Theorem to assess values. The ‘naive’ label in its name stems from its assumption of independence among predictors, simplifying computational tasks. This algorithm excels in contexts featuring text and categorical data, such as in applications like spam email identification, sentiment analysis, and document categorization. Despite its seemingly ‘naive’ premise, Naive Bayes consistently delivers impressive real-world performance, making it a crucial tool for various data science classification tasks.  Common varients of Naive Bayes include Multinomial, Guassian, and Bernoulli Naive Bayes. Multinomial Naive Bayes is the most common variant and is often used for text classification. Gaussian Naive Bayes is appropriate for continuous numerical data, while Bernoulli Naive Bayes is a derivation of Multinomial Naive Bayes that is appropriate for binary or boolean data.  The purpose of this page is to implement Naïve Bayes classification on a variety of datasets, some of which may be more for suitable than others for this method. This work is a component of my DSAN 5000 class project.\n\n\nData Preparation\nData must initially be prepared to utilize a Naive Bayes model. Although a substantial part of this process has been covered in the data cleaning and exploratory data analysis (EDA) phases, all categorical and label columns must be converted into factor types. Additionally, data must be split into training and test subsets. This is done to train the model on one subset and subsequently evaluate the model’s performance on an independent dataset which can be used to asses the bias and variance of the machine learning model. In the following code, we will complete the preparation of the 2021-22 NCAA data and news data for modeling.\n\nNCAA Data\n\n\nCode\n#here we are using R\n#let's read in the dataset\nnova2122 &lt;- read.csv('./data/modified_data/nova2122_updated.csv')\n\n\n\n\nCode\n# Load relevant libraries\nlibrary(tidyverse)\nlibrary(caret)\n\n\n\n\nCode\n#let's take another look at the dataset\nstr(nova2122)\n\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 -1 1 1 -1 1 -1 -1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : int  NA NA -1 NA NA -1 NA NA 1 NA ...\n $ lag2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n\nCode\n# changing columns to become a factor\nnova2122$lag1 &lt;- as.factor(nova2122$lag1)\nnova2122$lag2 &lt;- as.factor(nova2122$lag2)\nnova2122$lag3 &lt;- as.factor(nova2122$lag3)\nnova2122$lag4 &lt;- as.factor(nova2122$lag4)\nnova2122$lag5 &lt;- as.factor(nova2122$lag5)\nnova2122$lag6 &lt;- as.factor(nova2122$lag6)\nnova2122$shot_outcome_numeric &lt;- as.factor(nova2122$shot_outcome_numeric)\n\n\n\n\nCode\n#looking at how this changed the dataset\nstr(nova2122)\n\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: Factor w/ 2 levels \"-1\",\"1\": 1 1 1 1 2 2 1 2 1 1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : Factor w/ 2 levels \"-1\",\"1\": NA NA 1 NA NA 1 NA NA 2 NA ...\n $ lag2                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n\nCode\n# Set a seed for reproducibility\nset.seed(137)\n\n# Create an index for splitting the data (70% for training, 30% for validation)\nindex &lt;- createDataPartition(y = nova2122$shot_outcome_numeric, p = 0.7, list = FALSE)\n\n# Create the training and validation subsets\ntraining_data &lt;- nova2122[index, ]\nvalidation_data &lt;- nova2122[-index, ]\n\n#save these for later use\nwrite.csv(training_data, file = \"./data/modified_data/nova2122_training.csv\", row.names = FALSE)\nwrite.csv(validation_data, file = \"./data/modified_data/nova2122_validation.csv\", row.names = FALSE)\n\n\n\n\nNews Data\n\n\nCode\n# now we are using python\n\n#load in relevant libraries and the cleaned data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnewsapi = pd.read_csv('./data/modified_data/news_api_naive.csv')\n\n\n\n\nCode\n# let's take another look at the data\nnewsapi.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\n\n\nCode\n# Make \"Sentiment Label\" a categorical variable\nnewsapi['Sentiment Label'] = newsapi['Sentiment Label'].astype('category')\n\n# Remove rows with missing data\nnewsapi.dropna(inplace=True)\n\n# Remove unnecessary columns\nnewsapi = newsapi[['Sentiment Label', 'cleaned_text']]\n\n# Split the data into training, test, and validation subsets\ntrain_data, test_data = train_test_split(newsapi, test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n\ntrain_data.head()\n\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n69\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n85\nnegative\nminnesota twin lost straight postseason game t...\n\n\n97\nnegative\npenn state spread season surprise many two les...\n\n\n38\npositive\nround scottish woman premier league celtic ran...\n\n\n2\npositive\nwant watch minnesota viking play green bay pac...\n\n\n\n\n\n\n\n\n\n\nFeature Selection\n\n\nCode\n#load in relevant libraries\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nimport itertools\nfrom scipy.stats import spearmanr\n\n\n\n\nCode\n# This function computers the figure of merit given a subset of features\n# it works for both the Pearson and Spearman correlation matrix\n\ndef merit(x, y, correlation='pearson'):\n    k = x.shape[1]\n    \n    if correlation == 'pearson':\n        rho_xx = np.mean(np.corrcoef(x, x, rowvar = False))\n        rho_xy = np.mean(np.corrcoef(x, y, rowvar = False))\n    elif correlation == 'spearman':\n        rho_xx = np.mean(spearmanr(x, x, axis = 0)[0])\n        rho_xy = np.mean(spearmanr(x, y, axis = 0)[0])\n    else:\n        raise ValueError(\"Error: Unsupported Correlation Method. Try Again.\")\n    \n    merit_numerator = k * np.absolute(rho_xy)\n    merit_denominator = np.sqrt(k + k * (k - 1) * np.absolute(rho_xx))\n    merit_score = merit_numerator / merit_denominator\n    \n    return merit_score\n\n\n# this function takes two matrices x and y, iterates over all possible subset combinations of the x features, \n# and computes the figure of merit for each subset. It keeps track of the max merit and returns the \n# optimal subset at the end\ndef maximize_CFS(x, y):\n    num_features = x.shape[1]\n    max_merit = 0\n    optimal_subset = None\n    list1 = [*range(0, num_features)]\n    for L in range(1, len(list1) + 1):\n        print(L/(len(list1)+1))\n        for subset in itertools.combinations(list1, L):\n            x_subset = x[:, list(subset)]\n            subset_merit = merit(x_subset, y)\n            if subset_merit &gt; max_merit:\n                max_merit = subset_merit\n                optimal_subset = list(subset)\n    return optimal_subset  # Return the indices of selected features\n\n\n\nNCAA Data\n\n\nCode\ntraining = pd.read_csv('./data/modified_data/nova2122_training.csv')\nvalidation = pd.read_csv('./data/modified_data/nova2122_validation.csv')\n\n# Convert DataFrames to numpy arrays\nx = training[['lag1', 'lag2', 'lag3']].values\nx = np.nan_to_num(x, nan=0)\ny = training[['shot_outcome_numeric']].values\n\nselected_indices = maximize_CFS(x, y)\nprint(selected_indices)\n\n\n[0]\n\n\n\n\nCode\nz = training['lag1'].values\nz = np.nan_to_num(x, nan=0)\n\nmerit(z,y)\n\n\n0.3818269784940867\n\n\nAn output of [0] in the above code indicates that, according to the Correlation-based Feature Selection (CFS) algorithm, the optimal subset comprises only the ‘lag1’ feature. This suggests that, under the criteria applied, ‘lag1’ provides the most valuable information for classifying the ‘shot_outcome_numeric’ variable.\n‘lag2’ and ‘lag3’ are considered less informative for predicting ‘shot_outcome_numeric’ using this particular feature selection approach and correlation-based merit score.\nAdditionally, a merit score of 0.3818 indicates a moderate positive correlation between the ‘lag1’ feature and ‘shot_outcome_numeric,’ suggesting that ‘lag1’ contains relevant information for predicting the target variable.\n\n\nNews Data\n\n\nCode\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Define features and target variable \nX_train = train_data['cleaned_text'].tolist()\ny_train = train_data['Sentiment Label']\n\n\n\n\nCode\n# our target variable needs to be labeled numerically (without strings)\n# lets fix that now\n\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\n\n# Assuming you have a DataFrame 'df' and a column 'Sentiment Label' that you want to map\ny_train = y_train.map(label_mapping)\n\ny_train.head()\n\n\n69    0\n85   -1\n97   -1\n38    1\n2     1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the text data\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# Convert text data to numerical features using CountVectorizer\nvectorizer = CountVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# convert the vectorized training data into a dataframe\ndf = pd.DataFrame(X_train_vectorized.toarray())\n\n#let's look at the new dataframe that will be used for training the naive bayes model\ndf.describe()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\ncount\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n...\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n\n\nmean\n0.013889\n0.013889\n0.013889\n0.013889\n0.041667\n0.013889\n0.013889\n0.013889\n0.013889\n0.013889\n...\n0.013889\n0.013889\n0.013889\n0.027778\n0.097222\n0.013889\n0.055556\n0.027778\n0.041667\n0.013889\n\n\nstd\n0.117851\n0.117851\n0.117851\n0.117851\n0.201229\n0.117851\n0.117851\n0.117851\n0.117851\n0.117851\n...\n0.117851\n0.117851\n0.117851\n0.165489\n0.298339\n0.117851\n0.230669\n0.165489\n0.201229\n0.117851\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 625 columns\n\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 625 columns\n\n\n\n\n\nCode\n# it would take way to long to iterate every possible combination of subsets for x features, so let's reduce the number of features\n# we can do this by taking out words that do not appear often\n\n# let's calculate the sum of each column\ncolumn_sums = df.sum()\n\n# Find columns where the sum is 5 or less\ncolumns_to_remove = column_sums[column_sums &lt; 6].index\n\n# Remove the selected columns from the DataFrame\ndf = df.drop(columns=columns_to_remove)\n\ndf.head()\n\n\n\n\n\n\n\n\n\n80\n182\n201\n235\n267\n280\n302\n322\n355\n359\n474\n503\n519\n527\n542\n551\n601\n610\n619\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\nCode\n# the above df is much smaller and more managable than what we had before\n\n#convert both to arrays\nx = df.values\ny = y_train.values\n\n# Implement feature selection using the maximize_CFS function\noptimal_subset_indices = maximize_CFS(x, y)\n\n# Select the optimal subset of features for training and validation data\nprint(optimal_subset_indices)\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n[17]\n\n\nThe numbers above [17] served as a progress indicator, representing the completion percentage of the maximize_CFS function. The earlier simplification was necessary to prevent the function from running indefinitely.  Much like the NCAA output mentioned earlier, the [17] index signifies the optimal subset as determined by the correlation-based feature selection algorithm. In this case, that index corresponds to the word “win,” as evident in the code below. This observation implies that, based on the given criteria, the presence of the word “win” offers the most significant information for accurately classifying the sentiment label variable.\n\n\nCode\nx_opt = df.iloc[:, 17]\nx_opt.head()\n\n\n0    0\n1    0\n2    0\n3    1\n4    0\nName: 610, dtype: int64\n\n\n\n\nCode\nword_index = 610 # The column index I want to look up\n\n# Get the vocabulary (word to column index mapping)\nvocabulary = vectorizer.vocabulary_\n\n# Inverse the vocabulary mapping to find the word for the given column index\nword = next(word for word, index in vocabulary.items() if index == word_index)\n\nprint(\"Word at column 610:\", word)\n\n\nWord at column 610: win\n\n\n\n\n\nNaive Bayes with Labeled Record Data\n\n\nCode\n# Load the e1071 package\nlibrary(e1071)\n\n# let's read in the data\nnova2122_training &lt;- read.csv(\"./data/modified_data/nova2122_training.csv\")\nnova2122_validation &lt;- read.csv('./data/modified_data/nova2122_validation.csv')\n\n# loading in the data caused some of the variables to become numeric, let's change them back to factors\nnova2122_training$lag1 &lt;- as.factor(nova2122_training$lag1)\nnova2122_training$shot_outcome_numeric &lt;- as.factor(nova2122_training$shot_outcome_numeric)\nnova2122_validation$lag1 &lt;- as.factor(nova2122_validation$lag1)\nnova2122_validation$shot_outcome_numeric &lt;- as.factor(nova2122_validation$shot_outcome_numeric)\n\n# Create a Naive Bayes model\nnb_model &lt;- naiveBayes(shot_outcome_numeric ~ lag1, data = nova2122_training)\n\n# Make predictions on the validation set\nvalidation_predictions &lt;- predict(nb_model, nova2122_validation, type = \"class\")\n\n# Assess the accuracy of the model\naccuracy &lt;- mean(validation_predictions == nova2122_validation$shot_outcome_numeric)\ncat(\"Accuracy of the Naive Bayes model:\", accuracy, \"\\n\")\n\n\nAccuracy of the Naive Bayes model: 0.5463535 \n\n\n\n\nCode\n# Create a confusion matrix\nconf_matrix &lt;- confusionMatrix(data = validation_predictions, reference = nova2122_validation$shot_outcome_numeric)\n\n# Print the confusion matrix\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  -1   1\n        -1 476 400\n        1  334 408\n                                          \n               Accuracy : 0.5464          \n                 95% CI : (0.5217, 0.5708)\n    No Information Rate : 0.5006          \n    P-Value [Acc &gt; NIR] : 0.0001276       \n                                          \n                  Kappa : 0.0926          \n                                          \n Mcnemar's Test P-Value : 0.0164312       \n                                          \n            Sensitivity : 0.5877          \n            Specificity : 0.5050          \n         Pos Pred Value : 0.5434          \n         Neg Pred Value : 0.5499          \n             Prevalence : 0.5006          \n         Detection Rate : 0.2942          \n   Detection Prevalence : 0.5414          \n      Balanced Accuracy : 0.5463          \n                                          \n       'Positive' Class : -1              \n                                          \n\n\nThe accuracy of the above model, 0.5464, indicates that the model is accurate 54.64% of the time. The model has a precision of 0.5499, inicating that 54.99% of the model’s positive predictions are correct. The recall of the above model, 0.5877, indicates that the model correctly predicted 58.77% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.5681 which can be used to compare performance to other classifiers.\n\n\nCode\n#using ggplot\n\n# Create the confusion matrix data\nconf_matrix_data &lt;- data.frame(\n  Prediction = c(\"missed\", \"made\", \"missed\", \"made\"),\n  Reference = c(\"missed\", \"missed\", \"made\", \"made\"),\n  Count = c(476, 334, 400, 408)\n)\n\n# Create the ggplot\ngg &lt;- ggplot(data = conf_matrix_data, aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Count)) +\n  geom_text(aes(label = Count), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Prediction\",\n    y = \"Reference\",\n    fill = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12))\n\ngg + ggtitle(\"Confusion Matrix for 2021-22 NCAA Villanova MBB Shot Data\")\n\n\n\n\n\nAs evident in the initial matrix, the subsequent table, and the confusion matrix created using ggplot, the performance of the Naive Bayes model appears to be rather lackluster. With an accuracy rate slightly exceeding 50%, the model’s predictive ability seems only marginally better than random chance. This outcome aligns with the null hypothesis, indicating that the concept of a “hot hand” is likely a misconception, and past performance may not be a reliable predictor of success. It’s worth highlighting that applying Naive Bayes to time series data, as done here, may not be the most suitable approach, given the unique characteristics of this type of data.\n\n\nNaive Bayes with Labeled Text Data\n\n\nCode\n# what does the test data look like? \ntest_data.head()\n\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n83\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n53\nneutral\nnfl week odds includes division matchup sunday...\n\n\n70\nneutral\nremoved\n\n\n45\npositive\nreal madrid look continue winning way keep per...\n\n\n44\npositive\nflorida gator hope end road game loss streak w...\n\n\n\n\n\n\n\n\n\nCode\n# let's separate our feature(s) from our target variable\nX_test = test_data['cleaned_text']\ny_test = test_data['Sentiment Label']\nX_test.head()\n\n\n83    cbs sport network weekly coverage season keep ...\n53    nfl week odds includes division matchup sunday...\n70                                              removed\n45    real madrid look continue winning way keep per...\n44    florida gator hope end road game loss streak w...\nName: cleaned_text, dtype: object\n\n\n\n\nCode\n# since we know the feature column only includes the occurances of the word \"win\", let's create a new column 'win_count'\nX_test_win = X_test.str.lower().str.count('win')\nX_test_win.head()\n\n\n83    0.0\n53    0.0\n70    0.0\n45    1.0\n44    0.0\nName: cleaned_text, dtype: float64\n\n\n\n\nCode\n# once again, we must convert our target variable to numeric labels \n\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\ny_test = test_data['Sentiment Label'].map(label_mapping)\n\ny_test.head()\n\n\n83    0\n53    0\n70    0\n45    1\n44    1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.metrics import accuracy_score\n\n# Convert features to a DataFrame\nnews_training_x = pd.Series(x_opt, name='x')\nnews_training_y = y_train\nnews_test_x = pd.Series(X_test_win, name='x')\nnews_test_y = y_test\n\n# Create a training DataFrame\ntraining_df = pd.DataFrame({'x': news_training_x, 'y': news_training_y})\n\n# Create a test DataFrame\ntest_df = pd.DataFrame({'x': news_test_x, 'y': news_test_y})\n\n# Create a Naive Bayes model\nnb_model = CategoricalNB()\n\n# Remove rows with NaN values\ntraining_df.dropna(subset=['x'], inplace=True)\ntest_df.dropna(subset=['x'], inplace=True)\n\n# Remove rows with NaN values in the target variable 'y'\ntraining_df.dropna(subset=['y'], inplace=True)\ntest_df.dropna(subset=['y'], inplace=True)\n\n# Now, let's fit the model\nnb_model.fit(training_df[['x']], training_df['y'])\n\n# Make predictions on the validation set\nvalidation_predictions = nb_model.predict(test_df[['x']])\n\n# and we can finally assess the accuracy of the model\naccuracy = accuracy_score(test_df['y'], validation_predictions)\nprint(\"Accuracy of the Naive Bayes model:\", accuracy)\n\n\nAccuracy of the Naive Bayes model: 0.5\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(validation_predictions, test_df['y'])\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n\nConfusion Matrix:\n[[ 0  0  0]\n [ 0  0  0]\n [ 5  5 10]]\n\n\n\n\nCode\n#calculate the various metrics\naccuracy = 10/20\nprecision = 10/20\nrecall = 10/10\nf1 = 2 * ((precision * recall) / (precision + recall))\n\n# Create a dictionary with the metrics\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n    'Value': [accuracy, precision, recall, f1]\n}\n\n# Create a DataFrame from the dictionary\nmetrics_df = pd.DataFrame(metrics)\n\nprint(metrics_df)\n\n\n      Metric     Value\n0   Accuracy  0.500000\n1  Precision  0.500000\n2     Recall  1.000000\n3         F1  0.666667\n\n\nThe accuracy of the above model, 0.5, indicates that the model is accurate 50% of the time. The model has a precision of 0.5, inicating that 50% of the model’s positive predictions are correct. The recall of the above model, 1, indicates that the model correctly predicted 100% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.67 which can be used to compare performance to other classifiers.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'conf_matrix' is the confusion matrix obtained previously\n\n# Create a Seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=0.5, cbar=False,\n            xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n\n# Add labels and title\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Confusion Matrix')\n\n# Show the heatmap\nplt.show()\n\n\n\n\n\nUpon reviewing the initial matrix, subsequent table, and the Seaborn-generated confusion matrix, it becomes apparent that the Naive Bayes model’s performance is notably subpar. Despite achieving a 50% accuracy, a significant challenge emerges: the consistent prediction of positive label values points to a considerable underfitting problem. This result indicates that the employed Naive Bayes classifier was overly simplistic, resulting in inaccurate sentiment predictions for the news articles.\n\n\nExtra Joke (x2)\nAre monsters good at math? Not unless you Count Dracula.    What’s the official animal of Pi day? The Pi-thon!"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusion",
    "section": "",
    "text": "The Hot Hand: A Fallacy\nThroughout the semester, my exploration of the hot hand phenomenon aimed to uncover whether past success could serve as a predictor for future success. Employing diverse datasets—NCAA basketball data in R, news text data in Python, and baseball data from an R package and FanGraphs—I navigated through distinct scales of sports data, from play-by-play to game-by-game. The cleaning processes varied, providing a foundation for Exploratory Data Analysis (EDA) to refine our hypotheses and delineate our investigative path.\nDespite promising signals in the EDA phase suggesting the existence of the hot hand phenomenon, our Naive Bayes model, designed to predict made or missed shots using a lag variable, performed on par with random guessing. For both the clustering analysis and dimensionality reduction, the NCAA shot data required the addition of numerous numerical columns such as shot value, score differential, and game number. Although successful in reducing dimensions while preserving information, this approach did not yield a conclusive answer regarding the existence of data clusters.\nTransitioning to decision trees and random forests, the models displayed improved accuracy in predicting shot outcomes, predominantly relying on variables such as shot value (previous shots were not an indicitave factor). No evidence found substantiated the existence of the hot hand. As I look forward, I plan to revisit this project with additional datasets, including Major League Baseball at-bat data with launch speed and angle, which may offer more robust indicators of success. Once armed with a deeper understanding of time-series data and various machine learning algorithms, I am eager to revisit the topic, acknowledging the limitations of predominantly NCAA shot data in confirming the hot hand fallacy. My findings align with previous research, reinforcing the notion that the hot hand remains a fallacy.\n\n\nExtra Joke\nWhy did the conclusion take a break? Because it needed some time to ‘sum up’ its thoughts!"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nBrief Introduction to EDA\nExploratory Data Analysis (EDA) is a fundamental starting point in data analysis, helping us grasp the data’s characteristics, patterns, and possible outliers. It provides essential insights for making informed modeling decisions.\nBy analyzing the below data, I hope to gain an understanding of overall trends that can aid in refining my hypothesis and inform the construction of a more accurate model.\n\n\nncaahoopR\n\n2021-22 season\n\n\nCode\n# let's read in the data and load in relevant libraries\nnova2122 &lt;- read.csv('./data/modified_data/nova2122.csv')\n\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova2122 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player (aka field goal percentage)\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\n\nA tibble: 12 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nJustin Moore\n574\n0.4721254\n\n\nCollin Gillespie\n549\n0.5336976\n\n\nJermaine Samuels\n439\n0.5535308\n\n\nCaleb Daniels\n356\n0.5056180\n\n\nEric Dixon\n340\n0.5794118\n\n\nBrandon Slater\n308\n0.5876623\n\n\nChris Arcidiacono\n69\n0.4927536\n\n\nJordan Longino\n57\n0.4210526\n\n\nBryan Antoine\n46\n0.3043478\n\n\nTrey Patterson\n12\n0.3333333\n\n\nDhamir Cosby-Roundtree\n8\n0.5000000\n\n\nNnanna Njoku\n6\n0.5000000\n\n\n\n\n\n\n\n\nThe table displayed above, arranged in descending order based on the number of shots attempted, presents the field goal percentages of Villanova Men’s Basketball (MBB) players for the 2021-22 season. The accompanying ggplot-generated graph visually represents the count of both missed and successful shots for each player. This visualization emphasizes the significant variation in the number of shots taken by different players, which could offer richer data and potential insights for subsequent modeling.\n\n\nCode\n# Create lag variables within each shooter and game_id group\nnova2122 &lt;- nova2122 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\nwrite.csv(nova2122, file = \"./data/modified_data/nova2122_updated.csv\", row.names = FALSE)\n\n# View the updated data with lag variables\nhead(nova2122)\n\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401365747\n4\n1\nJustin Moore\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n7\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n11\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n13\n1\nEric Dixon\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n16\n1\nCollin Gillespie\nmade\nVillanova\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n18\n1\nEric Dixon\nmade\nVillanova\n1\n1\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova2122[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe correlation heatmap presented above carries an intriguing insight. Although it may not reveal strong correlations between “shot_outcome_numeric” and the lag variables individually, a notable descending trend emerges from “lag1” to “lag6.” This observation could provide valuable insight, suggesting that a player’s shot outcome is more likely to be influenced by their immediate prior shot, rather than a shot taken several attempts ago.\n\n\n2019-20 season\nTo assess potential disparities, let’s replicate the same analysis for the 2019-20 season and compare the resulting graphs and tables with those generated earlier. This comparative approach will help us identify any noticeable differences and potential insights.\n\n\nCode\n#let's read in the data\nnova1920 &lt;- read.csv('./data/modified_data/nova1920.csv')\n\n\n\n\nCode\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova1920 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\n\nA tibble: 10 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nCollin Gillespie\n491\n0.4969450\n\n\nSaddiq Bey\n458\n0.5349345\n\n\nJustin Moore\n355\n0.4647887\n\n\nJeremiah Robinson-Earl\n347\n0.5533141\n\n\nJermaine Samuels\n334\n0.5419162\n\n\nCole Swider\n171\n0.4561404\n\n\nBrandon Slater\n68\n0.3823529\n\n\nDhamir Cosby-Roundtree\n36\n0.6666667\n\n\nBryan Antoine\n25\n0.3600000\n\n\nChris Arcidiacono\n6\n0.1666667\n\n\n\n\n\n\n\n\n\n\nCode\n# Create lag variables within each shooter and game_id group\nnova1920 &lt;- nova1920 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\n# View the updated data with lag variables\nhead(nova1920)\n\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401166061\n2\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n4\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n6\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n8\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n2\n1\n1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n9\n1\nCollin Gillespie\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n11\n1\nCJ Walker\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova1920[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n\n\n\n\n\nWe can observe that, despite some player variations, most of the graphs maintain a substantial degree of consistency, which further supports the earlier findings.\n\n\n\nNews API\n\n\nCode\n#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnews_api = pd.read_csv('./data/modified_data/sentiment_scores_with_titles.csv')\n\n\n\n\nCode\n#what does this data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\n\n\n\n\n\n\n\n\n\nCode\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnews_api['cleaned_text'] = news_api['Description'].apply(preprocess_text)\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\n\nCode\nnews_api.to_csv('./data/modified_data/news_api_naive.csv', index=False)\n\n#what does the new column of data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\n\n\nCode\n# Import more necessary libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Define the function to plot the word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(10, 6))\n    # Display the word cloud\n    plt.imshow(wordcloud)\n    # Remove axis details\n    plt.axis(\"off\")\n    # Show the word cloud\n    plt.show()\n\n# Define the function to generate and display the word cloud\ndef generate_word_cloud(my_text):\n    # Generate the word cloud\n    wordcloud = WordCloud(\n        width=800,\n        height=400,\n        background_color='white',\n        colormap='viridis',\n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n    # Plot and display the word cloud\n    plot_cloud(wordcloud)\n\n# let's pass the 'cleaned_text' column to the function\ngenerate_word_cloud(' '.join(news_api['cleaned_text']))\n\n\n\n\n\nWithin the word cloud, generated from articles collected through the news API, notable recurring terms include “win,” “losing,” “victory,” “winning,” “matchup,” and others. These terms hold the potential to offer insights into the articles’ context and serve as valuable cues for conducting sentiment analysis.\n\n\nIndividual Player Data\n\n\nCode\n#let's import some libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\naaronjudge = pd.read_csv('./data/modified_data/aaronjudge.csv')\n\n\n\n\nCode\n#let's learn about the data\naaronjudge.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 36 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              100 non-null    object \n 1   Team              100 non-null    object \n 2   Opp               100 non-null    object \n 3   BO                100 non-null    int64  \n 4   Pos               100 non-null    object \n 5   PA                100 non-null    float64\n 6   H                 100 non-null    int64  \n 7   2B                100 non-null    int64  \n 8   3B                100 non-null    int64  \n 9   HR                100 non-null    int64  \n 10  R                 100 non-null    int64  \n 11  RBI               100 non-null    int64  \n 12  SB                100 non-null    int64  \n 13  CS                100 non-null    int64  \n 14  BB%               100 non-null    float64\n 15  K%                100 non-null    object \n 16  ISO               100 non-null    float64\n 17  BABIP             100 non-null    float64\n 18  EV                100 non-null    float64\n 19  AVG               100 non-null    float64\n 20  OBP               100 non-null    float64\n 21  SLG               100 non-null    float64\n 22  wOBA              100 non-null    float64\n 23  wRC+              100 non-null    int64  \n 24  Events            100 non-null    float64\n 25  EV.1              100 non-null    float64\n 26  maxEV             100 non-null    float64\n 27  LA                100 non-null    float64\n 28  Barrels           100 non-null    int64  \n 29  Barrel%           100 non-null    object \n 30  HardHit           100 non-null    int64  \n 31  HardHit%          100 non-null    float64\n 32  location          100 non-null    object \n 33  at_bats           100 non-null    float64\n 34  hard_hits         100 non-null    float64\n 35  correct_hardhit%  100 non-null    float64\ndtypes: float64(17), int64(12), object(7)\nmemory usage: 28.2+ KB\n\n\n\n\nCode\n# Create a pivot table to count the observations\npivot_table = aaronjudge.pivot_table(index='hard_hits', columns='H', aggfunc='size', fill_value=0)\n\n# Create a heatmap\nax = sns.heatmap(pivot_table, cmap=\"Blues\", annot=True, fmt=\"d\")\n\n# Customize the y-axis to start at 0 and increase as you go up\nax.set_yticklabels(ax.get_yticklabels(), rotation=0)\nax.invert_yaxis()\n\n# Customize the plot if needed\nplt.title(\"Heat Map Showing Hits v. Hard Hits\")\nplt.xlabel(\"Hits\")\nplt.ylabel(\"hard_hits\")\n\nplt.show()\n\n\n\n\n\nIn this heatmap, hits are represented on the x-axis, while hard hits are depicted on the y-axis. The unit of observation corresponds to a player’s at-bats within a game. Notably, there are instances, such as nine games for the specific player Aaron Judge, where he had two hard-hit balls but only managed to secure one hit. While the seaborn-generated graph above indeed suggests a positive correlation between these variables, there are discernible distinctions between them. It prompts the consideration that using hard hit percentage as a target variable to measure success may offer a more robust approach, as it mitigates factors beyond the batter’s control. For example, a batter might make solid contact (barrel the ball) but hit it directly to a fielder, categorizing it as a hard hit ball without resulting in a hit. Hence, hard hit percentage emerges as a more suitable target variable for assessment.\n\n\nCode\n# Sort the DataFrame by Date in ascending order\naaronjudge = aaronjudge.sort_values(by='Date')\n\n# Create subplots with 2 rows and 1 column\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\n\n# First subplot - correct_hardhit%\nsns.barplot(data=aaronjudge, y='correct_hardhit%', x='Date', ax=axes[0])\naxes[0].set_title(\"Aaron Judge Hard Hit Percentage (per each individual game) over the course of the 2023 season\")\naxes[0].set_xlabel(\"Date\")\naxes[0].set_ylabel(\"hard hit %\")\n# Get the x-axis tick positions\nx_ticks = axes[0].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[0].set_xticks(visible_ticks)\n\n# Second subplot - H\nsns.barplot(data=aaronjudge, y='H', x='Date', ax=axes[1])\naxes[1].set_title(\"Aaron Judge Hits over the course of the 2023 season\")\naxes[1].set_xlabel(\"Date\")\naxes[1].set_ylabel(\"Hits\")\n# Get the x-axis tick positions\nx_ticks = axes[1].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[1].set_xticks(visible_ticks)\n\n# Adjust the layout to avoid overlap\nplt.tight_layout()\n\n# Show the combined figure\nplt.show()\n\n\n\n\n\nThe depicted graph highlights the potential for uncovering meaningful trends in hard hit data, surpassing the simplistic examination of hits alone. It suggests the feasibility of leveraging past hard hit data to predict future hard hit performance, potentially driven by autocorrelation or seasonality. This insight holds promise for enhancing the precision of future models.\n\n\nHypothesis Refinement\nFollowing the above analysis, my null hypothesis, asserting that the “hot hand” is actually a fallacy, remains unchanged. However, the alternative hypotheses have been refined for both basketball and baseball analyses based on the insights drawn from the visualizations. In the context of basketball, the refined alternative hypothesis suggests that a player’s shot outcome is more likely to be influenced by their immediately preceding shot, rather than one taken several attempts ago. In the context of baseball, my alternative hypothesis suggests that a batter’s hard hit percentage is more likely influenced by their prior hard hit percentage rather than solely assessing success or streaks based on hits.\n\n\nExtra Joke\nWhat kind of car does Darth Vader drive? A toy-Yoda!  \n\n\nWatch Out!\nyou can’t be too careful when exploring!"
  }
]