[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "I will replace this later with my actual data"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Why did the conclusion take a break? Because it needed some time to ‘sum up’ its thoughts!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello!\nHello and welcome to my website! My name is Billy and I am a graduate student at Georgetown University. I am currently pursuing a Master’s in Data Science and Analytics and will use this website as a place to display my work. I hope you find my work insightful and enjoy your time on my website!\n\n\nGrowing Up\nI grew up in Scotch Plains, New Jersey along with my two sisters, Melissa and Jessica. We are triplets! Below is a picture of me with my family and another of our dog Roxie.\n\n \n\n\n\nEducation\nI graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics along with a PPE (Politics, Philosophy, & Economics) Concentration and Political Science minor. I absolutely loved my time at Villanova between the academics, friendships, the school winning the NCAA Championship and more!. As an undergrad, I was involved in a number of student organizations, including the Blue Key Society, where I served as a tour guide for undergraduate admissions. I also organized accepted students days for the university.\nOne of the best experiences I had during college was studying abroad. In 2019, I participated in the Institute of Politics and Economics Program at the University of Cambridge where I took a number of economic classes and study international relations and history. Seeing how these subject matters interact with analytics truly fascinated me, which is one reason why I am pursuing an advanced degree in data science. I am extremely excited to continue my education and pursue a masters degree in data science!\n\n\nTop Shelf Designs, LLC\nDuring my senior year at Villanova, I had the opportunity to start my own business with my sisters! Our business, Top Shelf Designs, LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. Being an entrepreneur has been an incredibly rewarding journey thus far, and I am eager to see where it takes us. Here is a link to our website!\n\n\nHobbies\nI’m an avid sports fan, and love watching the New York Yankees, Villanova basketball, and NY Giants. Unfortunately, none of my teams have been doing too well lately, but I’m hoping the Giants can turn it around this season! I’m fascinated by how analytics is being used to transform the sports landscape - even across college sports! While at Villanova, I researched the Houston Astros Cheating Scandal. I would love to go back and take an even deeper dive into the subject one day!\nI love music - you can often find me with headphones or earbuds in while doing work or exploring the DC area! Additionally, I find playing instruments to be a great creative outlet after a long day of work. I grew up taking piano lessons and during COVID started to teach myself how to play the guitar!\nWhenever I have the chance, I try to travel. Wile abroad, I was lucky enough to travel to a few places in Europe. My favorite spot has to be Italy - from the history and culture to the food, wine, and sports, I am counting down the days until I can go back.\nFinally, I thoroughly enjoy books, movies, video games in addition to fun, in-person experiences. Lately, I’ve been exploring the DC area which I am loving so far! If you have any recommendations - please let me know! I’m alwyas willing to try something new!\n\n\nGeneral Info (inc. netID)\n\n\n\nname\nBilly McGloin\n\n\nGU netID\nwtm30\n\n\nLinkedIn\nlink\n\n\n\n\n\nJokes\nI always like to have a laugh so below are some jokes I hope you’ll enjoy!\n\nWebsite Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki\n\n\n\nAssorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!\n\n\n\n\nHoya Saxa!\n\n\n\nme and a bunch of rocks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN_website",
    "section": "",
    "text": "This will be updated with some cool stuff in a bit! Stay tuned!\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Baseballr",
    "section": "",
    "text": "---\ntitle: \"Data Cleaning\"\nformat: html\ntoc: true\ntoc_depth: 3 \nnumber sections: true \ncode-fold: show\n---\n\n*Disclaimer - this section is a work in progress  This page shows the raw data, the code used to clean it, and the modified data. It’s a journal of my data cleaning process. Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nncaahoopR\nlet’s clean the Villanova 2021-22 data with R:\nhere is a screen shot of the first few rows and columns of the raw data:  \n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# let's load in the data\nnova2122 &lt;- read.csv('./data/raw_data/villanova2122.csv')\n\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n1140539\n\n\n\n# what are the column names?\ncolnames(nova2122)\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova2122 &lt;- nova2122 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n539939\n\n\n\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- nova2122 %&gt;% select(game_id, play_id, half, shooter, shot_outcome)\n\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova2122.csv\", row.names = FALSE)\n\nHere is a screen shot of the modified data:  \n\n\nreddit\n\n\nnewsapi\nlet’s clean this using python:  here is a picture of the first few rows of the raw data:  \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#reading in the file\nnewsapi = pd.read_csv('./data/raw_data/newsapi.csv')\n\n\n# what is the shape of this data\nnewsapi.shape\n\n(100, 4)\n\n\n\n# what are the column names\nnewsapi.columns\n\nIndex(['0', '1', '2', '3'], dtype='object')\n\n\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n\n\nTrue\n\n\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnewsapi['cleaned_text'] = newsapi['2'].apply(preprocess_text)\n\n\n# Initialize the CountVectorizer\ncount_vectorizer = CountVectorizer()\n\n# Fit and transform the preprocessed text data\nX = count_vectorizer.fit_transform(newsapi['cleaned_text'])\n\n# printing part of the sparse matrix\nprint(X[:20, :20].toarray())\n\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]]\n\n\n\n# Convert the sparse matrix to a Pandas DataFrame\nX_df = pd.DataFrame(X.toarray())\n\n# Display the first few rows of the DataFrame\nprint(X_df.head())\n\n   0    1    2    3    4    5    6    7    8    9    ...  539  540  541  542  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   543  544  545  546  547  548  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 549 columns]\n\n\n\nvocab = count_vectorizer.get_feature_names_out()\n\nprint(vocab)\n\n['abnormality' 'abound' 'ac' 'acc' 'action' 'advice' 'aew' 'ahead' 'al'\n 'alds' 'alert' 'already' 'alyssa' 'amid' 'andy' 'angelos' 'another'\n 'answer' 'apple' 'argentina' 'arizona' 'arkansas' 'armondo' 'arsenal'\n 'astonishing' 'austin' 'back' 'balogun' 'ban' 'baseball' 'battle' 'bear'\n 'bearcat' 'beat' 'beaten' 'become' 'becoming' 'behind' 'belief'\n 'bellingham' 'bengal' 'besides' 'best' 'bet' 'better' 'beyond' 'big'\n 'biggs' 'bird' 'blount' 'blue' 'blunder' 'bonus' 'booing' 'booking'\n 'boston' 'bottom' 'brad' 'brain' 'brave' 'breaking' 'breanna' 'brewer'\n 'buccaneer' 'building' 'bukayo' 'bumper' 'call' 'candidate' 'cant' 'card'\n 'cardinal' 'cargill' 'carlos' 'case' 'casino' 'catch' 'central' 'cfb'\n 'champion' 'charge' 'chicago' 'christian' 'cincinnati' 'city' 'climate'\n 'clinch' 'close' 'closer' 'clue' 'clutch' 'coach' 'coaching'\n 'cockeysville' 'college' 'colorado' 'column' 'come' 'coming' 'commander'\n 'commits' 'comparison' 'complete' 'concern' 'conference' 'contender'\n 'corner' 'could' 'cover' 'coverage' 'cowboy' 'cpa' 'craziness' 'crazy'\n 'crop' 'crowd' 'cub' 'cup' 'dalton' 'dame' 'daniel' 'david' 'debut'\n 'defense' 'deion' 'delight' 'deserved' 'desmond' 'desperate' 'despite'\n 'division' 'dolphin' 'doubleheader' 'draw' 'driven' 'drought' 'duck'\n 'duke' 'dy' 'eagle' 'earns' 'eberflus' 'edge' 'emerges' 'end' 'enter'\n 'episode' 'er' 'europe' 'evaluation' 'even' 'event' 'ever' 'everywhere'\n 'evokes' 'excited' 'exit' 'expert' 'eye' 'fact' 'fade' 'falcon' 'famu'\n 'fantasy' 'father' 'favored' 'favorite' 'fiba' 'field' 'fighter'\n 'fighting' 'final' 'fire' 'fired' 'first' 'five' 'fix' 'fiziev' 'fizzle'\n 'florida' 'flyweight' 'focus' 'folarin' 'football' 'force' 'form' 'found'\n 'franklin' 'game' 'gamrot' 'gen' 'giant' 'goal' 'god' 'goff' 'going'\n 'good' 'got' 'grade' 'grasso' 'guez' 'ham' 'hardy' 'harsh' 'head' 'heat'\n 'heavily' 'here' 'hero' 'highlight' 'hit' 'home' 'honor' 'hot' 'huge'\n 'hurricane' 'hype' 'image' 'infamous' 'injures' 'issue' 'ja' 'jack'\n 'jade' 'jay' 'jet' 'jimmy' 'john' 'join' 'jones' 'journey' 'jr' 'jude'\n 'julio' 'justin' 'kansa' 'keep' 'kelce' 'key' 'king' 'knee' 'knockout'\n 'larger' 'laugh' 'leaf' 'league' 'learned' 'life' 'line' 'lion' 'lionel'\n 'live' 'livestream' 'locked' 'lofty' 'logan' 'look' 'loose' 'loser'\n 'loses' 'losing' 'loss' 'lsu' 'lucas' 'madrid' 'mailbag' 'main' 'make'\n 'makeover' 'man' 'manager' 'manchester' 'marquee' 'maryland' 'match'\n 'mateusz' 'matt' 'matter' 'mature' 'meeting' 'megan' 'messi' 'miami'\n 'middleweight' 'milan' 'mlb' 'mojo' 'moment' 'monday' 'morning' 'move'\n 'must' 'mvp' 'nailed' 'napoli' 'nbas' 'near' 'network' 'new' 'newcastle'\n 'nfl' 'night' 'noche' 'normal' 'notify' 'notre' 'nowhere' 'number' 'nwsl'\n 'nycs' 'nz' 'odds' 'ode' 'offense' 'ohio' 'oklahoma' 'onuachu' 'opponent'\n 'option' 'oregon' 'oriole' 'padre' 'panther' 'pass' 'pat' 'patriot'\n 'penn' 'penultimate' 'perfect' 'peter' 'phillies' 'philly' 'pick'\n 'pirate' 'play' 'player' 'playing' 'playoff' 'plenty' 'plus' 'point'\n 'politics' 'pool' 'pound' 'power' 'prediction' 'pretender' 'preview'\n 'previewing' 'problem' 'prop' 'provides' 'pulisic' 'purdy' 'question'\n 'race' 'racer' 'racing' 'rafael' 'ram' 'ranger' 'ranking' 'rapinoe' 'ray'\n 'read' 'real' 'recap' 'record' 'recruit' 'red' 'regular' 'reign' 'relish'\n 'remain' 'removed' 'report' 'resident' 'retro' 'return' 'review' 'revved'\n 'ridder' 'ripe' 'rise' 'river' 'rock' 'rodon' 'rodr' 'rome' 'ross'\n 'rumor' 'run' 'running' 'ryder' 'saka' 'sander' 'say' 'schedule' 'score'\n 'sean' 'search' 'season' 'seat' 'sec' 'secure' 'sends' 'sept' 'series'\n 'served' 'shadow' 'shanahan' 'shevchenko' 'shift' 'show' 'showdown'\n 'since' 'skid' 'slate' 'smack' 'snap' 'sold' 'someone' 'sooner' 'sox'\n 'special' 'speed' 'spiro' 'sport' 'spot' 'spotlight' 'spread' 'st'\n 'stamp' 'stand' 'star' 'start' 'starting' 'state' 'station' 'stats'\n 'stay' 'stellar' 'stewart' 'stoppage' 'straight' 'strategy' 'streak'\n 'stream' 'strickland' 'struggling' 'success' 'summer' 'survivor'\n 'suwinski' 'suzuki' 'sv' 'sweep' 'swift' 'tailspin' 'take' 'takeaway'\n 'taking' 'tale' 'talking' 'tape' 'taylor' 'tcu' 'team' 'ten' 'test'\n 'texas' 'theater' 'thing' 'think' 'thirteen' 'thomas' 'thought' 'three'\n 'thriller' 'thursday' 'tight' 'time' 'title' 'tko' 'tnf' 'today' 'tom'\n 'top' 'total' 'tournament' 'trail' 'transformation' 'travis' 'trea'\n 'trojan' 'troy' 'trust' 'trying' 'tssaa' 'turkey' 'turner' 'tv' 'twin'\n 'two' 'ucf' 'ufc' 'unbeatable' 'unbeaten' 'united' 'unsung' 'upset'\n 'upside' 'usc' 'usmnt' 'uwf' 'vega' 'veloudos' 'video' 'view' 'vlad'\n 'wagon' 'warm' 'watch' 'watching' 'week' 'weekend' 'welcome' 'well'\n 'went' 'white' 'whitner' 'wild' 'wildcat' 'williams' 'wilson' 'win'\n 'winner' 'winning' 'wnba' 'woman' 'world' 'wr' 'wrong' 'wwe' 'yahoo'\n 'yankee' 'year' 'yet' 'youth']\n\n\n\n\nindividual player data\nlet’s clean the aaron judge game data with python:\nhere is a screen shot of the first few rows of the raw data:  \n\nimport pandas as pd\nimport numpy as np\n\n\n#reading in the file\naaronjudge = pd.read_csv('./data/raw_data/AaronJudgeData.csv')\n\n\n#how many rows are in this dataset?\naaronjudge.shape\n\n(111, 37)\n\n\n\n#what are the column names?\naaronjudge.columns\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1',\n       'Events', 'EV.1', 'maxEV', 'LA', 'Barrels', 'Barrel%', 'HardHit',\n       'HardHit%'],\n      dtype='object')\n\n\n\n#removing the repeated columns\ncolumns_to_remove = ['Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1']\naaronjudge.drop(columns=columns_to_remove, inplace=True)\naaronjudge.columns\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Events', 'EV.1', 'maxEV', 'LA', 'Barrels',\n       'Barrel%', 'HardHit', 'HardHit%'],\n      dtype='object')\n\n\n\n# i belive the initial row with the column names is repeated throughou the data. let's check\nprint((aaronjudge['Date'] == 'Date').sum())\n\n5\n\n\n\n# let's remove these rows and then check the shape again\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Date'].index, inplace=True)\naaronjudge.shape\n\n(106, 32)\n\n\n\n# there is also a total row which I want to remove as well. let's do that now\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Total'].index, inplace=True)\naaronjudge.shape\n\n(105, 32)\n\n\n\n# so far, I have removed 6 rows and 5 columns. \n\n# I want to create a \"location\" column based on the \"@\" in the \"Opp\" column\naaronjudge['location'] = aaronjudge['Opp'].apply(lambda x: 'away' if '@' in x else 'home')\n\n# Remove the \"@\" symbol from the values in the \"Opp\" column\naaronjudge['Opp'] = aaronjudge['Opp'].str.replace('@', '')\n\n# check value counts of the new \"location\" column\nprint(aaronjudge['location'].value_counts()) #this seems accurate\n\nhome    53\naway    52\nName: location, dtype: int64\n\n\n\nprint(aaronjudge['PA'].dtype)\nprint(aaronjudge['BB%'].dtype)\n\nobject\nobject\n\n\n\n# I want to create two new columns. The number of at bats per each game and the number of hard hits in each game. \n# for this project, we are going to calculate at-bats as should be the number of plate appearances minus walks (sacrifices and HBP are not included in this dataset)\n\n#first i have to remove the '%' symbol and convert 'BB%' to a float\n\naaronjudge['BB%'] = aaronjudge['BB%'].astype(str)\naaronjudge['BB%'] = aaronjudge['BB%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'BB%' column to three decimal places\naaronjudge['BB%'] = aaronjudge['BB%'].round(3)\n\n#print(aaronjudge['BB%'].mean())\n\n#convert 'PA' to a float\naaronjudge['PA'] = aaronjudge['PA'].astype(float)\n\n# now I can create the new at_bats column\naaronjudge['at_bats'] = aaronjudge['PA'] * (1 - aaronjudge['BB%'])\n\n#now lets see the average number of at bats vs the average number of plate appearances\nprint(aaronjudge['at_bats'].mean())\nprint(aaronjudge['PA'].mean())\n\n3.4857333333333336\n4.314285714285714\n\n\n\n# now I want to create a new column for hard hits per game\n# we can do this by multiplying the hard hit percentage by the events column (these columns were part of a different table that was merged with the original table)\n\nprint(aaronjudge['HardHit%'].dtype)\nprint(aaronjudge['Events'].dtype)\n\nobject\nobject\n\n\n\n# this code is very similar to what we just did\n\n#first i have to remove the '%' symbol and convert 'HardHit%' to a float\n\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].astype(str)\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'HardHit%' column to three decimal places\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].round(3)\n\n#print(aaronjudge['HardHit%'].mean())\n\n#convert 'Events' to a float\naaronjudge['Events'] = aaronjudge['Events'].astype(float)\n\n# now I can create the new hard_hits column\naaronjudge['hard_hits'] = (aaronjudge['Events'] * aaronjudge['HardHit%']).round(0)\n\n#now lets see the average number of hard_hits per game\nprint(aaronjudge['hard_hits'].mean())\n\n1.52\n\n\n\n# finally, let's create a correct hardhit% column that is based on the number of at-bats, not the number of times a player puts the ball in play\naaronjudge['correct_hardhit%'] = (aaronjudge['hard_hits'] / aaronjudge['at_bats']).round(2)\n\n# now let's see the average correct hardhit% for Aaron Judge\nprint(aaronjudge['correct_hardhit%'].mean())\n\n0.42829999999999996\n\n\n\n# sometimes in certain stadiums or based on the weather, the HardHit% data is missing\n# this causes the value of the new correct_hardhit% column to be NaN, so let's remove those few rows\naaronjudge.dropna(subset=['correct_hardhit%'], inplace=True)\n\n#let's check the shape again\naaronjudge.shape #loss of 5 rows\n\n(100, 36)\n\n\n\n# now we can save this to a csv file\naaronjudge.to_csv('./data/modified_data/aaronjudge.csv', index=False)\n\nhere is a screenshot of the first couple rows of the modified csv file:  \n\n\nExtra Joke\nWhat did the broom say to the vacuum?\n“I’m so tired of people pushing us around.”"
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "three more to go!"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "ipsum lorem"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "yoda data master!"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  This page will take you through the data sources and methodologies employed in this specific project. Furthermore, you can find brief descriptions/images/tables of the various datasets mentioned. Data must be acquired using at least one Python API and one R API. This project will use various data formats that may include labeled data, qualitative data, text data, geo data, record-data, etc.\n\nBaseballr\n“Baseballr” is a package in R that focuses on baseball analytics, also known as sabremetrics. It includes various functions that can be used for scraping data from websites like FanGraphs.com, Baseball-Reference.com, and BaseballSavant.mlb.com. It also includes functions for calculating specific baseball metrics such as wOBA (weighted on-base average) and FIP (fielding independent pitching). I will mainly use this package to gather data (which uses an API as can be seen below).\n\nSource Code\nThe below source code was pulled from the baseballr github repository. This specific code uses a mlb api to acquire play-by-play data for a specific game. I will use these functions later on through the baseballr package.\n\n\nCode\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\nmlb_api_call &lt;- function(url){\n  res &lt;-\n    httr::RETRY(\"GET\", url)\n  \n  json &lt;- res$content %&gt;%\n    rawToChar() %&gt;%\n    jsonlite::fromJSON(simplifyVector = T)\n  \n  return(json)\n}\n\nmlb_stats_endpoint &lt;- function(endpoint){\n  all_endpoints = c(\n    \"v1/attendance\",#\n    \"v1/conferences\",#\n    \"v1/conferences/{conferenceId}\",#\n    \"v1/awards/{awardId}/recipients\",#\n    \"v1/awards\",#\n    \"v1/baseballStats\",#\n    \"v1/eventTypes\",#\n    \"v1/fielderDetailTypes\",#\n    \"v1/gameStatus\",#\n    \"v1/gameTypes\",#\n    \"v1/highLow/types\",#\n    \"v1/hitTrajectories\",#\n    \"v1/jobTypes\",#\n    \"v1/languages\",\n    \"v1/leagueLeaderTypes\",#\n    \"v1/logicalEvents\",#\n    \"v1/metrics\",#\n    \"v1/pitchCodes\",#\n    \"v1/pitchTypes\",#\n    \"v1/playerStatusCodes\",#\n    \"v1/positions\",#\n    \"v1/reviewReasons\",#\n    \"v1/rosterTypes\",#\n    \"v1/runnerDetailTypes\",#\n    \"v1/scheduleEventTypes\",#\n    \"v1/situationCodes\",#\n    \"v1/sky\",#\n    \"v1/standingsTypes\",#\n    \"v1/statGroups\",#\n    \"v1/statTypes\",#\n    \"v1/windDirection\",#\n    \"v1/divisions\",#\n    \"v1/draft/{year}\",#\n    \"v1/draft/prospects/{year}\",#\n    \"v1/draft/{year}/latest\",#\n    \"v1.1/game/{gamePk}/feed/live\",\n    \"v1.1/game/{gamePk}/feed/live/diffPatch\",#\n    \"v1.1/game/{gamePk}/feed/live/timestamps\",#\n    \"v1/game/changes\",##x\n    \"v1/game/analytics/game\",##x\n    \"v1/game/analytics/guids\",##x\n    \"v1/game/{gamePk}/guids\",##x\n    \"v1/game/{gamePk}/{GUID}/analytics\",##x\n    \"v1/game/{gamePk}/{GUID}/contextMetricsAverages\",##x\n    \"v1/game/{gamePk}/contextMetrics\",#\n    \"v1/game/{gamePk}/winProbability\",#\n    \"v1/game/{gamePk}/boxscore\",#\n    \"v1/game/{gamePk}/content\",#\n    \"v1/game/{gamePk}/feed/color\",##x\n    \"v1/game/{gamePk}/feed/color/diffPatch\",##x\n    \"v1/game/{gamePk}/feed/color/timestamps\",##x\n    \"v1/game/{gamePk}/linescore\",#\n    \"v1/game/{gamePk}/playByPlay\",#\n    \"v1/gamePace\",#\n    \"v1/highLow/{orgType}\",#\n    \"v1/homeRunDerby/{gamePk}\",#\n    \"v1/homeRunDerby/{gamePk}/bracket\",#\n    \"v1/homeRunDerby/{gamePk}/pool\",#\n    \"v1/league\",#\n    \"v1/league/{leagueId}/allStarBallot\",#\n    \"v1/league/{leagueId}/allStarWriteIns\",#\n    \"v1/league/{leagueId}/allStarFinalVote\",#\n    \"v1/people\",#\n    \"v1/people/freeAgents\",#\n    \"v1/people/{personId}\",##U\n    \"v1/people/{personId}/stats/game/{gamePk}\",#\n    \"v1/people/{personId}/stats/game/current\",#\n    \"v1/jobs\",#\n    \"v1/jobs/umpires\",#\n    \"v1/jobs/datacasters\",#\n    \"v1/jobs/officialScorers\",#\n    \"v1/jobs/umpires/games/{umpireId}\",##x\n    \"v1/schedule/\",#\n    \"v1/schedule/games/tied\",#\n    \"v1/schedule/postseason\",#\n    \"v1/schedule/postseason/series\",#\n    \"v1/schedule/postseason/tuneIn\",##x\n    \"v1/seasons\",#\n    \"v1/seasons/all\",#\n    \"v1/seasons/{seasonId}\",#\n    \"v1/sports\",#\n    \"v1/sports/{sportId}\",#\n    \"v1/sports/{sportId}/players\",#\n    \"v1/standings\",#\n    \"v1/stats\",#\n    \"v1/stats/metrics\",##x\n    \"v1/stats/leaders\",#\n    \"v1/stats/streaks\",##404\n    \"v1/teams\",#\n    \"v1/teams/history\",#\n    \"v1/teams/stats\",#\n    \"v1/teams/stats/leaders\",#\n    \"v1/teams/affiliates\",#\n    \"v1/teams/{teamId}\",#\n    \"v1/teams/{teamId}/stats\",#\n    \"v1/teams/{teamId}/affiliates\",#\n    \"v1/teams/{teamId}/alumni\",#\n    \"v1/teams/{teamId}/coaches\",#\n    \"v1/teams/{teamId}/personnel\",#\n    \"v1/teams/{teamId}/leaders\",#\n    \"v1/teams/{teamId}/roster\",##x\n    \"v1/teams/{teamId}/roster/{rosterType}\",#\n    \"v1/venues\"#\n  )\n  base_url = glue::glue('http://statsapi.mlb.com/api/{endpoint}')\n  return(base_url)\n}\n\n\n\n\n\nCode\nx &lt;- \"http://statsapi.mlb.com/api/v1/game/575156/playByPlay\"\n\noutput &lt;- mlb_api_call(x)\n\n\n“output” is a very messy list that is extremely long. Instead of printing “output”, below are three images of part of the list.\n  \nThe below code builds on the previous code, returning a tibble that includes over 100 columns of data provided by the MLB Stats API at a pitch level. As you will see, the output is much cleaner and easier to work with.\n\n\nCode\n#' @rdname mlb_pbp\n#' @title **Acquire pitch-by-pitch data for Major and Minor League games**\n#'\n#' @param game_pk The date for which you want to find game_pk values for MLB games\n#' @importFrom jsonlite fromJSON\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#'\n#' Some data will vary depending on the\n#' park and the league level, as most sensor data is not available in\n#' minor league parks via this API. Note that the column names have mostly\n#' been left as-is and there are likely duplicate columns in terms of the\n#' information they provide. I plan to clean the output up down the road, but\n#' for now I am leaving the majority as-is.\n#'\n#' Both major and minor league pitch-by-pitch data can be pulled with this function.\n#' \n#'  |col_name                       |types     |\n#'  |:------------------------------|:---------|\n#'  |game_pk                        |numeric   |\n#'  |game_date                      |character |\n#'  |index                          |integer   |\n#'  |startTime                      |character |\n#'  |endTime                        |character |\n#'  |isPitch                        |logical   |\n#'  |type                           |character |\n#'  |playId                         |character |\n#'  |pitchNumber                    |integer   |\n#'  |details.description            |character |\n#'  |details.event                  |character |\n#'  |details.awayScore              |integer   |\n#'  |details.homeScore              |integer   |\n#'  |details.isScoringPlay          |logical   |\n#'  |details.hasReview              |logical   |\n#'  |details.code                   |character |\n#'  |details.ballColor              |character |\n#'  |details.isInPlay               |logical   |\n#'  |details.isStrike               |logical   |\n#'  |details.isBall                 |logical   |\n#'  |details.call.code              |character |\n#'  |details.call.description       |character |\n#'  |count.balls.start              |integer   |\n#'  |count.strikes.start            |integer   |\n#'  |count.outs.start               |integer   |\n#'  |player.id                      |integer   |\n#'  |player.link                    |character |\n#'  |pitchData.strikeZoneTop        |numeric   |\n#'  |pitchData.strikeZoneBottom     |numeric   |\n#'  |details.fromCatcher            |logical   |\n#'  |pitchData.coordinates.x        |numeric   |\n#'  |pitchData.coordinates.y        |numeric   |\n#'  |hitData.trajectory             |character |\n#'  |hitData.hardness               |character |\n#'  |hitData.location               |character |\n#'  |hitData.coordinates.coordX     |numeric   |\n#'  |hitData.coordinates.coordY     |numeric   |\n#'  |actionPlayId                   |character |\n#'  |details.eventType              |character |\n#'  |details.runnerGoing            |logical   |\n#'  |position.code                  |character |\n#'  |position.name                  |character |\n#'  |position.type                  |character |\n#'  |position.abbreviation          |character |\n#'  |battingOrder                   |character |\n#'  |atBatIndex                     |character |\n#'  |result.type                    |character |\n#'  |result.event                   |character |\n#'  |result.eventType               |character |\n#'  |result.description             |character |\n#'  |result.rbi                     |integer   |\n#'  |result.awayScore               |integer   |\n#'  |result.homeScore               |integer   |\n#'  |about.atBatIndex               |integer   |\n#'  |about.halfInning               |character |\n#'  |about.inning                   |integer   |\n#'  |about.startTime                |character |\n#'  |about.endTime                  |character |\n#'  |about.isComplete               |logical   |\n#'  |about.isScoringPlay            |logical   |\n#'  |about.hasReview                |logical   |\n#'  |about.hasOut                   |logical   |\n#'  |about.captivatingIndex         |integer   |\n#'  |count.balls.end                |integer   |\n#'  |count.strikes.end              |integer   |\n#'  |count.outs.end                 |integer   |\n#'  |matchup.batter.id              |integer   |\n#'  |matchup.batter.fullName        |character |\n#'  |matchup.batter.link            |character |\n#'  |matchup.batSide.code           |character |\n#'  |matchup.batSide.description    |character |\n#'  |matchup.pitcher.id             |integer   |\n#'  |matchup.pitcher.fullName       |character |\n#'  |matchup.pitcher.link           |character |\n#'  |matchup.pitchHand.code         |character |\n#'  |matchup.pitchHand.description  |character |\n#'  |matchup.splits.batter          |character |\n#'  |matchup.splits.pitcher         |character |\n#'  |matchup.splits.menOnBase       |character |\n#'  |batted.ball.result             |factor    |\n#'  |home_team                      |character |\n#'  |home_level_id                  |integer   |\n#'  |home_level_name                |character |\n#'  |home_parentOrg_id              |integer   |\n#'  |home_parentOrg_name            |character |\n#'  |home_league_id                 |integer   |\n#'  |home_league_name               |character |\n#'  |away_team                      |character |\n#'  |away_level_id                  |integer   |\n#'  |away_level_name                |character |\n#'  |away_parentOrg_id              |integer   |\n#'  |away_parentOrg_name            |character |\n#'  |away_league_id                 |integer   |\n#'  |away_league_name               |character |\n#'  |batting_team                   |character |\n#'  |fielding_team                  |character |\n#'  |last.pitch.of.ab               |character |\n#'  |pfxId                          |character |\n#'  |details.trailColor             |character |\n#'  |details.type.code              |character |\n#'  |details.type.description       |character |\n#'  |pitchData.startSpeed           |numeric   |\n#'  |pitchData.endSpeed             |numeric   |\n#'  |pitchData.zone                 |integer   |\n#'  |pitchData.typeConfidence       |numeric   |\n#'  |pitchData.plateTime            |numeric   |\n#'  |pitchData.extension            |numeric   |\n#'  |pitchData.coordinates.aY       |numeric   |\n#'  |pitchData.coordinates.aZ       |numeric   |\n#'  |pitchData.coordinates.pfxX     |numeric   |\n#'  |pitchData.coordinates.pfxZ     |numeric   |\n#'  |pitchData.coordinates.pX       |numeric   |\n#'  |pitchData.coordinates.pZ       |numeric   |\n#'  |pitchData.coordinates.vX0      |numeric   |\n#'  |pitchData.coordinates.vY0      |numeric   |\n#'  |pitchData.coordinates.vZ0      |numeric   |\n#'  |pitchData.coordinates.x0       |numeric   |\n#'  |pitchData.coordinates.y0       |numeric   |\n#'  |pitchData.coordinates.z0       |numeric   |\n#'  |pitchData.coordinates.aX       |numeric   |\n#'  |pitchData.breaks.breakAngle    |numeric   |\n#'  |pitchData.breaks.breakLength   |numeric   |\n#'  |pitchData.breaks.breakY        |numeric   |\n#'  |pitchData.breaks.spinRate      |integer   |\n#'  |pitchData.breaks.spinDirection |integer   |\n#'  |hitData.launchSpeed            |numeric   |\n#'  |hitData.launchAngle            |numeric   |\n#'  |hitData.totalDistance          |numeric   |\n#'  |injuryType                     |character |\n#'  |umpire.id                      |integer   |\n#'  |umpire.link                    |character |\n#'  |isBaseRunningPlay              |logical   |\n#'  |isSubstitution                 |logical   |\n#'  |about.isTopInning              |logical   |\n#'  |matchup.postOnFirst.id         |integer   |\n#'  |matchup.postOnFirst.fullName   |character |\n#'  |matchup.postOnFirst.link       |character |\n#'  |matchup.postOnSecond.id        |integer   |\n#'  |matchup.postOnSecond.fullName  |character |\n#'  |matchup.postOnSecond.link      |character |\n#'  |matchup.postOnThird.id         |integer   |\n#'  |matchup.postOnThird.fullName   |character |\n#'  |matchup.postOnThird.link       |character |\n#' @export\n#' @examples \\donttest{\n#'   try(mlb_pbp(game_pk = 632970))\n#' }\n\nmlb_pbp &lt;- function(game_pk) {\n  \n  mlb_endpoint &lt;- mlb_stats_endpoint(glue::glue(\"v1.1/game/{game_pk}/feed/live\"))\n  \n  tryCatch(\n    expr = {\n      payload &lt;- mlb_endpoint %&gt;% \n        mlb_api_call() %&gt;% \n        jsonlite::toJSON() %&gt;% \n        jsonlite::fromJSON(flatten = TRUE)\n      \n      plays &lt;- payload$liveData$plays$allPlays$playEvents %&gt;% \n        dplyr::bind_rows()\n      \n      at_bats &lt;- payload$liveData$plays$allPlays\n      \n      current &lt;- payload$liveData$plays$currentPlay\n      \n      game_status &lt;- payload$gameData$status$abstractGameState\n      \n      home_team &lt;- payload$gameData$teams$home$name\n      \n      home_level &lt;- payload$gameData$teams$home$sport\n      \n      home_league &lt;- payload$gameData$teams$home$league\n      \n      away_team &lt;- payload$gameData$teams$away$name\n      \n      away_level &lt;- payload$gameData$teams$away$sport\n      \n      away_league &lt;- payload$gameData$teams$away$league\n      \n      columns &lt;- lapply(at_bats, function(x) class(x)) %&gt;%\n        dplyr::bind_rows(.id = \"variable\")\n      cols &lt;- c(colnames(columns))\n      classes &lt;- c(t(unname(columns[1,])))\n      \n      df &lt;- data.frame(cols, classes)\n      list_columns &lt;- df %&gt;%\n        dplyr::filter(.data$classes == \"list\") %&gt;%\n        dplyr::pull(\"cols\")\n      \n      at_bats &lt;- at_bats %&gt;%\n        dplyr::select(-c(tidyr::one_of(list_columns)))\n      \n      pbp &lt;- plays %&gt;%\n        dplyr::left_join(at_bats, by = c(\"endTime\" = \"playEndTime\"))\n      \n      pbp &lt;- pbp %&gt;%\n        tidyr::fill(\"atBatIndex\":\"matchup.splits.menOnBase\", .direction = \"up\") %&gt;%\n        dplyr::mutate(\n          game_pk = game_pk,\n          game_date = substr(payload$gameData$datetime$dateTime, 1, 10)) %&gt;%\n        dplyr::select(\"game_pk\", \"game_date\", tidyr::everything())\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::mutate(\n          matchup.batter.fullName = factor(.data$matchup.batter.fullName),\n          matchup.pitcher.fullName = factor(.data$matchup.pitcher.fullName),\n          atBatIndex = factor(.data$atBatIndex)\n          # batted.ball.result = case_when(!result.event %in% c(\n          #   \"Single\", \"Double\", \"Triple\", \"Home Run\") ~ \"Out/Other\",\n          #   TRUE ~ result.event),\n          # batted.ball.result = factor(batted.ball.result,\n          #                             levels = c(\"Single\", \"Double\", \"Triple\", \"Home Run\", \"Out/Other\"))\n        ) %&gt;%\n        dplyr::mutate(\n          home_team = home_team,\n          home_level_id = home_level$id,\n          home_level_name = home_level$name,\n          home_parentOrg_id = payload$gameData$teams$home$parentOrgId,\n          home_parentOrg_name = payload$gameData$teams$home$parentOrgName,\n          home_league_id = home_league$id,\n          home_league_name = home_league$name,\n          away_team = away_team,\n          away_level_id = away_level$id,\n          away_level_name = away_level$name,\n          away_parentOrg_id = payload$gameData$teams$away$parentOrgId,\n          away_parentOrg_name = payload$gameData$teams$away$parentOrgName,\n          away_league_id = away_league$id,\n          away_league_name = away_league$name,\n          batting_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                       .data$home_team,\n                                       .data$away_team)),\n          fielding_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                        .data$away_team,\n                                        .data$home_team)))\n      pbp &lt;- pbp %&gt;%\n        dplyr::arrange(desc(.data$atBatIndex), desc(.data$pitchNumber))\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::group_by(.data$atBatIndex) %&gt;%\n        dplyr::mutate(\n          last.pitch.of.ab =  ifelse(.data$pitchNumber == max(.data$pitchNumber), \"true\", \"false\"),\n          last.pitch.of.ab = factor(.data$last.pitch.of.ab)) %&gt;%\n        dplyr::ungroup()\n      \n      pbp &lt;- dplyr::bind_rows(baseballr::stats_api_live_empty_df, pbp)\n      \n      check_home_level &lt;- pbp %&gt;%\n        dplyr::distinct(.data$home_level_id) %&gt;%\n        dplyr::pull()\n      \n      # this will need to be updated in the future to properly estimate X,Z coordinates at the minor league level\n      \n      # if(check_home_level != 1) {\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.pX_est = predict(x_model, pbp),\n      #                   pitchData.coordinates.pZ_est = predict(y_model, pbp))\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      # }\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::rename(\n          \"count.balls.start\" = \"count.balls.x\",\n          \"count.strikes.start\" = \"count.strikes.x\",\n          \"count.outs.start\" = \"count.outs.x\",\n          \"count.balls.end\" = \"count.balls.y\",\n          \"count.strikes.end\" = \"count.strikes.y\",\n          \"count.outs.end\" = \"count.outs.y\") %&gt;%\n        make_baseballr_data(\"MLB Play-by-Play data from MLB.com\",Sys.time())\n    },\n    error = function(e) {\n      message(glue::glue(\"{Sys.time()}: Invalid arguments provided\"))\n    },\n    finally = {\n    }\n  ) \n  return(pbp)\n}\n\n#' @rdname get_pbp_mlb\n#' @title **(legacy) Acquire pitch-by-pitch data for Major and Minor League games**\n#' @inheritParams mlb_pbp\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#' @keywords legacy\n#' @export\n# get_pbp_mlb &lt;- mlb_pbp\n\n\n\n\nExample\nHere is an example using the mlb_pbp function.\n\n\nCode\nexample &lt;- (mlb_pbp(575156))\nhead(example)\n\n\n2023-10-12 13:40:46.684707: Invalid arguments provided\n\n\n\n\nA tibble: 6 x 146\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nabout.isTopInning\nmatchup.postOnFirst.id\nmatchup.postOnFirst.fullName\nmatchup.postOnFirst.link\nmatchup.postOnSecond.id\nmatchup.postOnSecond.fullName\nmatchup.postOnSecond.link\nmatchup.postOnThird.id\nmatchup.postOnThird.fullName\nmatchup.postOnThird.link\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n575156\n2019-06-01\n5\n2019-06-01T15:38:42.000Z\n2019-06-01T19:38:07.354Z\nTRUE\npitch\n05751566-0846-0063-000c-f08cd117d70a\n6\nIn play, out(s)\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n4\n2019-06-01T15:38:19.000Z\n2019-06-01T15:38:42.000Z\nTRUE\npitch\n05751566-0846-0053-000c-f08cd117d70a\n5\nFoul\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n3\n2019-06-01T15:38:02.000Z\n2019-06-01T15:38:19.000Z\nTRUE\npitch\n05751566-0846-0043-000c-f08cd117d70a\n4\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n2\n2019-06-01T15:37:45.000Z\n2019-06-01T15:38:02.000Z\nTRUE\npitch\n05751566-0846-0033-000c-f08cd117d70a\n3\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n1\n2019-06-01T15:37:31.000Z\n2019-06-01T15:37:45.000Z\nTRUE\npitch\n05751566-0846-0023-000c-f08cd117d70a\n2\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n0\n2019-06-01T15:37:15.000Z\n2019-06-01T15:37:31.000Z\nTRUE\npitch\n05751566-0846-0013-000c-f08cd117d70a\n1\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAcquiring Data\nI will pull more data eventually, but for now I am scraping two series of games from the 2023 season.\n\n\nCode\nlibrary(baseballr)\n\n\nThe below code allows me to find the correct game_pk values that I can then use to pull play-by-play data.\n\n\nCode\n#mlb_game_pks(\"2023-06-25\")\n# mlb_game_pks(\"2023-06-24\")\n# mlb_game_pks(\"2023-06-23\")\n\n\n\n\nCode\n#game_pk values\n\n#diamondbacks/giants - 717641, 717639, 717612\n\n#mariners/orioles - 717651, 717628, 717627\n\n\n\n\nCode\nx &lt;- c(717641, 717639, 717612, 717651, 717628, 717627)\nresult &lt;- lapply(x, mlb_pbp)\ncombined_tibble &lt;- bind_rows(result)\n# Save the data to a CSV file\nwrite.csv(combined_tibble, file = \"./data/raw_data/baseballr_six_games.csv\", row.names = FALSE)\nhead(combined_tibble)\n\n\n\nA baseballr_data: 6 x 160\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nncaahoopR\n“ncaahoopR” is an R package tailored for NCAA Basketball Play-by-Play Data analysis. It excels at retrieving play-by-play data in a tidy format. For the purposes of this project, I will start by scraping play-by-play data for the Villanova Wildcats Men’s Basketball team from both the 2019-20 and 2021-22 seasons (the 2020-21 was shortened due to COVID-19).\n\n\nCode\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lbenz730/ncaahoopR\")\nlibrary(ncaahoopR)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/lb/dk54cbx965z7nj61zps2fzr00000gn/T//RtmpqXinFj/downloaded_packages\n\n\nSkipping install of 'ncaahoopR' from a github remote, the SHA1 (9bd97fec) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n\n\nCode\nVillanova1920 &lt;- get_pbp(\"Villanova\", \"2019-20\")\nVillanova2122 &lt;- get_pbp(\"Villanova\", \"2021-22\")\nwrite.csv(Villanova1920, file = \"./data/raw_data/villanova1920.csv\", row.names = FALSE)\nwrite.csv(Villanova2122, file = \"./data/raw_data/villanova2122.csv\", row.names = FALSE)\n\n\n\n\nCode\nhead(Villanova1920)\n\n\n\nA data.frame: 6 x 39\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\nshot_y\nshot_team\nshot_outcome\nshooter\nassist\nthree_pt\nfree_throw\npossession_before\npossession_after\nwrong_time\n\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\n401169778\n2019-11-05\nVillanova\nArmy\n1\n1\n19:37\n2377\n2377\nSaddiq Bey made Jumper.\n...\nNA\nVillanova\nmade\nSaddiq Bey\nNA\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n2\n401169778\n2019-11-05\nVillanova\nArmy\n2\n1\n19:16\n2356\n2356\nTucker Blackwell made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nTucker Blackwell\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n3\n401169778\n2019-11-05\nVillanova\nArmy\n3\n1\n19:01\n2341\n2341\nFoul on Jermaine Samuels.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n4\n401169778\n2019-11-05\nVillanova\nArmy\n4\n1\n19:01\n2341\n2341\nJermaine Samuels Turnover.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n5\n401169778\n2019-11-05\nVillanova\nArmy\n5\n1\n18:42\n2322\n2322\nMatt Wilson made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nMatt Wilson\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n6\n401169778\n2019-11-05\nVillanova\nArmy\n6\n1\n18:31\n2311\n2311\nJeremiah Robinson-Earl made Jumper. Assisted by Justin Moore.\n...\nNA\nVillanova\nmade\nJeremiah Robinson-Earl\nJustin Moore\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n\n\n\n\n\nReddit\n\n\nCode\n#install.packages(\"RedditExtractoR\") #only executable in Rstudio\nlibrary(RedditExtractoR)\n\n\nsubreddit &lt;- \"baseball\"\n\n# Get posts from the r/baseball subreddit\nstreaks &lt;- find_thread_urls(keywords = \"streak\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nhot &lt;- find_thread_urls(keywords = \"hot\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nwrite.csv(streaks, file = \"./streaks.csv\", row.names = FALSE)\nwrite.csv(hot, file = \"./hot.csv\", row.names = FALSE)\n\n\nBelow you can see the first few rows of both the “hot” and “streaks” csv files.\n \n\n\nNews API\n\n\nCode\nAPI_KEY='05d7ae99b5b7455191c97c2c5c3a1f9b'\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\ndef string_cleaner(input_string):\n    try: \n        out=re.sub(r\"\"\"\n                    [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’.]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n\n\nCode\n%%capture\n\nTOPIC = 'hot streak sports'\n\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\n#print(\"AVAILABLE KEYS:\")\n#print(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        # if(key=='description'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        # if(key=='content'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\ndf1 = pd.DataFrame(cleaned_data)\ndf1.to_csv('./data/raw_data/newsapi.csv', index=False) #,index_label=['title','src','author','date','description'])\n\n\nBelow you can see the first few rows of the newsapi.csv file:  \n\n\nIndividual Player Data\nI also want to eventually scrape specific data from fangraphs. For now, I was able to download a few tables that had game data for Aaron Judge and then merge them together. Below are screen shots of the initial csv file.   \n\n\nExtra Joke\nHow much data can be stored in a glacier? A frostbite!"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "more files!"
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM",
    "section": "",
    "text": "Why did the arm apply for a job? Because it wanted to lend a helping hand!"
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "What did the tree do when the bank closed?\nIt started its own branch."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Why did the linear regression model break up with the logistic regression model? Because it wanted a more ‘linear’ relationship!"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress"
  },
  {
    "objectID": "about.html#my-story",
    "href": "about.html#my-story",
    "title": "About me",
    "section": "",
    "text": "I am a graduate student at Georgetown University studying Data Science and Analytics. I graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics with a PPE (Politics, Philosophy, & Economics) Concentration and a Minor in Political Science. While a student at Villanova, I studied abroad at the University of Cambridge. During my senior year at Villanova, I had the opportunity to start my own business along with my sisters - we are triplets! Our business, Top Shelf Designs LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. I am extremely excited to continue my education and pursue a masters degree in data science."
  },
  {
    "objectID": "about.html#website-jokes",
    "href": "about.html#website-jokes",
    "title": "About me",
    "section": "Website Jokes",
    "text": "Website Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki"
  },
  {
    "objectID": "about.html#assorted-jokes",
    "href": "about.html#assorted-jokes",
    "title": "About me",
    "section": "Assorted Jokes",
    "text": "Assorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!"
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "introduction.html#why-this-is-important",
    "href": "introduction.html#why-this-is-important",
    "title": "Introduction",
    "section": "Why this is important",
    "text": "Why this is important"
  },
  {
    "objectID": "introduction.html#why-the-reader-should-continue",
    "href": "introduction.html#why-the-reader-should-continue",
    "title": "Introduction",
    "section": "Why the reader should continue",
    "text": "Why the reader should continue"
  },
  {
    "objectID": "introduction.html#what-work-has-been-done-in-the-past",
    "href": "introduction.html#what-work-has-been-done-in-the-past",
    "title": "Introduction",
    "section": "what work has been done in the past",
    "text": "what work has been done in the past"
  },
  {
    "objectID": "introduction.html#what-are-the-different-points-of-viewsinterpretations-in-the-literature",
    "href": "introduction.html#what-are-the-different-points-of-viewsinterpretations-in-the-literature",
    "title": "Introduction",
    "section": "what are the different points of views/interpretations in the literature",
    "text": "what are the different points of views/interpretations in the literature"
  },
  {
    "objectID": "introduction.html#what-i-am-exploring",
    "href": "introduction.html#what-i-am-exploring",
    "title": "Introduction",
    "section": "what I am exploring",
    "text": "what I am exploring"
  },
  {
    "objectID": "introduction.html#questions",
    "href": "introduction.html#questions",
    "title": "Introduction",
    "section": "10 Questions",
    "text": "10 Questions\n\nWhat data is available for my topic?\nWhat does current literature on the topic argue?\nHow can I build off of the current research and approach the topic in a novel way?\nShould I limit the scope of my topic to sports or can I expand past that?\nHow should I define success? (the concept of the hot hand is that success breeds success)\nWhat is the best way to visualize the data?\nDo athletes and/or the public believe in this phenomenon?\nIs there any evidence that the hot hand exists?\nDoes the hot hand impact strategy of a game? Should it impact strategy?\nIf the hot hand does exist, in what sport (or area outside of sports) is there the most evidence in support of the phenomenon?"
  },
  {
    "objectID": "introduction.html#goals-and-hypothesis",
    "href": "introduction.html#goals-and-hypothesis",
    "title": "Introduction",
    "section": "Goals and Hypothesis",
    "text": "Goals and Hypothesis\n\nI would love to find some evidence that the hot hand exists"
  },
  {
    "objectID": "introduction.html#prior-research",
    "href": "introduction.html#prior-research",
    "title": "Introduction",
    "section": "Prior Research",
    "text": "Prior Research\nThe initial investigation into this topic Gilovich, Vallone, and Tversky (1985) was published in 1985. It analyzed assorted data, including professional basketball field goal data from the 1980-1981 season, professional basketball free-throw data from the 1980-1982 seasons, and a controlled shooting experiment. While the study found that over 91% of fans agreed that a player has a better chance of making a shot after having just made his last two or three shots than he does after having just missed his last two or three shots, none of their data showed any evidence of this phenomenon. Instead, Gilovich, Vallone, and Tversky (1985) argued that there is a wide-spread misperception of random sequences:\n\nPeople’s intuitive conceptions of randomness depart systematically from the laws of chance. It appears that people expect the essential characteristics of a chance process to be represented not only globally in the entire sequences, but also locally, in each of its parts. For instance, people expect even short sequences of heads and tails to reflect the fairness of a coin and contain roughly 50% heads and 50% tails. This conception of chance has been described as a ‘belief in the law of small numbers’ according to which the law of large numbers applies to small samples as well. A locally representative sequence, however, deviates systematically from chance expectation: It contains too many alternations and not enough long runs.\n\nBar-Eli, Avugos, and Raab (2006) published a review and critique of 20 years of “hot hand” research in 2006. This paper reviewed the Gilovich, Vallone, and Tversky (1985) study in addition to subsequent research that used data from various sports including basketball, baseball, golf, darts, tennis, bowling, and more. Baseball and basketball studies dominate the literature on this subject, yet the strongest support for the “hot hand” can be found in more individual sports such as horseshoe pitching and tennis.\n\nDemonstrations of hot hands per se are rare and often weak, due to various reasons: using and unrealistic model and questionable data, setting questionable definitions for hot and cold players, relating streakiness to difficulty of task, combining and analyzing data of all players a as a group, and other constraints related to the kind of sport studied.\n\nIn the end, this study found that the question remains unresolved.  MORE RESEARCH TO COME"
  },
  {
    "objectID": "introduction.html#the-debate",
    "href": "introduction.html#the-debate",
    "title": "Introduction",
    "section": "The Debate",
    "text": "The Debate\nMORE TO BE ADDED"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilovich, Vallone, and Tversky (1985)↩︎\nBar-Eli, Avugos, and Raab (2006)↩︎"
  }
]