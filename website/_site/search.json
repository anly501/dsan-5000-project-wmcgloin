[
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction\nOn this page, I’ll employ three clustering techniques to analyze Villanova’s 2021-22 NCAA shot data (the same data from the dimensionality-reduction tab), considering all six features: lag1 (previous shot), shot_value, field_goal_percentage, game_num, home_crowd, and score_diff. The primary objective is to uncover patterns within the dataset through k-means, DBSCAN, and hierarchical clustering. Each step of the process will be explained in a straightfowrad manner with interpretations of the results.\n\nImport libraries and load the dataset\n\n\nCode\n# import the necessary packages...\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# read in the feature dataset\ndf = pd.read_csv('./data/modified_data/nova_features.csv')\n\n\n\n\n\nKMeans\n\nTheory\nK-Means clustering, a widely recognized algorithm, is valued for its simplicity and effectiveness, making it particularly appealing in various applications. This technique involves grouping data points into ‘k’ clusters, where ‘k’ is a user-specified parameter. The algorithm starts by assigning random points to these clusters, with centroids acting as their centers. Distances, usually calculated using Euclidean distance, guide the process. Iteratively, points shift between clusters, and new centroids emerge through Lloyd’s algorithm until no better cluster assignments are feasible.  K-Means’ core lies in optimizing the sum of squared distances between data points and their assigned cluster mean. The choice of ‘k’ dictates the number of centroids, which represent cluster centers. The algorithm strategically redistributes points to minimize the in-cluster sum of squares. Its simplicity and widespread usage make it a fundamental tool in unsupervised machine learning, showcasing its ability to uncover inherent patterns in data.  In a broader context, K-Means clustering serves as a technique to group data points based on their similarity to an average grouping. Distance metrics, such as Euclidean or Manhattan distance, play a pivotal role, and normalizing input data becomes crucial for robust performance. Centroids, serving as the algorithm’s starting point, evolve through successive iterations, refining cluster assignments and centroids’ positions. The algorithm’s convergence reveals meaningful clusters, transforming data chaos into structured insights. Implementation through sklearn’s KMeans algorithm enhances efficiency, and feature selection enables practitioners to customize the clustering process to their data’s nuances.\n\n\nImplementation\n\n\nCode\n# import relevent libraries for clustering. we will use KMeans, AgglomerativeClustering, MeanShift, Birch, and DBSCAN\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.cluster import MeanShift, Birch\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport random\n\n\nThe below code performs K-Means clustering for different numbers of clusters (ranging from 2 to 10). For each cluster number, it calculates and stores the distortion, which measures the average Euclidean distance between each data point and its assigned cluster center. Inertia is also computed and stored, representing the sum of squared distances of data points to their closest cluster center. Additionally, the silhouette score is determined and recorded, offering insights into how well-defined and separated the clusters are. The results, encompassing these crucial metrics—distortion, inertia, and silhouette score—are then organized into a DataFrame for detailed analysis and printed. This is in an attempt to identify the optimal number of clusters based on a comprehensive evaluation of these key clustering indicators.\n\n\nCode\n# Create empty lists to store the results\nclus = []\ndistortions = []\ninertias = []\nsilhouette_scores = []\n\n# Loop through the range of clusters\nfor i in range(2, 11):  # Silhouette score is not defined for a single cluster\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(df)  \n    clus.append(i)\n    centers = kmeans.cluster_centers_\n    distortions.append(sum(np.min(pairwise_distances(df, centers, metric='euclidean'), axis=1)) / df.shape[0])\n    inertias.append(kmeans.inertia_)\n    \n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(df, kmeans.labels_))\n\n# Create a DataFrame from the lists\nresults = pd.DataFrame({'Cluster': clus, 'Distortion': distortions, 'Inertia': inertias, 'Silhouette Score': silhouette_scores})\n\nprint(results)\n\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n   Cluster  Distortion        Inertia  Silhouette Score\n0        2   10.622510  447410.233953          0.372238\n1        3    8.828917  272970.775541          0.408870\n2        4    7.974259  215311.617146          0.390174\n3        5    6.985968  163526.599625          0.380241\n4        6    6.394550  134984.307974          0.368484\n5        7    5.753101  113388.811614          0.384348\n6        8    5.309537   94219.154891          0.403127\n7        9    5.020084   83855.750340          0.403238\n8       10    4.698719   75156.752577          0.386414\n\n\nAmong the tested cluster counts (2 to 10), the silhouette score, a measure of cluster quality, is highest when there are three clusters. A silhouette score close to 1 indicates well-separated clusters. In this case, the silhouette score peaks at three clusters, suggesting that the data is most naturally organized into this number of distinct groups. This finding signifies a meaningful and clear grouping in the data, enabling better understanding and interpretation of underlying patterns.  The subsequent block of code generates three plots: one for distortion, one for inertia, and one for the silhouette score. Each plot has the number of clusters on the x-axis and the corresponding metric on the y-axis. These visualizations help interpret the results, allowing for the identification of trends and patterns.\n\n\nCode\n# Create subplots with 1 row and 3 columns\nfig, ax = plt.subplots(1, 3, figsize=(18, 4))\n\n# Plot Distortion\nax[0].plot(results['Cluster'], results['Distortion'], marker='o')\nax[0].set_title('Distortion')\nax[0].set_xlabel('Cluster')\nax[0].set_ylabel('Distortion')\n\n# Plot Inertia\nax[1].plot(results['Cluster'], results['Inertia'], marker='o')\nax[1].set_title('Inertia')\nax[1].set_xlabel('Cluster')\nax[1].set_ylabel('Inertia')\n\n# Plot Silhouette Score\nax[2].plot(results['Cluster'], results['Silhouette Score'], marker='o')\nax[2].set_title('Silhouette Score')\nax[2].set_xlabel('Cluster')\nax[2].set_ylabel('Silhouette Score')\n\n# Display the side-by-side plots\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe analysis of the graphs and results indicates that opting for 3 clusters is appropriate. The elbow and silhouette score methods suggest that this choice effectively captures meaningful patterns within the data. Having 3 clusters strikes a balance between simplicity and preserving relevant information. The subsequent 3D scatter plot visually represents the clustered data points in a three-dimensional space, offering a comprehensive view of the results.\n\n\nCode\npca_result = pd.read_csv('./data/modified_data/nova_pca.csv')\n\nimport plotly.express as px\n\n# Assuming pca_result has columns '0', '1', and '2'\nfig = px.scatter_3d(pca_result, x='0', y='1', z='2', color=labels, symbol=labels, opacity=0.7,\n                    size_max=10, title='3D Scatter Plot of PC1, PC2, and PC3 with Cluster Labels',\n                    labels={'0': 'Principal Component 1 (PC1)',\n                            '1': 'Principal Component 2 (PC2)',\n                            '2': 'Principal Component 3 (PC3)',\n                            'color': 'Cluster'},\n                    )\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nDBSCAN\n\nTheory\nDBSCAN, or Density-Based Spatial Clustering of Applications with Noise, is a robust clustering algorithm that distinguishes clusters based on the density of data points, incorporating both distance metrics and a minimum number of points. Unlike K-Means, DBSCAN doesn’t require users to specify the number of clusters (‘k’) in advance; instead, it dynamically identifies clusters by expanding neighborhoods around data points.  The algorithm starts by randomly selecting and expanding a neighborhood around a data point. If the density within this neighborhood is sufficient, a cluster is formed, and the process iterates until no more data points can be added. Outliers are identified as points in low-density regions. DBSCAN’s strength lies in its ability to uncover clusters of arbitrary shapes and sizes, making it particularly valuable for datasets where the number of clusters is unknown.  DBSCAN requires two key parameters: epsilon (ε) and minimum samples. Epsilon defines the maximum distance between two points for them to be considered in the same cluster, while minimum samples specify the minimum number of points required in each cluster. Experimentation is often needed to find optimal parameters, as the algorithm’s outcome is sensitive to these choices.  In summary, DBSCAN offers a unique approach to clustering, emphasizing data density over distances to centroids. Its flexibility in identifying clusters of varying shapes and the ability to mark outliers enhances its effectiveness in exploring complex datasets. Implementation through sklearn’s DBSCAN module provides a practical means to apply this algorithm to diverse datasets.\n\n\nImplementation\nThe below code performs an exhaustive search for optimal parameters (epsilon and minimum samples) for the DBSCAN algorithm. It calculates silhouette scores for different combinations of epsilon values (z1) and minimum sample sizes (z2), aiming to identify the best configuration that yields the highest silhouette score and the corresponding number of clusters. The resulting dataframe, df1, is then printed and visualized with a line plot.\n\n\nCode\nbest_scores = []\neps = []\nclus = []\nz1 = [i / 10 for i in range(5, 20)]\nz2 = range(2, 10) # explain why 2 to 10 or just do 1 to 10 but then u have to fix smth in the code if i dont remember wrong. i suggest explaining is a common assumption to do here\n\nfor i in z1:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in z2:\n        model = DBSCAN(eps=i, min_samples=j)\n        predics = model.fit_predict(df)\n        num_clusters = len(pd.Series(predics).unique())\n        if num_clusters &gt; 1:\n            score = silhouette_score(df, predics)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    clus.append(best_cluster)\n    eps.append(best_eps)\n\ndf1 = pd.DataFrame({'Epsilons': eps, 'Best_Clusters': clus, 'Best_Silhouette': best_scores})\nprint(df1.sort_values(by=\"Best_Silhouette\", ascending=False))\nsns.lineplot(data=df1, x='Best_Clusters',y='Best_Silhouette')\nplt.show()\n\n\n    Epsilons  Best_Clusters  Best_Silhouette\n0        0.5            595         0.065783\n1        0.6            595         0.065783\n2        0.7            595         0.065783\n3        0.8            595         0.065783\n4        0.9            595         0.065783\n5        1.0            456         0.020257\n10       1.5             33        -0.029439\n11       1.6             33        -0.029439\n12       1.7             33        -0.029439\n6        1.1            216        -0.040136\n7        1.2            216        -0.040136\n8        1.3            216        -0.040136\n9        1.4            216        -0.040136\n13       1.8             22        -0.117656\n14       1.9             22        -0.117656\n\n\n\n\n\nThe results suggest that varying epsilon values from 0.5 to 0.9 consistently yield a high number of clusters (595) with a low silhouette score (0.065783). This pattern persists, indicating that DBSCAN struggles to identify distinct clusters, possibly due to the nature of the data. The diminishing silhouette scores as epsilon increases, coupled with the large number of clusters, may imply that this clustering method is not well-suited for the dataset. To validate this observation, hierarchical clustering and visualization through T-SNE plots will be explored for further confirmation.\n\n\n\nHierarchical Clustering (Agglomerative Clustering)\n\nTheory\nHierarchical Clustering, also known as Agglomerative Clustering, is a versatile algorithm that builds a hierarchy of clusters without the need to predefine the number of clusters. It treats each data point individually and progressively merges the closest clusters until forming a single cluster, using methods like Ward’s method to calculate distances. The resulting linkage matrix constructs a dendrogram, enabling visualization of clustering hierarchy and facilitating the choice of cluster count. Normalizing input data is crucial for meaningful results, and the choice of linkage method, such as ward linkage, influences cluster formation.  Hierarchical Clustering focuses on finding clusters based on distance, repeatedly finding the two closest points and forming clusters until all points are assigned. The algorithm is sensitive to distance, requiring multiple runs with different distance values, and scaling the dataset to mitigate outlier effects. This method’s adaptability and visualization through a dendrogram make it valuable for exploring data structures.\n\n\nImplementation\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward') #chose 3 as that is the number of species. We could have changed it.\nlabels = hierarchical_cluster.fit_predict(df)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\nCluster Labels total:\n[0, 1, 2]\n\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning:\n\nAttribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n\n\n\nLet’s generates a dendrogram for Agglomerative Clustering, visualizing the hierarchical linkage between data points.\n\n\nCode\n# create linkage for agglomerative clustering, and the dendrogram for the linkage. Suggest the optimal number of clusters based on the dendrogram.\nlinkage_matrix = linkage(df, method='ward')\n\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, orientation='top', labels=labels, distance_sort='ascending', show_leaf_counts=True)\nplt.show()\n\n\n\n\n\nThe below code defines a function that performs hierarchical clustering on input data, varying the number of clusters. It calculates silhouette scores for each clustering iteration and outputs the optimal number of clusters that maximizes the silhouette score, in addition to plotting a graph showing how the silhouette score changes with different cluster numbers. The last lines of code apply this function to the data.\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn.cluster\n\ndef maximize_silhouette(X, algo=\"ag\", nmax=None, i_plot=False):\n    # PARAM\n    i_print = False\n\n    # FORCE CONTIGUOUS\n    X = np.ascontiguousarray(X)\n\n    # LOOP OVER HYPER-PARAM\n    params = []\n    sil_scores = []\n    sil_max = -10\n\n    for param in range(2, nmax + 1):\n        if algo == \"ag\":\n            model = AgglomerativeClustering(n_clusters=param).fit(X)\n            labels = model.labels_\n            \n            try:\n                sil_scores.append(silhouette_score(X, labels))\n                params.append(param)\n            except ValueError:\n                continue\n\n            if i_print:\n                print(param, sil_scores[-1])\n\n            if sil_scores[-1] &gt; sil_max:\n                opt_param = param\n                sil_max = sil_scores[-1]\n                opt_labels = labels\n\n    print(\"Maximum Silhouette score =\", sil_max)\n    print(\"OPTIMAL CLUSTERS (btwn 2-10) =\", opt_param)\n\n    if i_plot:\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")\n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n# Example usage:\nopt_labels = maximize_silhouette(df, algo=\"ag\", nmax=10, i_plot=True)\n\n\nMaximum Silhouette score = 0.3971383731982046\nOPTIMAL CLUSTERS (btwn 2-10) = 3\n\n\n\n\n\nThe results from the code suggest that, within the considered range of 2 to 10 clusters, the maximum silhouette score for agglomerative clustering is achieved at 3 clusters, indicating it as the optimal number of clusters. This aligns with the observed trend in K-Means clustering, where both methods highlight 3 clusters as optimal based on the silhouette score.\n\n\n\nConclusions\nThe analysis using K-Means and hierarchical clustering both implied the presence of three clusters within the dataset, providing a consistent pattern. However, DBSCAN did not seem well-suited for this dataset, suggesting varying performance across clustering methods. This indicates the potential for meaningful clustering, and further exploration in the Dimensionality Reduction tab may unveil clearer insights into the distinctive patterns within the data.\n\n\nExtra Joke\nMovie Pitch: It’s a movie about high school girls trying to figure out what clique they belong in. They move from clique to clique and eventually stop when they minimize their differences. It’s called K-Means girls."
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nBrief Introduction to EDA\nExploratory Data Analysis (EDA) is a fundamental starting point in data analysis, helping us grasp the data’s characteristics, patterns, and possible outliers. It provides essential insights for making informed modeling decisions.\nBy analyzing the below data, I hope to gain an understanding of overall trends that can aid in refining my hypothesis and inform the construction of a more accurate model.\n\n\nncaahoopR\n\n2021-22 season\n\n# let's read in the data and load in relevant libraries\nnova2122 &lt;- read.csv('./data/modified_data/nova2122.csv')\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova2122 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player (aka field goal percentage)\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\nA tibble: 12 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nJustin Moore\n574\n0.4721254\n\n\nCollin Gillespie\n549\n0.5336976\n\n\nJermaine Samuels\n439\n0.5535308\n\n\nCaleb Daniels\n356\n0.5056180\n\n\nEric Dixon\n340\n0.5794118\n\n\nBrandon Slater\n308\n0.5876623\n\n\nChris Arcidiacono\n69\n0.4927536\n\n\nJordan Longino\n57\n0.4210526\n\n\nBryan Antoine\n46\n0.3043478\n\n\nTrey Patterson\n12\n0.3333333\n\n\nDhamir Cosby-Roundtree\n8\n0.5000000\n\n\nNnanna Njoku\n6\n0.5000000\n\n\n\n\n\n\n\n\nThe table displayed above, arranged in descending order based on the number of shots attempted, presents the field goal percentages of Villanova Men’s Basketball (MBB) players for the 2021-22 season. The accompanying ggplot-generated graph visually represents the count of both missed and successful shots for each player. This visualization emphasizes the significant variation in the number of shots taken by different players, which could offer richer data and potential insights for subsequent modeling.\n\n# Create lag variables within each shooter and game_id group\nnova2122 &lt;- nova2122 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\nwrite.csv(nova2122, file = \"./data/modified_data/nova2122_updated.csv\", row.names = FALSE)\n\n# View the updated data with lag variables\nhead(nova2122)\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401365747\n4\n1\nJustin Moore\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n7\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n11\n1\nClifton Moore\nmissed\nLa Salle\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n13\n1\nEric Dixon\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n16\n1\nCollin Gillespie\nmade\nVillanova\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401365747\n18\n1\nEric Dixon\nmade\nVillanova\n1\n1\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova2122[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThe correlation heatmap presented above carries an intriguing insight. Although it may not reveal strong correlations between “shot_outcome_numeric” and the lag variables individually, a notable descending trend emerges from “lag1” to “lag6.” This observation could provide valuable insight, suggesting that a player’s shot outcome is more likely to be influenced by their immediate prior shot, rather than a shot taken several attempts ago.\n\n\n2019-20 season\nTo assess potential disparities, let’s replicate the same analysis for the 2019-20 season and compare the resulting graphs and tables with those generated earlier. This comparative approach will help us identify any noticeable differences and potential insights.\n\n#let's read in the data\nnova1920 &lt;- read.csv('./data/modified_data/nova1920.csv')\n\n\n# Create a ggplot for shot outcome distribution by villanova players\nnova_players &lt;- nova1920 %&gt;% filter(shooter_team == \"Villanova\")\n\nggplot(nova_players, aes(x = shooter, fill = shot_outcome)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Shot Outcome Distribution by Player\", x = \"Player\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = c(\"missed\" = \"#3464e9\", \"made\" = \"#4de9e6\")) +\n  guides(fill = guide_legend(title = \"Shot Outcome\"))\n\n# Calculate the mean of shot_outcome for each player\nmean_and_count_data &lt;- nova_players %&gt;%\n  group_by(shooter) %&gt;%\n   summarize(\n    shots = n(),\n    field_goal_percentage = mean(ifelse(shot_outcome_numeric == -1, 0, shot_outcome_numeric), na.rm = TRUE)\n  ) %&gt;%\n  arrange(-shots) \n\nmean_and_count_data\n\n\nA tibble: 10 x 3\n\n\nshooter\nshots\nfield_goal_percentage\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nCollin Gillespie\n491\n0.4969450\n\n\nSaddiq Bey\n458\n0.5349345\n\n\nJustin Moore\n355\n0.4647887\n\n\nJeremiah Robinson-Earl\n347\n0.5533141\n\n\nJermaine Samuels\n334\n0.5419162\n\n\nCole Swider\n171\n0.4561404\n\n\nBrandon Slater\n68\n0.3823529\n\n\nDhamir Cosby-Roundtree\n36\n0.6666667\n\n\nBryan Antoine\n25\n0.3600000\n\n\nChris Arcidiacono\n6\n0.1666667\n\n\n\n\n\n\n\n\n\n# Create lag variables within each shooter and game_id group\nnova1920 &lt;- nova1920 %&gt;%\n  arrange(shooter, game_id, play_id) %&gt;%  # Arrange the data by shooter, game_id, and play_id\n  group_by(shooter, game_id) %&gt;%\n  mutate(\n    lag1 = lag(shot_outcome_numeric, order_by = play_id),\n    lag2 = lag(shot_outcome_numeric, order_by = play_id, n = 2),\n    lag3 = lag(shot_outcome_numeric, order_by = play_id, n = 3),\n    lag4 = lag(shot_outcome_numeric, order_by = play_id, n = 4),\n    lag5 = lag(shot_outcome_numeric, order_by = play_id, n = 5),\n    lag6 = lag(shot_outcome_numeric, order_by = play_id, n = 6)) %&gt;%\n    ungroup() %&gt;%\n    arrange(game_id, play_id)\n\n# View the updated data with lag variables\nhead(nova1920)\n\n\nA tibble: 6 x 15\n\n\ngame_id\nplay_id\nhalf\nshooter\nshot_outcome\nshooter_team\nshot_outcome_numeric\nshot_sequence\nprevious_shots\nlag1\nlag2\nlag3\nlag4\nlag5\nlag6\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n401166061\n2\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n4\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n6\n1\nSaddiq Bey\nmissed\nVillanova\n-1\n-2\n-1\n-1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n8\n1\nDuane Washington Jr.\nmade\nOhio State\n1\n2\n1\n1\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n9\n1\nCollin Gillespie\nmissed\nVillanova\n-1\n-1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n401166061\n11\n1\nCJ Walker\nmade\nOhio State\n1\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n# Calculate the correlation matrix\ncor_matrix &lt;- cor(nova1920[, c(\"shot_outcome_numeric\", \"lag1\", \"lag2\", \"lag3\", \"lag4\", \"lag5\", \"lag6\")], use = \"pairwise.complete.obs\")\n\nlibrary(reshape2)\ncor_data &lt;- melt(cor_matrix)\n\nggplot(cor_data, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"#f69696\", high = \"#9a1717\", midpoint = 0) +\n  labs(title = \"Correlation Heatmap\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n\n\n\n\n\nWe can observe that, despite some player variations, most of the graphs maintain a substantial degree of consistency, which further supports the earlier findings.\n\n\n\nNews API\n\n#import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnews_api = pd.read_csv('./data/modified_data/sentiment_scores_with_titles.csv')\n\n\n#what does this data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\n\n\n\n\n\n\n\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnews_api['cleaned_text'] = news_api['Description'].apply(preprocess_text)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\n\nnews_api.to_csv('./data/modified_data/news_api_naive.csv', index=False)\n\n#what does the new column of data look like?\nnews_api.head()\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\n\n# Import more necessary libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Define the function to plot the word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(10, 6))\n    # Display the word cloud\n    plt.imshow(wordcloud)\n    # Remove axis details\n    plt.axis(\"off\")\n    # Show the word cloud\n    plt.show()\n\n# Define the function to generate and display the word cloud\ndef generate_word_cloud(my_text):\n    # Generate the word cloud\n    wordcloud = WordCloud(\n        width=800,\n        height=400,\n        background_color='white',\n        colormap='viridis',\n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n    # Plot and display the word cloud\n    plot_cloud(wordcloud)\n\n# let's pass the 'cleaned_text' column to the function\ngenerate_word_cloud(' '.join(news_api['cleaned_text']))\n\n\n\n\nWithin the word cloud, generated from articles collected through the news API, notable recurring terms include “win,” “losing,” “victory,” “winning,” “matchup,” and others. These terms hold the potential to offer insights into the articles’ context and serve as valuable cues for conducting sentiment analysis.\n\n\nIndividual Player Data\n\n#let's import some libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\naaronjudge = pd.read_csv('./data/modified_data/aaronjudge.csv')\n\n\n#let's learn about the data\naaronjudge.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 36 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Date              100 non-null    object \n 1   Team              100 non-null    object \n 2   Opp               100 non-null    object \n 3   BO                100 non-null    int64  \n 4   Pos               100 non-null    object \n 5   PA                100 non-null    float64\n 6   H                 100 non-null    int64  \n 7   2B                100 non-null    int64  \n 8   3B                100 non-null    int64  \n 9   HR                100 non-null    int64  \n 10  R                 100 non-null    int64  \n 11  RBI               100 non-null    int64  \n 12  SB                100 non-null    int64  \n 13  CS                100 non-null    int64  \n 14  BB%               100 non-null    float64\n 15  K%                100 non-null    object \n 16  ISO               100 non-null    float64\n 17  BABIP             100 non-null    float64\n 18  EV                100 non-null    float64\n 19  AVG               100 non-null    float64\n 20  OBP               100 non-null    float64\n 21  SLG               100 non-null    float64\n 22  wOBA              100 non-null    float64\n 23  wRC+              100 non-null    int64  \n 24  Events            100 non-null    float64\n 25  EV.1              100 non-null    float64\n 26  maxEV             100 non-null    float64\n 27  LA                100 non-null    float64\n 28  Barrels           100 non-null    int64  \n 29  Barrel%           100 non-null    object \n 30  HardHit           100 non-null    int64  \n 31  HardHit%          100 non-null    float64\n 32  location          100 non-null    object \n 33  at_bats           100 non-null    float64\n 34  hard_hits         100 non-null    float64\n 35  correct_hardhit%  100 non-null    float64\ndtypes: float64(17), int64(12), object(7)\nmemory usage: 28.2+ KB\n\n\n\n# Create a pivot table to count the observations\npivot_table = aaronjudge.pivot_table(index='hard_hits', columns='H', aggfunc='size', fill_value=0)\n\n# Create a heatmap\nax = sns.heatmap(pivot_table, cmap=\"Blues\", annot=True, fmt=\"d\")\n\n# Customize the y-axis to start at 0 and increase as you go up\nax.set_yticklabels(ax.get_yticklabels(), rotation=0)\nax.invert_yaxis()\n\n# Customize the plot if needed\nplt.title(\"Heat Map Showing Hits v. Hard Hits\")\nplt.xlabel(\"Hits\")\nplt.ylabel(\"hard_hits\")\n\nplt.show()\n\n\n\n\nIn this heatmap, hits are represented on the x-axis, while hard hits are depicted on the y-axis. The unit of observation corresponds to a player’s at-bats within a game. Notably, there are instances, such as nine games for the specific player Aaron Judge, where he had two hard-hit balls but only managed to secure one hit. While the seaborn-generated graph above indeed suggests a positive correlation between these variables, there are discernible distinctions between them. It prompts the consideration that using hard hit percentage as a target variable to measure success may offer a more robust approach, as it mitigates factors beyond the batter’s control. For example, a batter might make solid contact (barrel the ball) but hit it directly to a fielder, categorizing it as a hard hit ball without resulting in a hit. Hence, hard hit percentage emerges as a more suitable target variable for assessment.\n\n# Sort the DataFrame by Date in ascending order\naaronjudge = aaronjudge.sort_values(by='Date')\n\n# Create subplots with 2 rows and 1 column\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\n\n# First subplot - correct_hardhit%\nsns.barplot(data=aaronjudge, y='correct_hardhit%', x='Date', ax=axes[0])\naxes[0].set_title(\"Aaron Judge Hard Hit Percentage (per each individual game) over the course of the 2023 season\")\naxes[0].set_xlabel(\"Date\")\naxes[0].set_ylabel(\"hard hit %\")\n# Get the x-axis tick positions\nx_ticks = axes[0].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[0].set_xticks(visible_ticks)\n\n# Second subplot - H\nsns.barplot(data=aaronjudge, y='H', x='Date', ax=axes[1])\naxes[1].set_title(\"Aaron Judge Hits over the course of the 2023 season\")\naxes[1].set_xlabel(\"Date\")\naxes[1].set_ylabel(\"Hits\")\n# Get the x-axis tick positions\nx_ticks = axes[1].get_xticks()\n\n# Show every 10th label\nvisible_ticks = x_ticks[::10]\n\n# Set the x-axis labels\naxes[1].set_xticks(visible_ticks)\n\n# Adjust the layout to avoid overlap\nplt.tight_layout()\n\n# Show the combined figure\nplt.show()\n\n\n\n\nThe depicted graph highlights the potential for uncovering meaningful trends in hard hit data, surpassing the simplistic examination of hits alone. It suggests the feasibility of leveraging past hard hit data to predict future hard hit performance, potentially driven by autocorrelation or seasonality. This insight holds promise for enhancing the precision of future models.\n\n\nHypothesis Refinement\nFollowing the above analysis, my null hypothesis, asserting that the “hot hand” is actually a fallacy, remains unchanged. However, the alternative hypotheses have been refined for both basketball and baseball analyses based on the insights drawn from the visualizations. In the context of basketball, the refined alternative hypothesis suggests that a player’s shot outcome is more likely to be influenced by their immediately preceding shot, rather than one taken several attempts ago. In the context of baseball, my alternative hypothesis suggests that a batter’s hard hit percentage is more likely influenced by their prior hard hit percentage rather than solely assessing success or streaks based on hits.\n\n\nExtra Joke\nWhat kind of car does Darth Vader drive? A toy-Yoda!  \n\n\nWatch Out!\nyou can’t be too careful when exploring!"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Why did the conclusion take a break? Because it needed some time to ‘sum up’ its thoughts!"
  },
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nIntroduction to Naive Bayes\nNaive Bayes, a widely acclaimed machine learning algorithm, harnesses Bayes’ Theorem to categorize data into predefined classes or categories. Praised for its simplicity, swift training capabilities, and robust performance, it stands as a foundational tool in data science. At its core, Bayes’ Theorem calculates the probability of event A given the occurrence of event B, expressed as: \\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\] Naive Bayes accomplishes classifications by leveraging feature vectors and the principles of Bayes’ Theorem to assess values. The ‘naive’ label in its name stems from its assumption of independence among predictors, simplifying computational tasks. This algorithm excels in contexts featuring text and categorical data, such as in applications like spam email identification, sentiment analysis, and document categorization. Despite its seemingly ‘naive’ premise, Naive Bayes consistently delivers impressive real-world performance, making it a crucial tool for various data science classification tasks.  Common varients of Naive Bayes include Multinomial, Guassian, and Bernoulli Naive Bayes. Multinomial Naive Bayes is the most common variant and is often used for text classification. Gaussian Naive Bayes is appropriate for continuous numerical data, while Bernoulli Naive Bayes is a derivation of Multinomial Naive Bayes that is appropriate for binary or boolean data.  The purpose of this page is to implement Naïve Bayes classification on a variety of datasets, some of which may be more for suitable than others for this method. This work is a component of my DSAN 5000 class project.\n\n\nData Preparation\nData must initially be prepared to utilize a Naive Bayes model. Although a substantial part of this process has been covered in the data cleaning and exploratory data analysis (EDA) phases, all categorical and label columns must be converted into factor types. Additionally, data must be split into training and test subsets. This is done to train the model on one subset and subsequently evaluate the model’s performance on an independent dataset which can be used to asses the bias and variance of the machine learning model. In the following code, we will complete the preparation of the 2021-22 NCAA data and news data for modeling.\n\nNCAA Data\n\n#here we are using R\n#let's read in the dataset\nnova2122 &lt;- read.csv('./data/modified_data/nova2122_updated.csv')\n\n\n# Load relevant libraries\nlibrary(tidyverse)\nlibrary(caret)\n\n\n#let's take another look at the dataset\nstr(nova2122)\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: int  -1 -1 -1 -1 1 1 -1 1 -1 -1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : int  NA NA -1 NA NA -1 NA NA 1 NA ...\n $ lag2                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : int  NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n# changing columns to become a factor\nnova2122$lag1 &lt;- as.factor(nova2122$lag1)\nnova2122$lag2 &lt;- as.factor(nova2122$lag2)\nnova2122$lag3 &lt;- as.factor(nova2122$lag3)\nnova2122$lag4 &lt;- as.factor(nova2122$lag4)\nnova2122$lag5 &lt;- as.factor(nova2122$lag5)\nnova2122$lag6 &lt;- as.factor(nova2122$lag6)\nnova2122$shot_outcome_numeric &lt;- as.factor(nova2122$shot_outcome_numeric)\n\n\n#looking at how this changed the dataset\nstr(nova2122)\n\n'data.frame':   5399 obs. of  15 variables:\n $ game_id             : int  401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 401365747 ...\n $ play_id             : int  4 7 11 13 16 18 19 21 23 25 ...\n $ half                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ shooter             : chr  \"Justin Moore\" \"Clifton Moore\" \"Clifton Moore\" \"Eric Dixon\" ...\n $ shot_outcome        : chr  \"missed\" \"missed\" \"missed\" \"missed\" ...\n $ shooter_team        : chr  \"Villanova\" \"La Salle\" \"La Salle\" \"Villanova\" ...\n $ shot_outcome_numeric: Factor w/ 2 levels \"-1\",\"1\": 1 1 1 1 2 2 1 2 1 1 ...\n $ shot_sequence       : int  -1 -1 -2 -1 1 1 -1 1 -1 -1 ...\n $ previous_shots      : int  0 0 -1 0 0 -1 0 0 1 0 ...\n $ lag1                : Factor w/ 2 levels \"-1\",\"1\": NA NA 1 NA NA 1 NA NA 2 NA ...\n $ lag2                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag3                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag4                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag5                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n $ lag6                : Factor w/ 2 levels \"-1\",\"1\": NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n# Set a seed for reproducibility\nset.seed(137)\n\n# Create an index for splitting the data (70% for training, 30% for validation)\nindex &lt;- createDataPartition(y = nova2122$shot_outcome_numeric, p = 0.7, list = FALSE)\n\n# Create the training and validation subsets\ntraining_data &lt;- nova2122[index, ]\nvalidation_data &lt;- nova2122[-index, ]\n\n#save these for later use\nwrite.csv(training_data, file = \"./data/modified_data/nova2122_training.csv\", row.names = FALSE)\nwrite.csv(validation_data, file = \"./data/modified_data/nova2122_validation.csv\", row.names = FALSE)\n\n\n\nNews Data\n\n# now we are using python\n\n#load in relevant libraries and the cleaned data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnewsapi = pd.read_csv('./data/modified_data/news_api_naive.csv')\n\n\n# let's take another look at the data\nnewsapi.head()\n\n\n\n\n\n\n\n\nTitle\nDescription\nSentiment Label\ncleaned_text\n\n\n\n\n0\nhow to watch jack catterall vs jorge linares l...\njack catterall hopes to add a win to his resum...\npositive\njack catterall hope add win resume redeem loss...\n\n\n1\njaguars vs steelers livestream: how to watch n...\njacksonville look to make it five wins in a ro...\npositive\njacksonville look make five win row head pitts...\n\n\n2\nvikings vs packers livestream: how to watch nf...\nwant to watch the minnesota vikings play the g...\npositive\nwant watch minnesota viking play green bay pac...\n\n\n3\ndolphins' chase claypool says there was 'frust...\nafter being traded from the 1-4 chicago bears ...\nnegative\ntraded chicago bear miami dolphin last friday ...\n\n\n4\nseahawks vs bengals livestream: how to watch n...\ntwo of the nfl's most potent offenses clash in...\nnegative\ntwo nfl potent offense clash cincinnati\n\n\n\n\n\n\n\n\n# Make \"Sentiment Label\" a categorical variable\nnewsapi['Sentiment Label'] = newsapi['Sentiment Label'].astype('category')\n\n# Remove rows with missing data\nnewsapi.dropna(inplace=True)\n\n# Remove unnecessary columns\nnewsapi = newsapi[['Sentiment Label', 'cleaned_text']]\n\n# Split the data into training, test, and validation subsets\ntrain_data, test_data = train_test_split(newsapi, test_size=0.2, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n\ntrain_data.head()\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n69\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n85\nnegative\nminnesota twin lost straight postseason game t...\n\n\n97\nnegative\npenn state spread season surprise many two les...\n\n\n38\npositive\nround scottish woman premier league celtic ran...\n\n\n2\npositive\nwant watch minnesota viking play green bay pac...\n\n\n\n\n\n\n\n\n\n\nFeature Selection\n\n#load in relevant libraries\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport sklearn \nimport itertools\nfrom scipy.stats import spearmanr\n\n\n# This function computers the figure of merit given a subset of features\n# it works for both the Pearson and Spearman correlation matrix\n\ndef merit(x, y, correlation='pearson'):\n    k = x.shape[1]\n    \n    if correlation == 'pearson':\n        rho_xx = np.mean(np.corrcoef(x, x, rowvar = False))\n        rho_xy = np.mean(np.corrcoef(x, y, rowvar = False))\n    elif correlation == 'spearman':\n        rho_xx = np.mean(spearmanr(x, x, axis = 0)[0])\n        rho_xy = np.mean(spearmanr(x, y, axis = 0)[0])\n    else:\n        raise ValueError(\"Error: Unsupported Correlation Method. Try Again.\")\n    \n    merit_numerator = k * np.absolute(rho_xy)\n    merit_denominator = np.sqrt(k + k * (k - 1) * np.absolute(rho_xx))\n    merit_score = merit_numerator / merit_denominator\n    \n    return merit_score\n\n\n# this function takes two matrices x and y, iterates over all possible subset combinations of the x features, \n# and computes the figure of merit for each subset. It keeps track of the max merit and returns the \n# optimal subset at the end\ndef maximize_CFS(x, y):\n    num_features = x.shape[1]\n    max_merit = 0\n    optimal_subset = None\n    list1 = [*range(0, num_features)]\n    for L in range(1, len(list1) + 1):\n        print(L/(len(list1)+1))\n        for subset in itertools.combinations(list1, L):\n            x_subset = x[:, list(subset)]\n            subset_merit = merit(x_subset, y)\n            if subset_merit &gt; max_merit:\n                max_merit = subset_merit\n                optimal_subset = list(subset)\n    return optimal_subset  # Return the indices of selected features\n\n\nNCAA Data\n\ntraining = pd.read_csv('./data/modified_data/nova2122_training.csv')\nvalidation = pd.read_csv('./data/modified_data/nova2122_validation.csv')\n\n# Convert DataFrames to numpy arrays\nx = training[['lag1', 'lag2', 'lag3']].values\nx = np.nan_to_num(x, nan=0)\ny = training[['shot_outcome_numeric']].values\n\nselected_indices = maximize_CFS(x, y)\nprint(selected_indices)\n\n[0]\n\n\n\nz = training['lag1'].values\nz = np.nan_to_num(x, nan=0)\n\nmerit(z,y)\n\n0.3818269784940867\n\n\nAn output of [0] in the above code indicates that, according to the Correlation-based Feature Selection (CFS) algorithm, the optimal subset comprises only the ‘lag1’ feature. This suggests that, under the criteria applied, ‘lag1’ provides the most valuable information for classifying the ‘shot_outcome_numeric’ variable.\n‘lag2’ and ‘lag3’ are considered less informative for predicting ‘shot_outcome_numeric’ using this particular feature selection approach and correlation-based merit score.\nAdditionally, a merit score of 0.3818 indicates a moderate positive correlation between the ‘lag1’ feature and ‘shot_outcome_numeric,’ suggesting that ‘lag1’ contains relevant information for predicting the target variable.\n\n\nNews Data\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Define features and target variable \nX_train = train_data['cleaned_text'].tolist()\ny_train = train_data['Sentiment Label']\n\n\n# our target variable needs to be labeled numerically (without strings)\n# lets fix that now\n\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\n\n# Assuming you have a DataFrame 'df' and a column 'Sentiment Label' that you want to map\ny_train = y_train.map(label_mapping)\n\ny_train.head()\n\n69    0\n85   -1\n97   -1\n38    1\n2     1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the text data\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# Convert text data to numerical features using CountVectorizer\nvectorizer = CountVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train)\n\n# convert the vectorized training data into a dataframe\ndf = pd.DataFrame(X_train_vectorized.toarray())\n\n#let's look at the new dataframe that will be used for training the naive bayes model\ndf.describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\ncount\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n...\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n72.000000\n\n\nmean\n0.013889\n0.013889\n0.013889\n0.013889\n0.041667\n0.013889\n0.013889\n0.013889\n0.013889\n0.013889\n...\n0.013889\n0.013889\n0.013889\n0.027778\n0.097222\n0.013889\n0.055556\n0.027778\n0.041667\n0.013889\n\n\nstd\n0.117851\n0.117851\n0.117851\n0.117851\n0.201229\n0.117851\n0.117851\n0.117851\n0.117851\n0.117851\n...\n0.117851\n0.117851\n0.117851\n0.165489\n0.298339\n0.117851\n0.230669\n0.165489\n0.201229\n0.117851\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 625 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 625 columns\n\n\n\n\n# it would take way to long to iterate every possible combination of subsets for x features, so let's reduce the number of features\n# we can do this by taking out words that do not appear often\n\n# let's calculate the sum of each column\ncolumn_sums = df.sum()\n\n# Find columns where the sum is 5 or less\ncolumns_to_remove = column_sums[column_sums &lt; 6].index\n\n# Remove the selected columns from the DataFrame\ndf = df.drop(columns=columns_to_remove)\n\ndf.head()\n\n\n\n\n\n\n\n\n80\n182\n201\n235\n267\n280\n302\n322\n355\n359\n474\n503\n519\n527\n542\n551\n601\n610\n619\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n# the above df is much smaller and more managable than what we had before\n\n#convert both to arrays\nx = df.values\ny = y_train.values\n\n# Implement feature selection using the maximize_CFS function\noptimal_subset_indices = maximize_CFS(x, y)\n\n# Select the optimal subset of features for training and validation data\nprint(optimal_subset_indices)\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n[17]\n\n\nThe numbers above [17] served as a progress indicator, representing the completion percentage of the maximize_CFS function. The earlier simplification was necessary to prevent the function from running indefinitely.  Much like the NCAA output mentioned earlier, the [17] index signifies the optimal subset as determined by the correlation-based feature selection algorithm. In this case, that index corresponds to the word “win,” as evident in the code below. This observation implies that, based on the given criteria, the presence of the word “win” offers the most significant information for accurately classifying the sentiment label variable.\n\nx_opt = df.iloc[:, 17]\nx_opt.head()\n\n0    0\n1    0\n2    0\n3    1\n4    0\nName: 610, dtype: int64\n\n\n\nword_index = 610 # The column index I want to look up\n\n# Get the vocabulary (word to column index mapping)\nvocabulary = vectorizer.vocabulary_\n\n# Inverse the vocabulary mapping to find the word for the given column index\nword = next(word for word, index in vocabulary.items() if index == word_index)\n\nprint(\"Word at column 610:\", word)\n\nWord at column 610: win\n\n\n\n\n\nNaive Bayes with Labeled Record Data\n\n# Load the e1071 package\nlibrary(e1071)\n\n# let's read in the data\nnova2122_training &lt;- read.csv(\"./data/modified_data/nova2122_training.csv\")\nnova2122_validation &lt;- read.csv('./data/modified_data/nova2122_validation.csv')\n\n# loading in the data caused some of the variables to become numeric, let's change them back to factors\nnova2122_training$lag1 &lt;- as.factor(nova2122_training$lag1)\nnova2122_training$shot_outcome_numeric &lt;- as.factor(nova2122_training$shot_outcome_numeric)\nnova2122_validation$lag1 &lt;- as.factor(nova2122_validation$lag1)\nnova2122_validation$shot_outcome_numeric &lt;- as.factor(nova2122_validation$shot_outcome_numeric)\n\n# Create a Naive Bayes model\nnb_model &lt;- naiveBayes(shot_outcome_numeric ~ lag1, data = nova2122_training)\n\n# Make predictions on the validation set\nvalidation_predictions &lt;- predict(nb_model, nova2122_validation, type = \"class\")\n\n# Assess the accuracy of the model\naccuracy &lt;- mean(validation_predictions == nova2122_validation$shot_outcome_numeric)\ncat(\"Accuracy of the Naive Bayes model:\", accuracy, \"\\n\")\n\nAccuracy of the Naive Bayes model: 0.5463535 \n\n\n\n# Create a confusion matrix\nconf_matrix &lt;- confusionMatrix(data = validation_predictions, reference = nova2122_validation$shot_outcome_numeric)\n\n# Print the confusion matrix\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  -1   1\n        -1 476 400\n        1  334 408\n                                          \n               Accuracy : 0.5464          \n                 95% CI : (0.5217, 0.5708)\n    No Information Rate : 0.5006          \n    P-Value [Acc &gt; NIR] : 0.0001276       \n                                          \n                  Kappa : 0.0926          \n                                          \n Mcnemar's Test P-Value : 0.0164312       \n                                          \n            Sensitivity : 0.5877          \n            Specificity : 0.5050          \n         Pos Pred Value : 0.5434          \n         Neg Pred Value : 0.5499          \n             Prevalence : 0.5006          \n         Detection Rate : 0.2942          \n   Detection Prevalence : 0.5414          \n      Balanced Accuracy : 0.5463          \n                                          \n       'Positive' Class : -1              \n                                          \n\n\nThe accuracy of the above model, 0.5464, indicates that the model is accurate 54.64% of the time. The model has a precision of 0.5499, inicating that 54.99% of the model’s positive predictions are correct. The recall of the above model, 0.5877, indicates that the model correctly predicted 58.77% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.5681 which can be used to compare performance to other classifiers.\n\n#using ggplot\n\n# Create the confusion matrix data\nconf_matrix_data &lt;- data.frame(\n  Prediction = c(\"missed\", \"made\", \"missed\", \"made\"),\n  Reference = c(\"missed\", \"missed\", \"made\", \"made\"),\n  Count = c(476, 334, 400, 408)\n)\n\n# Create the ggplot\ngg &lt;- ggplot(data = conf_matrix_data, aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Count)) +\n  geom_text(aes(label = Count), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Prediction\",\n    y = \"Reference\",\n    fill = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12))\n\ngg + ggtitle(\"Confusion Matrix for 2021-22 NCAA Villanova MBB Shot Data\")\n\n\n\n\nAs evident in the initial matrix, the subsequent table, and the confusion matrix created using ggplot, the performance of the Naive Bayes model appears to be rather lackluster. With an accuracy rate slightly exceeding 50%, the model’s predictive ability seems only marginally better than random chance. This outcome aligns with the null hypothesis, indicating that the concept of a “hot hand” is likely a misconception, and past performance may not be a reliable predictor of success. It’s worth highlighting that applying Naive Bayes to time series data, as done here, may not be the most suitable approach, given the unique characteristics of this type of data.\n\n\nNaive Bayes with Labeled Text Data\n\n# what does the test data look like? \ntest_data.head()\n\n\n\n\n\n\n\n\nSentiment Label\ncleaned_text\n\n\n\n\n83\nneutral\ncbs sport network weekly coverage season keep ...\n\n\n53\nneutral\nnfl week odds includes division matchup sunday...\n\n\n70\nneutral\nremoved\n\n\n45\npositive\nreal madrid look continue winning way keep per...\n\n\n44\npositive\nflorida gator hope end road game loss streak w...\n\n\n\n\n\n\n\n\n# let's separate our feature(s) from our target variable\nX_test = test_data['cleaned_text']\ny_test = test_data['Sentiment Label']\nX_test.head()\n\n83    cbs sport network weekly coverage season keep ...\n53    nfl week odds includes division matchup sunday...\n70                                              removed\n45    real madrid look continue winning way keep per...\n44    florida gator hope end road game loss streak w...\nName: cleaned_text, dtype: object\n\n\n\n# since we know the feature column only includes the occurances of the word \"win\", let's create a new column 'win_count'\nX_test_win = X_test.str.lower().str.count('win')\nX_test_win.head()\n\n83    0.0\n53    0.0\n70    0.0\n45    1.0\n44    0.0\nName: cleaned_text, dtype: float64\n\n\n\n# once again, we must convert our target variable to numeric labels \n\n# Define a dictionary to map labels to numeric values\nlabel_mapping = {'neutral': 0, 'negative': -1, 'positive': 1}\ny_test = test_data['Sentiment Label'].map(label_mapping)\n\ny_test.head()\n\n83    0\n53    0\n70    0\n45    1\n44    1\nName: Sentiment Label, dtype: category\nCategories (3, int64): [-1, 0, 1]\n\n\n\nimport pandas as pd\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.metrics import accuracy_score\n\n# Convert features to a DataFrame\nnews_training_x = pd.Series(x_opt, name='x')\nnews_training_y = y_train\nnews_test_x = pd.Series(X_test_win, name='x')\nnews_test_y = y_test\n\n# Create a training DataFrame\ntraining_df = pd.DataFrame({'x': news_training_x, 'y': news_training_y})\n\n# Create a test DataFrame\ntest_df = pd.DataFrame({'x': news_test_x, 'y': news_test_y})\n\n# Create a Naive Bayes model\nnb_model = CategoricalNB()\n\n# Remove rows with NaN values\ntraining_df.dropna(subset=['x'], inplace=True)\ntest_df.dropna(subset=['x'], inplace=True)\n\n# Remove rows with NaN values in the target variable 'y'\ntraining_df.dropna(subset=['y'], inplace=True)\ntest_df.dropna(subset=['y'], inplace=True)\n\n# Now, let's fit the model\nnb_model.fit(training_df[['x']], training_df['y'])\n\n# Make predictions on the validation set\nvalidation_predictions = nb_model.predict(test_df[['x']])\n\n# and we can finally assess the accuracy of the model\naccuracy = accuracy_score(test_df['y'], validation_predictions)\nprint(\"Accuracy of the Naive Bayes model:\", accuracy)\n\nAccuracy of the Naive Bayes model: 0.5\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(validation_predictions, test_df['y'])\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\nConfusion Matrix:\n[[ 0  0  0]\n [ 0  0  0]\n [ 5  5 10]]\n\n\n\n#calculate the various metrics\naccuracy = 10/20\nprecision = 10/20\nrecall = 10/10\nf1 = 2 * ((precision * recall) / (precision + recall))\n\n# Create a dictionary with the metrics\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n    'Value': [accuracy, precision, recall, f1]\n}\n\n# Create a DataFrame from the dictionary\nmetrics_df = pd.DataFrame(metrics)\n\nprint(metrics_df)\n\n      Metric     Value\n0   Accuracy  0.500000\n1  Precision  0.500000\n2     Recall  1.000000\n3         F1  0.666667\n\n\nThe accuracy of the above model, 0.5, indicates that the model is accurate 50% of the time. The model has a precision of 0.5, inicating that 50% of the model’s positive predictions are correct. The recall of the above model, 1, indicates that the model correctly predicted 100% of the made shots. Finally, the F1-score (combines precision and recall of a classifier by taking their harmonic mean) of the model is 0.67 which can be used to compare performance to other classifiers.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'conf_matrix' is the confusion matrix obtained previously\n\n# Create a Seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', linewidths=0.5, cbar=False,\n            xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n\n# Add labels and title\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Confusion Matrix')\n\n# Show the heatmap\nplt.show()\n\n\n\n\nAs demonstrated by the initial matrix, the subsequent table, and the confusion matrix crafted with Seaborn, the performance of the Naive Bayes model appears to be notably poor. The model’s accuracy reached 50%, but it’s vital to recognize a significant issue: it consistently predicted positive label values, indicating a significant underfitting of the data. This outcome tells us that the naive bayes classifier used was too simplistic to accurately predict the sentiment of the news articles.\n\n\nExtra Joke (x2)\nAre monsters good at math? Not unless you Count Dracula.    What’s the official animal of Pi day? The Pi-thon!"
  },
  {
    "objectID": "data-gathering.html",
    "href": "data-gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  This page will take you through the data sources and methodologies employed in this specific project. Furthermore, you can find brief descriptions/images/tables of the various datasets mentioned. Data must be acquired using at least one Python API and one R API. This project will use various data formats that may include labeled data, qualitative data, text data, geo data, record-data, etc.\n\nBaseballr\n“Baseballr” is a package in R that focuses on baseball analytics, also known as sabremetrics. It includes various functions that can be used for scraping data from websites like FanGraphs.com, Baseball-Reference.com, and BaseballSavant.mlb.com. It also includes functions for calculating specific baseball metrics such as wOBA (weighted on-base average) and FIP (fielding independent pitching). I will mainly use this package to gather data (which uses an API as can be seen below).\n\nSource Code\nThe below source code was pulled from the baseballr github repository. This specific code uses a mlb api to acquire play-by-play data for a specific game. I will use these functions later on through the baseballr package.\n\n\nCode\nlibrary(tidyverse)\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nCode\nmlb_api_call &lt;- function(url){\n  res &lt;-\n    httr::RETRY(\"GET\", url)\n  \n  json &lt;- res$content %&gt;%\n    rawToChar() %&gt;%\n    jsonlite::fromJSON(simplifyVector = T)\n  \n  return(json)\n}\n\nmlb_stats_endpoint &lt;- function(endpoint){\n  all_endpoints = c(\n    \"v1/attendance\",#\n    \"v1/conferences\",#\n    \"v1/conferences/{conferenceId}\",#\n    \"v1/awards/{awardId}/recipients\",#\n    \"v1/awards\",#\n    \"v1/baseballStats\",#\n    \"v1/eventTypes\",#\n    \"v1/fielderDetailTypes\",#\n    \"v1/gameStatus\",#\n    \"v1/gameTypes\",#\n    \"v1/highLow/types\",#\n    \"v1/hitTrajectories\",#\n    \"v1/jobTypes\",#\n    \"v1/languages\",\n    \"v1/leagueLeaderTypes\",#\n    \"v1/logicalEvents\",#\n    \"v1/metrics\",#\n    \"v1/pitchCodes\",#\n    \"v1/pitchTypes\",#\n    \"v1/playerStatusCodes\",#\n    \"v1/positions\",#\n    \"v1/reviewReasons\",#\n    \"v1/rosterTypes\",#\n    \"v1/runnerDetailTypes\",#\n    \"v1/scheduleEventTypes\",#\n    \"v1/situationCodes\",#\n    \"v1/sky\",#\n    \"v1/standingsTypes\",#\n    \"v1/statGroups\",#\n    \"v1/statTypes\",#\n    \"v1/windDirection\",#\n    \"v1/divisions\",#\n    \"v1/draft/{year}\",#\n    \"v1/draft/prospects/{year}\",#\n    \"v1/draft/{year}/latest\",#\n    \"v1.1/game/{gamePk}/feed/live\",\n    \"v1.1/game/{gamePk}/feed/live/diffPatch\",#\n    \"v1.1/game/{gamePk}/feed/live/timestamps\",#\n    \"v1/game/changes\",##x\n    \"v1/game/analytics/game\",##x\n    \"v1/game/analytics/guids\",##x\n    \"v1/game/{gamePk}/guids\",##x\n    \"v1/game/{gamePk}/{GUID}/analytics\",##x\n    \"v1/game/{gamePk}/{GUID}/contextMetricsAverages\",##x\n    \"v1/game/{gamePk}/contextMetrics\",#\n    \"v1/game/{gamePk}/winProbability\",#\n    \"v1/game/{gamePk}/boxscore\",#\n    \"v1/game/{gamePk}/content\",#\n    \"v1/game/{gamePk}/feed/color\",##x\n    \"v1/game/{gamePk}/feed/color/diffPatch\",##x\n    \"v1/game/{gamePk}/feed/color/timestamps\",##x\n    \"v1/game/{gamePk}/linescore\",#\n    \"v1/game/{gamePk}/playByPlay\",#\n    \"v1/gamePace\",#\n    \"v1/highLow/{orgType}\",#\n    \"v1/homeRunDerby/{gamePk}\",#\n    \"v1/homeRunDerby/{gamePk}/bracket\",#\n    \"v1/homeRunDerby/{gamePk}/pool\",#\n    \"v1/league\",#\n    \"v1/league/{leagueId}/allStarBallot\",#\n    \"v1/league/{leagueId}/allStarWriteIns\",#\n    \"v1/league/{leagueId}/allStarFinalVote\",#\n    \"v1/people\",#\n    \"v1/people/freeAgents\",#\n    \"v1/people/{personId}\",##U\n    \"v1/people/{personId}/stats/game/{gamePk}\",#\n    \"v1/people/{personId}/stats/game/current\",#\n    \"v1/jobs\",#\n    \"v1/jobs/umpires\",#\n    \"v1/jobs/datacasters\",#\n    \"v1/jobs/officialScorers\",#\n    \"v1/jobs/umpires/games/{umpireId}\",##x\n    \"v1/schedule/\",#\n    \"v1/schedule/games/tied\",#\n    \"v1/schedule/postseason\",#\n    \"v1/schedule/postseason/series\",#\n    \"v1/schedule/postseason/tuneIn\",##x\n    \"v1/seasons\",#\n    \"v1/seasons/all\",#\n    \"v1/seasons/{seasonId}\",#\n    \"v1/sports\",#\n    \"v1/sports/{sportId}\",#\n    \"v1/sports/{sportId}/players\",#\n    \"v1/standings\",#\n    \"v1/stats\",#\n    \"v1/stats/metrics\",##x\n    \"v1/stats/leaders\",#\n    \"v1/stats/streaks\",##404\n    \"v1/teams\",#\n    \"v1/teams/history\",#\n    \"v1/teams/stats\",#\n    \"v1/teams/stats/leaders\",#\n    \"v1/teams/affiliates\",#\n    \"v1/teams/{teamId}\",#\n    \"v1/teams/{teamId}/stats\",#\n    \"v1/teams/{teamId}/affiliates\",#\n    \"v1/teams/{teamId}/alumni\",#\n    \"v1/teams/{teamId}/coaches\",#\n    \"v1/teams/{teamId}/personnel\",#\n    \"v1/teams/{teamId}/leaders\",#\n    \"v1/teams/{teamId}/roster\",##x\n    \"v1/teams/{teamId}/roster/{rosterType}\",#\n    \"v1/venues\"#\n  )\n  base_url = glue::glue('http://statsapi.mlb.com/api/{endpoint}')\n  return(base_url)\n}\n\n\n\n\n\nCode\nx &lt;- \"http://statsapi.mlb.com/api/v1/game/575156/playByPlay\"\n\noutput &lt;- mlb_api_call(x)\n\n\n“output” is a very messy list that is extremely long. Instead of printing “output”, below are three images of part of the list.\n  \nThe below code builds on the previous code, returning a tibble that includes over 100 columns of data provided by the MLB Stats API at a pitch level. As you will see, the output is much cleaner and easier to work with.\n\n\nCode\n#' @rdname mlb_pbp\n#' @title **Acquire pitch-by-pitch data for Major and Minor League games**\n#'\n#' @param game_pk The date for which you want to find game_pk values for MLB games\n#' @importFrom jsonlite fromJSON\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#'\n#' Some data will vary depending on the\n#' park and the league level, as most sensor data is not available in\n#' minor league parks via this API. Note that the column names have mostly\n#' been left as-is and there are likely duplicate columns in terms of the\n#' information they provide. I plan to clean the output up down the road, but\n#' for now I am leaving the majority as-is.\n#'\n#' Both major and minor league pitch-by-pitch data can be pulled with this function.\n#' \n#'  |col_name                       |types     |\n#'  |:------------------------------|:---------|\n#'  |game_pk                        |numeric   |\n#'  |game_date                      |character |\n#'  |index                          |integer   |\n#'  |startTime                      |character |\n#'  |endTime                        |character |\n#'  |isPitch                        |logical   |\n#'  |type                           |character |\n#'  |playId                         |character |\n#'  |pitchNumber                    |integer   |\n#'  |details.description            |character |\n#'  |details.event                  |character |\n#'  |details.awayScore              |integer   |\n#'  |details.homeScore              |integer   |\n#'  |details.isScoringPlay          |logical   |\n#'  |details.hasReview              |logical   |\n#'  |details.code                   |character |\n#'  |details.ballColor              |character |\n#'  |details.isInPlay               |logical   |\n#'  |details.isStrike               |logical   |\n#'  |details.isBall                 |logical   |\n#'  |details.call.code              |character |\n#'  |details.call.description       |character |\n#'  |count.balls.start              |integer   |\n#'  |count.strikes.start            |integer   |\n#'  |count.outs.start               |integer   |\n#'  |player.id                      |integer   |\n#'  |player.link                    |character |\n#'  |pitchData.strikeZoneTop        |numeric   |\n#'  |pitchData.strikeZoneBottom     |numeric   |\n#'  |details.fromCatcher            |logical   |\n#'  |pitchData.coordinates.x        |numeric   |\n#'  |pitchData.coordinates.y        |numeric   |\n#'  |hitData.trajectory             |character |\n#'  |hitData.hardness               |character |\n#'  |hitData.location               |character |\n#'  |hitData.coordinates.coordX     |numeric   |\n#'  |hitData.coordinates.coordY     |numeric   |\n#'  |actionPlayId                   |character |\n#'  |details.eventType              |character |\n#'  |details.runnerGoing            |logical   |\n#'  |position.code                  |character |\n#'  |position.name                  |character |\n#'  |position.type                  |character |\n#'  |position.abbreviation          |character |\n#'  |battingOrder                   |character |\n#'  |atBatIndex                     |character |\n#'  |result.type                    |character |\n#'  |result.event                   |character |\n#'  |result.eventType               |character |\n#'  |result.description             |character |\n#'  |result.rbi                     |integer   |\n#'  |result.awayScore               |integer   |\n#'  |result.homeScore               |integer   |\n#'  |about.atBatIndex               |integer   |\n#'  |about.halfInning               |character |\n#'  |about.inning                   |integer   |\n#'  |about.startTime                |character |\n#'  |about.endTime                  |character |\n#'  |about.isComplete               |logical   |\n#'  |about.isScoringPlay            |logical   |\n#'  |about.hasReview                |logical   |\n#'  |about.hasOut                   |logical   |\n#'  |about.captivatingIndex         |integer   |\n#'  |count.balls.end                |integer   |\n#'  |count.strikes.end              |integer   |\n#'  |count.outs.end                 |integer   |\n#'  |matchup.batter.id              |integer   |\n#'  |matchup.batter.fullName        |character |\n#'  |matchup.batter.link            |character |\n#'  |matchup.batSide.code           |character |\n#'  |matchup.batSide.description    |character |\n#'  |matchup.pitcher.id             |integer   |\n#'  |matchup.pitcher.fullName       |character |\n#'  |matchup.pitcher.link           |character |\n#'  |matchup.pitchHand.code         |character |\n#'  |matchup.pitchHand.description  |character |\n#'  |matchup.splits.batter          |character |\n#'  |matchup.splits.pitcher         |character |\n#'  |matchup.splits.menOnBase       |character |\n#'  |batted.ball.result             |factor    |\n#'  |home_team                      |character |\n#'  |home_level_id                  |integer   |\n#'  |home_level_name                |character |\n#'  |home_parentOrg_id              |integer   |\n#'  |home_parentOrg_name            |character |\n#'  |home_league_id                 |integer   |\n#'  |home_league_name               |character |\n#'  |away_team                      |character |\n#'  |away_level_id                  |integer   |\n#'  |away_level_name                |character |\n#'  |away_parentOrg_id              |integer   |\n#'  |away_parentOrg_name            |character |\n#'  |away_league_id                 |integer   |\n#'  |away_league_name               |character |\n#'  |batting_team                   |character |\n#'  |fielding_team                  |character |\n#'  |last.pitch.of.ab               |character |\n#'  |pfxId                          |character |\n#'  |details.trailColor             |character |\n#'  |details.type.code              |character |\n#'  |details.type.description       |character |\n#'  |pitchData.startSpeed           |numeric   |\n#'  |pitchData.endSpeed             |numeric   |\n#'  |pitchData.zone                 |integer   |\n#'  |pitchData.typeConfidence       |numeric   |\n#'  |pitchData.plateTime            |numeric   |\n#'  |pitchData.extension            |numeric   |\n#'  |pitchData.coordinates.aY       |numeric   |\n#'  |pitchData.coordinates.aZ       |numeric   |\n#'  |pitchData.coordinates.pfxX     |numeric   |\n#'  |pitchData.coordinates.pfxZ     |numeric   |\n#'  |pitchData.coordinates.pX       |numeric   |\n#'  |pitchData.coordinates.pZ       |numeric   |\n#'  |pitchData.coordinates.vX0      |numeric   |\n#'  |pitchData.coordinates.vY0      |numeric   |\n#'  |pitchData.coordinates.vZ0      |numeric   |\n#'  |pitchData.coordinates.x0       |numeric   |\n#'  |pitchData.coordinates.y0       |numeric   |\n#'  |pitchData.coordinates.z0       |numeric   |\n#'  |pitchData.coordinates.aX       |numeric   |\n#'  |pitchData.breaks.breakAngle    |numeric   |\n#'  |pitchData.breaks.breakLength   |numeric   |\n#'  |pitchData.breaks.breakY        |numeric   |\n#'  |pitchData.breaks.spinRate      |integer   |\n#'  |pitchData.breaks.spinDirection |integer   |\n#'  |hitData.launchSpeed            |numeric   |\n#'  |hitData.launchAngle            |numeric   |\n#'  |hitData.totalDistance          |numeric   |\n#'  |injuryType                     |character |\n#'  |umpire.id                      |integer   |\n#'  |umpire.link                    |character |\n#'  |isBaseRunningPlay              |logical   |\n#'  |isSubstitution                 |logical   |\n#'  |about.isTopInning              |logical   |\n#'  |matchup.postOnFirst.id         |integer   |\n#'  |matchup.postOnFirst.fullName   |character |\n#'  |matchup.postOnFirst.link       |character |\n#'  |matchup.postOnSecond.id        |integer   |\n#'  |matchup.postOnSecond.fullName  |character |\n#'  |matchup.postOnSecond.link      |character |\n#'  |matchup.postOnThird.id         |integer   |\n#'  |matchup.postOnThird.fullName   |character |\n#'  |matchup.postOnThird.link       |character |\n#' @export\n#' @examples \\donttest{\n#'   try(mlb_pbp(game_pk = 632970))\n#' }\n\nmlb_pbp &lt;- function(game_pk) {\n  \n  mlb_endpoint &lt;- mlb_stats_endpoint(glue::glue(\"v1.1/game/{game_pk}/feed/live\"))\n  \n  tryCatch(\n    expr = {\n      payload &lt;- mlb_endpoint %&gt;% \n        mlb_api_call() %&gt;% \n        jsonlite::toJSON() %&gt;% \n        jsonlite::fromJSON(flatten = TRUE)\n      \n      plays &lt;- payload$liveData$plays$allPlays$playEvents %&gt;% \n        dplyr::bind_rows()\n      \n      at_bats &lt;- payload$liveData$plays$allPlays\n      \n      current &lt;- payload$liveData$plays$currentPlay\n      \n      game_status &lt;- payload$gameData$status$abstractGameState\n      \n      home_team &lt;- payload$gameData$teams$home$name\n      \n      home_level &lt;- payload$gameData$teams$home$sport\n      \n      home_league &lt;- payload$gameData$teams$home$league\n      \n      away_team &lt;- payload$gameData$teams$away$name\n      \n      away_level &lt;- payload$gameData$teams$away$sport\n      \n      away_league &lt;- payload$gameData$teams$away$league\n      \n      columns &lt;- lapply(at_bats, function(x) class(x)) %&gt;%\n        dplyr::bind_rows(.id = \"variable\")\n      cols &lt;- c(colnames(columns))\n      classes &lt;- c(t(unname(columns[1,])))\n      \n      df &lt;- data.frame(cols, classes)\n      list_columns &lt;- df %&gt;%\n        dplyr::filter(.data$classes == \"list\") %&gt;%\n        dplyr::pull(\"cols\")\n      \n      at_bats &lt;- at_bats %&gt;%\n        dplyr::select(-c(tidyr::one_of(list_columns)))\n      \n      pbp &lt;- plays %&gt;%\n        dplyr::left_join(at_bats, by = c(\"endTime\" = \"playEndTime\"))\n      \n      pbp &lt;- pbp %&gt;%\n        tidyr::fill(\"atBatIndex\":\"matchup.splits.menOnBase\", .direction = \"up\") %&gt;%\n        dplyr::mutate(\n          game_pk = game_pk,\n          game_date = substr(payload$gameData$datetime$dateTime, 1, 10)) %&gt;%\n        dplyr::select(\"game_pk\", \"game_date\", tidyr::everything())\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::mutate(\n          matchup.batter.fullName = factor(.data$matchup.batter.fullName),\n          matchup.pitcher.fullName = factor(.data$matchup.pitcher.fullName),\n          atBatIndex = factor(.data$atBatIndex)\n          # batted.ball.result = case_when(!result.event %in% c(\n          #   \"Single\", \"Double\", \"Triple\", \"Home Run\") ~ \"Out/Other\",\n          #   TRUE ~ result.event),\n          # batted.ball.result = factor(batted.ball.result,\n          #                             levels = c(\"Single\", \"Double\", \"Triple\", \"Home Run\", \"Out/Other\"))\n        ) %&gt;%\n        dplyr::mutate(\n          home_team = home_team,\n          home_level_id = home_level$id,\n          home_level_name = home_level$name,\n          home_parentOrg_id = payload$gameData$teams$home$parentOrgId,\n          home_parentOrg_name = payload$gameData$teams$home$parentOrgName,\n          home_league_id = home_league$id,\n          home_league_name = home_league$name,\n          away_team = away_team,\n          away_level_id = away_level$id,\n          away_level_name = away_level$name,\n          away_parentOrg_id = payload$gameData$teams$away$parentOrgId,\n          away_parentOrg_name = payload$gameData$teams$away$parentOrgName,\n          away_league_id = away_league$id,\n          away_league_name = away_league$name,\n          batting_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                       .data$home_team,\n                                       .data$away_team)),\n          fielding_team = factor(ifelse(.data$about.halfInning == \"bottom\",\n                                        .data$away_team,\n                                        .data$home_team)))\n      pbp &lt;- pbp %&gt;%\n        dplyr::arrange(desc(.data$atBatIndex), desc(.data$pitchNumber))\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::group_by(.data$atBatIndex) %&gt;%\n        dplyr::mutate(\n          last.pitch.of.ab =  ifelse(.data$pitchNumber == max(.data$pitchNumber), \"true\", \"false\"),\n          last.pitch.of.ab = factor(.data$last.pitch.of.ab)) %&gt;%\n        dplyr::ungroup()\n      \n      pbp &lt;- dplyr::bind_rows(baseballr::stats_api_live_empty_df, pbp)\n      \n      check_home_level &lt;- pbp %&gt;%\n        dplyr::distinct(.data$home_level_id) %&gt;%\n        dplyr::pull()\n      \n      # this will need to be updated in the future to properly estimate X,Z coordinates at the minor league level\n      \n      # if(check_home_level != 1) {\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.pX_est = predict(x_model, pbp),\n      #                   pitchData.coordinates.pZ_est = predict(y_model, pbp))\n      #\n      #   pbp &lt;- pbp %&gt;%\n      #     dplyr::mutate(pitchData.coordinates.x = -pitchData.coordinates.x,\n      #                   pitchData.coordinates.y = -pitchData.coordinates.y)\n      # }\n      \n      pbp &lt;- pbp %&gt;%\n        dplyr::rename(\n          \"count.balls.start\" = \"count.balls.x\",\n          \"count.strikes.start\" = \"count.strikes.x\",\n          \"count.outs.start\" = \"count.outs.x\",\n          \"count.balls.end\" = \"count.balls.y\",\n          \"count.strikes.end\" = \"count.strikes.y\",\n          \"count.outs.end\" = \"count.outs.y\") %&gt;%\n        make_baseballr_data(\"MLB Play-by-Play data from MLB.com\",Sys.time())\n    },\n    error = function(e) {\n      message(glue::glue(\"{Sys.time()}: Invalid arguments provided\"))\n    },\n    finally = {\n    }\n  ) \n  return(pbp)\n}\n\n#' @rdname get_pbp_mlb\n#' @title **(legacy) Acquire pitch-by-pitch data for Major and Minor League games**\n#' @inheritParams mlb_pbp\n#' @return Returns a tibble that includes over 100 columns of data provided\n#' by the MLB Stats API at a pitch level.\n#' @keywords legacy\n#' @export\n# get_pbp_mlb &lt;- mlb_pbp\n\n\n\n\nExample\nHere is an example using the mlb_pbp function.\n\n\nCode\nexample &lt;- (mlb_pbp(575156))\nhead(example)\n\n\n2023-10-12 13:40:46.684707: Invalid arguments provided\n\n\n\n\nA tibble: 6 x 146\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nabout.isTopInning\nmatchup.postOnFirst.id\nmatchup.postOnFirst.fullName\nmatchup.postOnFirst.link\nmatchup.postOnSecond.id\nmatchup.postOnSecond.fullName\nmatchup.postOnSecond.link\nmatchup.postOnThird.id\nmatchup.postOnThird.fullName\nmatchup.postOnThird.link\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;lgl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\n575156\n2019-06-01\n5\n2019-06-01T15:38:42.000Z\n2019-06-01T19:38:07.354Z\nTRUE\npitch\n05751566-0846-0063-000c-f08cd117d70a\n6\nIn play, out(s)\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n4\n2019-06-01T15:38:19.000Z\n2019-06-01T15:38:42.000Z\nTRUE\npitch\n05751566-0846-0053-000c-f08cd117d70a\n5\nFoul\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n3\n2019-06-01T15:38:02.000Z\n2019-06-01T15:38:19.000Z\nTRUE\npitch\n05751566-0846-0043-000c-f08cd117d70a\n4\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n2\n2019-06-01T15:37:45.000Z\n2019-06-01T15:38:02.000Z\nTRUE\npitch\n05751566-0846-0033-000c-f08cd117d70a\n3\nSwinging Strike\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n1\n2019-06-01T15:37:31.000Z\n2019-06-01T15:37:45.000Z\nTRUE\npitch\n05751566-0846-0023-000c-f08cd117d70a\n2\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n575156\n2019-06-01\n0\n2019-06-01T15:37:15.000Z\n2019-06-01T15:37:31.000Z\nTRUE\npitch\n05751566-0846-0013-000c-f08cd117d70a\n1\nBall\n...\nTRUE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nAcquiring Data\nI will pull more data eventually, but for now I am scraping two series of games from the 2023 season.\n\n\nCode\nlibrary(baseballr)\n\n\nThe below code allows me to find the correct game_pk values that I can then use to pull play-by-play data.\n\n\nCode\n#mlb_game_pks(\"2023-06-25\")\n# mlb_game_pks(\"2023-06-24\")\n# mlb_game_pks(\"2023-06-23\")\n\n\n\n\nCode\n#game_pk values\n\n#diamondbacks/giants - 717641, 717639, 717612\n\n#mariners/orioles - 717651, 717628, 717627\n\n\n\n\nCode\nx &lt;- c(717641, 717639, 717612, 717651, 717628, 717627)\nresult &lt;- lapply(x, mlb_pbp)\ncombined_tibble &lt;- bind_rows(result)\n# Save the data to a CSV file\nwrite.csv(combined_tibble, file = \"./data/raw_data/baseballr_six_games.csv\", row.names = FALSE)\nhead(combined_tibble)\n\n\n\nA baseballr_data: 6 x 160\n\n\ngame_pk\ngame_date\nindex\nstartTime\nendTime\nisPitch\ntype\nplayId\npitchNumber\ndetails.description\n...\nmatchup.postOnThird.link\nreviewDetails.isOverturned\nreviewDetails.inProgress\nreviewDetails.reviewType\nreviewDetails.challengeTeamId\nbase\ndetails.violation.type\ndetails.violation.description\ndetails.violation.player.id\ndetails.violation.player.fullName\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n...\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;chr&gt;\n\n\n\n\n717641\n2023-06-24\n2\n2023-06-24T04:40:41.468Z\n2023-06-24T04:40:49.543Z\nTRUE\npitch\na8483d6b-3cff-4190-827c-1b4c71f60ef8\n3\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n1\n2023-06-24T04:40:24.685Z\n2023-06-24T04:40:28.580Z\nTRUE\npitch\n49eba946-3aaa-4260-895b-3de29cb49043\n2\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n0\n2023-06-24T04:40:08.036Z\n2023-06-24T04:40:12.278Z\nTRUE\npitch\nf879f5a0-8570-4594-ae73-3f09d1a53ee1\n1\nBall\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n6\n2023-06-24T04:39:08.422Z\n2023-06-24T04:39:16.691Z\nTRUE\npitch\n3077f596-0221-4469-9841-f1684c629288\n6\nIn play, out(s)\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n5\n2023-06-24T04:38:49.567Z\n2023-06-24T04:38:53.482Z\nTRUE\npitch\n21a33e9d-e596-408b-9168-141acc0b1b63\n5\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n717641\n2023-06-24\n4\n2023-06-24T04:38:32.110Z\n2023-06-24T04:38:36.156Z\nTRUE\npitch\ndb083639-52be-41f4-b6d9-f72601ef1508\n4\nFoul\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nncaahoopR\n“ncaahoopR” is an R package tailored for NCAA Basketball Play-by-Play Data analysis. It excels at retrieving play-by-play data in a tidy format. For the purposes of this project, I will start by scraping play-by-play data for the Villanova Wildcats Men’s Basketball team from both the 2019-20 and 2021-22 seasons (the 2020-21 was shortened due to COVID-19).\n\n\nCode\ninstall.packages(\"devtools\")\ndevtools::install_github(\"lbenz730/ncaahoopR\")\nlibrary(ncaahoopR)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/lb/dk54cbx965z7nj61zps2fzr00000gn/T//RtmpqXinFj/downloaded_packages\n\n\nSkipping install of 'ncaahoopR' from a github remote, the SHA1 (9bd97fec) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\n\n\n\nCode\nVillanova1920 &lt;- get_pbp(\"Villanova\", \"2019-20\")\nVillanova2122 &lt;- get_pbp(\"Villanova\", \"2021-22\")\nwrite.csv(Villanova1920, file = \"./data/raw_data/villanova1920.csv\", row.names = FALSE)\nwrite.csv(Villanova2122, file = \"./data/raw_data/villanova2122.csv\", row.names = FALSE)\n\n\n\n\nCode\nhead(Villanova1920)\n\n\n\nA data.frame: 6 x 39\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\nshot_y\nshot_team\nshot_outcome\nshooter\nassist\nthree_pt\nfree_throw\npossession_before\npossession_after\nwrong_time\n\n\n\n&lt;chr&gt;\n&lt;date&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;lgl&gt;\n\n\n\n\n1\n401169778\n2019-11-05\nVillanova\nArmy\n1\n1\n19:37\n2377\n2377\nSaddiq Bey made Jumper.\n...\nNA\nVillanova\nmade\nSaddiq Bey\nNA\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n2\n401169778\n2019-11-05\nVillanova\nArmy\n2\n1\n19:16\n2356\n2356\nTucker Blackwell made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nTucker Blackwell\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n3\n401169778\n2019-11-05\nVillanova\nArmy\n3\n1\n19:01\n2341\n2341\nFoul on Jermaine Samuels.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n4\n401169778\n2019-11-05\nVillanova\nArmy\n4\n1\n19:01\n2341\n2341\nJermaine Samuels Turnover.\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nVillanova\nArmy\nFALSE\n\n\n5\n401169778\n2019-11-05\nVillanova\nArmy\n5\n1\n18:42\n2322\n2322\nMatt Wilson made Jumper. Assisted by Tommy Funk.\n...\nNA\nArmy\nmade\nMatt Wilson\nTommy Funk\nFALSE\nFALSE\nArmy\nVillanova\nFALSE\n\n\n6\n401169778\n2019-11-05\nVillanova\nArmy\n6\n1\n18:31\n2311\n2311\nJeremiah Robinson-Earl made Jumper. Assisted by Justin Moore.\n...\nNA\nVillanova\nmade\nJeremiah Robinson-Earl\nJustin Moore\nFALSE\nFALSE\nVillanova\nArmy\nFALSE\n\n\n\n\n\n\n\nReddit\n\n\nCode\n#install.packages(\"RedditExtractoR\") #only executable in Rstudio\nlibrary(RedditExtractoR)\n\n\nsubreddit &lt;- \"baseball\"\n\n# Get posts from the r/baseball subreddit\nstreaks &lt;- find_thread_urls(keywords = \"streak\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nhot &lt;- find_thread_urls(keywords = \"hot\" ,subreddit=subreddit, sort_by=\"top\", period = 'year')\n\nwrite.csv(streaks, file = \"./streaks.csv\", row.names = FALSE)\nwrite.csv(hot, file = \"./hot.csv\", row.names = FALSE)\n\n\nBelow you can see the first few rows of both the “hot” and “streaks” csv files.\n \n\n\nNews API\n\n\nCode\nAPI_KEY='05d7ae99b5b7455191c97c2c5c3a1f9b'\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n#updated code\n\n#| code-fold: true\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\ndef gettingdata(TOPIC):\n    URLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n    print(baseURL)\n    print(URLpost)\n\n    #GET DATA FROM API\n    response = requests.get(baseURL, URLpost) #request data from the server\n    print(response.url);  \n    response = response.json() #extract txt data from request into json\n\n    # PRETTY PRINT\n    # https://www.digitalocean.com/community/tutorials/python-pretty-print-json\n\n    #print(json.dumps(response, indent=2))\n\n    # #GET TIMESTAMP FOR PULL REQUEST\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n    # SAVE TO FILE \n    with open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n        json.dump(response, outfile, indent=4)\n\n    def string_cleaner(input_string):\n        try: \n            out=re.sub(r\"\"\"\n                        [,.;@#?!&$-]+  # Accept one or more copies of punctuation\n                        \\ *           # plus zero or more copies of a space,\n                        \"\"\",\n                         \" \",          # and replace it with a single space\n                        input_string, flags=re.VERBOSE)\n\n            #REPLACE SELECT CHARACTERS WITH NOTHING\n            out = re.sub('[’.]+', '', input_string)\n\n            #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n            out = re.sub(r'\\s+', ' ', out)\n\n            #CONVERT TO LOWER CASE\n            out=out.lower()\n        except:\n            print(\"ERROR\")\n            out=''\n        return out\n    \n    article_list=response['articles']   #list of dictionaries for each article\n    article_keys=article_list[0].keys()\n    #print(\"AVAILABLE KEYS:\")\n    #print(article_keys)\n    index=0\n    cleaned_data=[];  \n    for article in article_list:\n        tmp=[]\n        if(verbose):\n            print(\"#------------------------------------------\")\n            print(\"#\",index)\n            print(\"#------------------------------------------\")\n\n        for key in article_keys:\n            if(verbose):\n                print(\"----------------\")\n                print(key)\n                print(article[key])\n                print(\"----------------\")\n\n            #if(key=='source'):\n                #src=string_cleaner(article[key]['name'])\n                #tmp.append(src) \n\n            #if(key=='author'):\n                #author=string_cleaner(article[key])\n                #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n                #if(src in author): \n                    #print(\" AUTHOR ERROR:\",author);author='NA'\n                #tmp.append(author)\n\n            if(key=='title'):\n                tmp.append(string_cleaner(article[key]))\n\n            if(key=='description'):\n                tmp.append(string_cleaner(article[key]))\n\n            # if(key=='content'):\n            #     tmp.append(string_cleaner(article[key]))\n\n            #if(key=='publishedAt'):\n                #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n                #ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n                #date=article[key]\n                #if(not ref.match(date)):\n                    #print(\" DATE ERROR:\",date); date=\"NA\"\n                #tmp.append(date)\n\n        cleaned_data.append(tmp)\n        index+=1\n    text = cleaned_data\n    return text\n\nTOPIC = 'sports streak'\nhotstreak = gettingdata(TOPIC)\n\nhotstreak_df = pd.DataFrame(hotstreak, columns=['Title', 'Description'])\n\n\nhotstreak_df.to_csv('./data/raw_data/newsapi_2.csv', index=False)\n\n\n\n\nCode\n%%capture\n\nTOPIC = 'hot streak sports'\n\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n\n\n#GET DATA FROM API\nresponse = requests.get(baseURL, URLpost) #request data from the server\n# print(response.url);  \nresponse = response.json() #extract txt data from request into json\n\n\n\n# #GET TIMESTAMP FOR PULL REQUEST\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n\n# SAVE TO FILE \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\narticle_list=response['articles']   #list of dictionaries for each article\narticle_keys=article_list[0].keys()\n#print(\"AVAILABLE KEYS:\")\n#print(article_keys)\nindex=0\ncleaned_data=[];  \nfor article in article_list:\n    tmp=[]\n    if(verbose):\n        print(\"#------------------------------------------\")\n        print(\"#\",index)\n        print(\"#------------------------------------------\")\n\n    for key in article_keys:\n        if(verbose):\n            print(\"----------------\")\n            print(key)\n            print(article[key])\n            print(\"----------------\")\n\n        if(key=='source'):\n            src=string_cleaner(article[key]['name'])\n            tmp.append(src) \n\n        if(key=='author'):\n            author=string_cleaner(article[key])\n            #ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)\n            if(src in author): \n                print(\" AUTHOR ERROR:\",author);author='NA'\n            tmp.append(author)\n\n        if(key=='title'):\n            tmp.append(string_cleaner(article[key]))\n\n        # if(key=='description'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        # if(key=='content'):\n        #     tmp.append(string_cleaner(article[key]))\n\n        if(key=='publishedAt'):\n            #DEFINE DATA PATERN FOR RE TO CHECK  .* --&gt; wildcard\n            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n            date=article[key]\n            if(not ref.match(date)):\n                print(\" DATE ERROR:\",date); date=\"NA\"\n            tmp.append(date)\n\n    cleaned_data.append(tmp)\n    index+=1\n\ndf1 = pd.DataFrame(cleaned_data)\ndf1.to_csv('./data/raw_data/newsapi.csv', index=False) #,index_label=['title','src','author','date','description'])\n\n\nBelow you can see the first few rows of the newsapi.csv file:  \n\n\nIndividual Player Data\nI also want to eventually scrape specific data from fangraphs. For now, I was able to download a few tables that had game data for Aaron Judge and then merge them together. Below are screen shots of the initial csv file.   \n\n\nExtra Joke\nHow much data can be stored in a glacier? A frostbite!  \n\n\nCode\n# trying new stuff out\n# Obtain data using your gettingdata function\nTOPIC = 'sports streak'\nhotstreak = gettingdata(TOPIC)\n\n\n\n\nCode\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nnltk.download('vader_lexicon')\nsia = SentimentIntensityAnalyzer()\n\nsentiment_scores = []\n\nfor text_pair in hotstreak:\n    title, description = text_pair\n    score = sia.polarity_scores(description)\n\n    sentiment_scores.append({'title': title, 'description': description, 'sentiment_score': score})\n\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\n\n\nCode\nwith open('sentiment_scores.json', 'w') as json_file:\n    json.dump(sentiment_scores, json_file, indent=4)"
  },
  {
    "objectID": "decision-trees.html",
    "href": "decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "What did the tree do when the bank closed?\nIt started its own branch."
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "ARM",
    "section": "",
    "text": "Why did the arm apply for a job? Because it wanted to lend a helping hand!  Why did the naive Bayesian suddenly feel patriotic when he heard fireworks? He assumed independence.  I was surprised when my niece said she learned R at school yesterday, and then I remembered she’s 4 and she meant the letter. My priors are all too skewed."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "three more to go!"
  },
  {
    "objectID": "dimensionality-reduction.html",
    "href": "dimensionality-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Introduction\nThis study delves into Villanova’s 2021-22 season NCAA shot data, spotlighting six key features. Using Python and sklearn, we employ Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. This approach trims features while preserving variance, simplifying data for improved model comprehension and visualization.\n\n\nDimensionality Reduction with PCA\nPrincipal Component Analysis (PCA) is a valuable machine learning technique used to simplify large datasets by reducing their dimensionality. The primary goal is to decrease the number of variables while retaining crucial information.  Five Key Steps in PCA: \n\nStep 0. Load in relevant libraries and data\n\n\nCode\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nnova = pd.read_csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# only keeping the shot data\nnova = nova.dropna(subset=['shooter'])\n\n# Creating a new column to specify the team of the shooter\nnova['shooter_team'] = np.where(nova['action_team'] == \"home\", nova['home'], nova['away'])\n\n# only keeping the villanova shots\nnova = nova[nova['shooter_team'] == 'Villanova']\n\n# changing shot outcome to numeric\nnova['shot_outcome_numeric'] = nova['shot_outcome'].apply(lambda x: 1 if x == 'made' else 0)\n\n\n\n\nCode\n#creating a new column called shot value\nnova['shot_value'] = 2  # Default value for shots that are not free throws or three-pointers\nnova.loc[nova['free_throw'], 'shot_value'] = 1\nnova.loc[nova['three_pt'], 'shot_value'] = 3\n\n# Calculate the mean of shot_outcome for each player (field goal percentage)\nmean_and_count_data = nova.groupby('shooter').agg(\n    shots=('shot_outcome', 'count'),\n    field_goal_percentage=('shot_outcome_numeric', lambda x: x[x == 1].count() / len(x) if len(x) &gt; 0 else 0)\n).sort_values(by='shots', ascending=False)\n\n# Add the calculated field goal percentage to the original DataFrame\nnova = nova.merge(mean_and_count_data[['field_goal_percentage']], left_on='shooter', right_index=True, how='left').round(4)\n\n# create a lag variable for the previous shot (1 indicates made shot, -1 indicates miss, 0 indicates no previous shot in half\nnova = nova.sort_values(by=['shooter', 'game_id', 'play_id'])  # Arrange the data by shooter, game_id, and play_id\nnova['lag1'] = nova.groupby(['shooter', 'game_id'])['shot_outcome_numeric'].shift(1)\nnova['lag1'] = nova['lag1'].replace({0: -1}).fillna(0)  # Replace initial 0 values with -1, and NaN values with 0\nnova = nova.sort_values(by=['game_id', 'play_id'])\n\n# reset the index\nnova = nova.reset_index(drop=True)\n\n# create a new column for the home crowd\nnova['home_crowd'] = (nova['home'] == 'Villanova').astype(int)\n\n# create a new column for the game number in the season\nnova['game_num'] = nova['game_id'].astype('category').cat.codes + 1\n\nnova.head()\n\n\n\n\n\n\n\n\n\ngame_id\ndate\nhome\naway\nplay_id\nhalf\ntime_remaining_half\nsecs_remaining\nsecs_remaining_absolute\ndescription\n...\npossession_before\npossession_after\nwrong_time\nshooter_team\nshot_outcome_numeric\nshot_value\nfield_goal_percentage\nlag1\nhome_crowd\ngame_num\n\n\n\n\n0\n401365747\n2021-11-28\nLa Salle\nVillanova\n4\n1\n19:22\n2362\n2362\nJustin Moore missed Three Point Jumper.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n3\n0.4721\n0.0\n0\n1\n\n\n1\n401365747\n2021-11-28\nLa Salle\nVillanova\n13\n1\n18:32\n2312\n2312\nEric Dixon missed Dunk.\n...\nVillanova\nVillanova\nFalse\nVillanova\n0\n2\n0.5794\n0.0\n0\n1\n\n\n2\n401365747\n2021-11-28\nLa Salle\nVillanova\n16\n1\n18:18\n2298\n2298\nCollin Gillespie made Three Point Jumper.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n3\n0.5337\n0.0\n0\n1\n\n\n3\n401365747\n2021-11-28\nLa Salle\nVillanova\n18\n1\n17:35\n2255\n2255\nEric Dixon made Layup.\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5794\n-1.0\n0\n1\n\n\n4\n401365747\n2021-11-28\nLa Salle\nVillanova\n21\n1\n16:59\n2219\n2219\nJermaine Samuels made Layup. Assisted by Eric ...\n...\nVillanova\nLa Salle\nFalse\nVillanova\n1\n2\n0.5535\n0.0\n0\n1\n\n\n\n\n5 rows × 46 columns\n\n\n\n\n\nCode\n# subsetting my data into feature varaibles and target variable\nfeature_columns = ['shot_value', 'field_goal_percentage', 'lag1', 'home_crowd', 'score_diff', 'game_num']\nnova_features = nova[feature_columns].copy()\n\ntarget_column = ['shot_outcome_numeric']\nnova_target = nova[target_column].copy()\n\n# save feature_columns to csv for clustering\nnova_features.to_csv('./data/modified_data/nova_features.csv', index=False)\n\nnova_features.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2764 entries, 0 to 2763\nData columns (total 6 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   shot_value             2764 non-null   int64  \n 1   field_goal_percentage  2764 non-null   float64\n 2   lag1                   2764 non-null   float64\n 3   home_crowd             2764 non-null   int64  \n 4   score_diff             2764 non-null   int64  \n 5   game_num               2764 non-null   int8   \ndtypes: float64(2), int64(3), int8(1)\nmemory usage: 110.8 KB\n\n\n\n\nStep 1. Standardization\nNormalize data to have a mean of 0 and a standard deviation of 1.\n\n\nCode\n# Standardization\nscaler = StandardScaler()\nnova_features_standardized = scaler.fit_transform(nova_features)\n\n\n\n\nStep 2. Covariance Matrix Computation\nCalculate the covariance matrix to understand variable relationships.\n\n\nCode\n#covariance matrix\nco_ma = np.cov(nova_features_standardized, rowvar=False)\nprint(co_ma)\n\n\n[[ 1.00036193 -0.1413293  -0.16465368  0.00400472  0.0018319   0.00327538]\n [-0.1413293   1.00036193  0.08216798 -0.05311615 -0.04587765 -0.01289989]\n [-0.16465368  0.08216798  1.00036193  0.04351646  0.04402943 -0.00288759]\n [ 0.00400472 -0.05311615  0.04351646  1.00036193  0.49188691  0.31830859]\n [ 0.0018319  -0.04587765  0.04402943  0.49188691  1.00036193  0.12171352]\n [ 0.00327538 -0.01289989 -0.00288759  0.31830859  0.12171352  1.00036193]]\n\n\n\n\nStep 3. Computing Eigenvectors and Eigenvalues\nIdentify principal components using eigenvectors and eigenvalues.\n\n\nCode\n#eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(co_ma)\nprint(\"Eigenvalues\\n\",\"----------------------\")\nprint(eigenvalues)\nprint(\"\\nEigenvectors\\n\",\"----------------------\")\nprint(eigenvectors)\n\n\nEigenvalues\n ----------------------\n[1.65668919 1.26200685 0.46449209 0.81824262 0.8669812  0.9337596 ]\n\nEigenvectors\n ----------------------\n[[-0.01233438  0.63776031  0.00198086 -0.75043735 -0.17142614  0.02371907]\n [ 0.09828166 -0.51666678 -0.01569294 -0.30589464 -0.50573557  0.61139994]\n [-0.06667471 -0.57041517  0.01469836 -0.56821948  0.30014023 -0.50695901]\n [-0.66744252 -0.01515272 -0.73678     0.0201096  -0.1050718  -0.00127727]\n [-0.5923083  -0.02488482  0.60566041  0.08768672 -0.46111118 -0.24781971]\n [-0.43524065  0.00974191  0.29977402 -0.11093025  0.63332208  0.55425972]]\n\n\n\n\nStep 4. Feature Vectors\nSelect eigenvectors as new feature vectors.\n\n\nCode\n# choosing principal components\n\n# sort the eigenvalues in descending order\nsorted_index = np.argsort(eigenvalues)[::-1]\nsorted_eigenvalue = eigenvalues[sorted_index]\n\n\n\n\nStep 5. Recasting Data Among Principal Component Axis\nTransform data using chosen principal components.\n\n\nCode\n# PCA with components decided above\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\ndesired_variance = 0.75 \nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\n\npca = PCA(n_components=num_components)\nnova_pca = pca.fit_transform(nova_features_standardized)\n\n\n\n\nDeciding optimal number of components\nTo decide the optimal number of components, we can use both a cumulative explained variance plot and a scree plot to visualize explained variance ratio.\n\n\nCode\n# Cumulative Explained Variance Plot\ncumulative_explained_variance = np.cumsum(sorted_eigenvalue) / sum(sorted_eigenvalue)\nplt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', color='#FFB6C1')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Scree Plot\nplt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\nexplained_variance_ratio = pca.explained_variance_ratio_\nprint(\"Explained Variance Ratio for Each Component:\")\nprint(explained_variance_ratio)\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='#FFB6C1')\nplt.title('Scree Plot')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\n\nplt.tight_layout()  # Adjust layout for better spacing\nplt.show()\n\n# find the number of variables it takes to reach a variance of 0.75\ndesired_variance = 0.75\nnum_components = np.argmax(cumulative_explained_variance &gt;= desired_variance) + 1\nprint(f\"Number of components to capture {desired_variance * 100}% variance: {num_components}\")\n\n\nExplained Variance Ratio for Each Component:\n[0.27601497 0.21025838 0.1555703  0.14444459]\nNumber of components to capture 75.0% variance: 4\n\n\n\n\n\nAs a general guideline, the goal is to retain at least 80% of the variance. However, given the relatively small size of our dataset, we have adjusted the threshold to 75%. Therefore, we will select 4 components, ensuring the cumulative explained variance surpasses 75%.\n\n\nVisualizing reduced-dimensional data\nNow, let’s visualize the reduced-dimensional data using a scatter plot of the first two principal components.\n\n\nCode\n# pca scatter plot\nplt.scatter(nova_pca[:, 0], nova_pca[:, 1], alpha=0.5, color='#D8BFD8')\nplt.title('PCA Scatter Plot')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n# limit PCA to 4 components\npca = PCA(n_components=4)\n\n# save nova_pca to csv\nnova_pca_df = pd.DataFrame(nova_pca)\nnova_pca_df.to_csv('./data/modified_data/nova_pca.csv', index=False)\n\n\n\n\n\nThe scree plot guides us in determining that capturing 75% of the variance necessitates employing four principal components. The scatter plot, showcasing the reduced-dimensional data, visually represents patterns within the dataset. You many notice that a seperation occurs where Principal Component 1 equals 0. While PCA excels at identifying linear relationships, it’s important to acknowledge that observations with higher variability may be distant from the main cluster. These steps underscore how PCA simplifies dimensionality reduction, fostering a deeper understanding of the dataset. It’s worth noting that the sklearn library’s PCA function automates these procedures for ease of implementation.\n\n\n\nDimensionality Reduction with t-SNE\nt-SNE, or t-distributed Stochastic Neighbor Embedding, is an unsupervised non-linear dimensionality reduction technique designed to explore and visualize high-dimensional data. It transforms complex datasets into a lower-dimensional space, emphasizing preserving local relationships among data points. By finding similarity measures between pairs of instances in higher and lower dimensional spaces and optimizing these measures, t-SNE enhances our ability to interpret intricate datasets.\nAdditionally, exploring clustering in this context allows me to identify distinct groups or patterns within the NCAA shot data. By combining t-SNE, a dimensionality reduction technique, with KMeans clustering, I can uncover and visualize natural structures or associations in the dataset. The choice of three clusters is informed by the results obtained on the clustering page. Exploring different perplexity values enhances the flexibility of my analysis, helping me discover nuanced patterns at varying levels of detail.\n\n\nCode\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport pandas as pd\n\ndef explore_tsne(perplexity_value):\n    X = nova_features.iloc[:, :]\n\n    # t-SNE for 3 dimensions with different perplexity\n    tsne = TSNE(n_components=3, random_state=1, perplexity=perplexity_value)\n    X_tsne = tsne.fit_transform(X)\n\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    clusters = kmeans.fit_predict(X)\n\n    # Create a DataFrame with 3D data\n    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2', 'Dimension 3'])\n    tsne_df['Cluster'] = clusters\n\n    # Interactive 3D scatter plot with plotly\n    fig = px.scatter_3d(tsne_df, x='Dimension 1', y='Dimension 2', z='Dimension 3',\n                        color='Cluster', symbol='Cluster', opacity=0.7, size_max=10,\n                        title=f't-SNE 3D Visualization (Perplexity={perplexity_value})',\n                        labels={'Cluster': 'Cluster'})\n\n    # Show the plot\n    fig.show()\n\n# Explore t-SNE with different perplexity values\nperplexities = [5, 20, 40]  # Add more values as needed\nfor perplexity_value in perplexities:\n    explore_tsne(perplexity_value)\n\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/williammcgloin/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nPerplexity in t-SNE determines the balance between capturing local and global relationships in the data’s low-dimensional representation. Lower perplexity values focus on local details, higher values emphasize global structures, while moderate values strike a balance. Experimenting with different perplexity values helps find an optimal configuration for visualizing and understanding the dataset. As I explored various perplexity values, I noticed that with larger perplexity values, clusters became more distinct, revealing clearer patterns and structures within the data. This observation underscores the importance of choosing an appropriate perplexity value for the specific characteristics of the dataset, ultimately enhancing the effectiveness of t-SNE in revealing underlying structures.\n\n\nEvaluation & Comparison\nIn summary, PCA efficiently preserves the overall structure, making it well-suited for large datasets with linear relationships. Conversely, t-SNE excels at unveiling local structures and clusters, offering enhanced visualization for smaller datasets. The decision between these techniques hinges on factors like dataset size, structure, and specific analysis goals.\nIn my analysis, it became evident that certain variables play a crucial role in explaining most of the variance in our dataset. Despite having only six feature variables, retaining four allows us to preserve over 75% of the variance, indicating limited redundancy. The application of t-SNE for cluster visualization proved insightful, revealing subtle overlaps within the clusters. This aligns with previous observations, reinforcing that identifying distinct clusters in this dataset poses challenges.\n\n\nExtra Joke\nIf we were compressed down to a single dimension… what would be the point of it all?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN_website",
    "section": "",
    "text": "This will be updated with some cool stuff in a bit! Stay tuned!\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello!\nHello and welcome to my website! My name is Billy and I am a graduate student at Georgetown University. I am currently pursuing a Master’s in Data Science and Analytics and will use this website as a place to display my work. I hope you find my work insightful and enjoy your time on my website!\n\n\nGrowing Up\nI grew up in Scotch Plains, New Jersey along with my two sisters, Melissa and Jessica. We are triplets! Below is a picture of me with my family and another of our dog Roxie.\n\n \n\n\n\nEducation\nI graduated from Villanova University in 2021 with an Honors Degree Double Major in Economics and Business Analytics along with a PPE (Politics, Philosophy, & Economics) Concentration and Political Science minor. I absolutely loved my time at Villanova between the academics, friendships, the school winning the NCAA Championship and more!. As an undergrad, I was involved in a number of student organizations, including the Blue Key Society, where I served as a tour guide for undergraduate admissions. I also organized accepted students days for the university.\nOne of the best experiences I had during college was studying abroad. In 2019, I participated in the Institute of Politics and Economics Program at the University of Cambridge where I took a number of economic classes and study international relations and history. Seeing how these subject matters interact with analytics truly fascinated me, which is one reason why I am pursuing an advanced degree in data science. I am extremely excited to continue my education and pursue a masters degree in data science!\n\n\nTop Shelf Designs, LLC\nDuring my senior year at Villanova, I had the opportunity to start my own business with my sisters! Our business, Top Shelf Designs, LLC, is a retail business that designs, builds, and sells college dorm shelving units to help students maximize their space. Being an entrepreneur has been an incredibly rewarding journey thus far, and I am eager to see where it takes us. Here is a link to our website!\n\n\nHobbies\nI’m an avid sports fan, and love watching the New York Yankees, Villanova basketball, and NY Giants. Unfortunately, none of my teams have been doing too well lately, but I’m hoping the Giants can turn it around this season! I’m fascinated by how analytics is being used to transform the sports landscape - even across college sports! While at Villanova, I researched the Houston Astros Cheating Scandal. I would love to go back and take an even deeper dive into the subject one day!\nI love music - you can often find me with headphones or earbuds in while doing work or exploring the DC area! Additionally, I find playing instruments to be a great creative outlet after a long day of work. I grew up taking piano lessons and during COVID started to teach myself how to play the guitar!\nWhenever I have the chance, I try to travel. Wile abroad, I was lucky enough to travel to a few places in Europe. My favorite spot has to be Italy - from the history and culture to the food, wine, and sports, I am counting down the days until I can go back.\nFinally, I thoroughly enjoy books, movies, video games in addition to fun, in-person experiences. Lately, I’ve been exploring the DC area which I am loving so far! If you have any recommendations - please let me know! I’m alwyas willing to try something new!\n\n\nGeneral Info (inc. netID)\n\n\n\nname\nBilly McGloin\n\n\nGU netID\nwtm30\n\n\nLinkedIn\nlink\n\n\n\n\n\nJokes\nI always like to have a laugh so below are some jokes I hope you’ll enjoy!\n\nWebsite Jokes\n\nWhat do you call a doctor who fixes websites? A URL-ologist.\nWebsites use cookies to improve performance. I do the same.\nWhat website has the information on all DJs? The wiki wiki\n\n\n\nAssorted Jokes\n\nWhat do you call it when a caveman farts? A blast from the past.\nWhy didn’t the bell work at the gym? It was a dumb bell!\n\n\n\n\nHoya Saxa!\n\n\n\nme and a bunch of rocks!"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Why did the linear regression model break up with the logistic regression model? Because it wanted a more ‘linear’ relationship!"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress  This page shows the raw data, the code used to clean it, and the modified data. It’s a journal of my data cleaning process. Please be aware that this page contains both Python and R code, thus you should avoid running the source code all at once.\n\nBaseballr\n\n\nncaahoopR\nlet’s clean the Villanova 2021-22 data with R:\nhere is a screen shot of the first few rows and columns of the raw data:  \n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\n# let's load in the data\nnova2122 &lt;- read.csv('./data/raw_data/villanova2122.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n1140539\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova2122)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova2122 &lt;- nova2122 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova2122)\n\n\n\n539939\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova2122 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova2122.csv\", row.names = FALSE)\n\n\nHere is a screen shot of the modified data:  \n\n2019-20 Season cleaning\n\n\nCode\n# let's load in the data\nnova1920 &lt;- read.csv('./data/raw_data/villanova1920.csv')\n\n\n\n\nCode\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n958139\n\n\n\n\nCode\n# what are the column names?\ncolnames(nova1920)\n\n\n\n'game_id''date''home''away''play_id''half''time_remaining_half''secs_remaining''secs_remaining_absolute''description''action_team''home_score''away_score''score_diff''play_length''scoring_play''foul''win_prob''naive_win_prob''home_time_out_remaining''away_time_out_remaining''home_favored_by''total_line''referees''arena_location''arena''capacity''attendance''shot_x''shot_y''shot_team''shot_outcome''shooter''assist''three_pt''free_throw''possession_before''possession_after''wrong_time'\n\n\n\n\nCode\n# this data looks relatively clean, but we want only shooting data\n# let's get rid of rows where there isn't a shooter\n# this would be rows where the shooter is NA\n# such as a turnover, steal, rebound, or block\nnova1920 &lt;- nova1920 %&gt;%\n  filter(!is.na(shooter))\n\n# let's check the shape of the data\ndim(nova1920)\n\n\n\n454339\n\n\n\n\nCode\n# we can see that we removed about 5,000 rows and are left with just a little over half the initial data\n\n# only taking the columns I want from this dataset\nsample &lt;- nova1920 %&gt;% select(game_id, play_id, half, shooter, shot_outcome, home, away, action_team)\n\n#creating a new column shooter_team\nsample &lt;- sample %&gt;%\n  mutate(\n    shooter_team = ifelse(action_team == \"home\", home, away))\n\n# Specifying columns to drop and removing them from the dataframe\ncolumns_to_drop &lt;- c(\"home\", \"away\", \"action_team\")\n\nsample &lt;- sample %&gt;%\n  select(-one_of(columns_to_drop))\n\n#I want to create a previous_shots column that says how many shots the shooter has made or missed in a row before the current shot they are taking\nsample &lt;- sample %&gt;%\n  mutate(\n    shot_outcome_numeric = ifelse(shot_outcome == \"made\", 1, -1)\n  )\n\nsample &lt;- sample %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    shot_sequence = cumsum(shot_outcome_numeric)) %&gt;%\n  ungroup()\n\nsample3 &lt;- sample %&gt;%\n  mutate(\n    shot_sequence = ifelse(shot_outcome == \"made\" & shot_sequence &lt;= 0, 1,\n                  ifelse(shot_outcome == \"missed\" & shot_sequence &gt;= 0, -1, shot_sequence))\n  )\n\nsample3 &lt;- sample3 %&gt;%\n  group_by(game_id, half, shooter) %&gt;%\n  arrange(play_id) %&gt;%\n  mutate(\n    previous_shots = ifelse(row_number() == 1, 0, lag(shot_sequence, default = 0))\n  ) %&gt;%\n  ungroup()\n\nwrite.csv(sample3, file = \"./data/modified_data/nova1920.csv\", row.names = FALSE)\n\n\n\n\n\nreddit\n\n\nnewsapi\nlet’s clean this using python:  here is a picture of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n\nCode\n#reading in the file\nnewsapi = pd.read_csv('./data/raw_data/newsapi.csv')\n\n\n\n\nCode\n# what is the shape of this data\nnewsapi.shape\n\n\n(100, 4)\n\n\n\n\nCode\n# what are the column names\nnewsapi.columns\n\n\nIndex(['0', '1', '2', '3'], dtype='object')\n\n\n\n\nCode\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/williammcgloin/nltk_data...\n\n\nTrue\n\n\n\n\nCode\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the Lemmatizer and stopwords list\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    \n    # Tokenization and lowercase\n    words = text.lower().split()\n    \n    # Remove stopwords and apply lemmatization\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    \n    return ' '.join(words)\n\n# Apply preprocessing to the 'text' column\nnewsapi['cleaned_text'] = newsapi['2'].apply(preprocess_text)\n\n\n\n\nCode\n# Initialize the CountVectorizer\ncount_vectorizer = CountVectorizer()\n\n# Fit and transform the preprocessed text data\nX = count_vectorizer.fit_transform(newsapi['cleaned_text'])\n\n# printing part of the sparse matrix\nprint(X[:20, :20].toarray())\n\n\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]]\n\n\n\n\nCode\n# Convert the sparse matrix to a Pandas DataFrame\nX_df = pd.DataFrame(X.toarray())\n\n# Display the first few rows of the DataFrame\nprint(X_df.head())\n\n\n   0    1    2    3    4    5    6    7    8    9    ...  539  540  541  542  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   543  544  545  546  547  548  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 549 columns]\n\n\n\n\nCode\nvocab = count_vectorizer.get_feature_names_out()\n\nprint(vocab)\n\n\n['abnormality' 'abound' 'ac' 'acc' 'action' 'advice' 'aew' 'ahead' 'al'\n 'alds' 'alert' 'already' 'alyssa' 'amid' 'andy' 'angelos' 'another'\n 'answer' 'apple' 'argentina' 'arizona' 'arkansas' 'armondo' 'arsenal'\n 'astonishing' 'austin' 'back' 'balogun' 'ban' 'baseball' 'battle' 'bear'\n 'bearcat' 'beat' 'beaten' 'become' 'becoming' 'behind' 'belief'\n 'bellingham' 'bengal' 'besides' 'best' 'bet' 'better' 'beyond' 'big'\n 'biggs' 'bird' 'blount' 'blue' 'blunder' 'bonus' 'booing' 'booking'\n 'boston' 'bottom' 'brad' 'brain' 'brave' 'breaking' 'breanna' 'brewer'\n 'buccaneer' 'building' 'bukayo' 'bumper' 'call' 'candidate' 'cant' 'card'\n 'cardinal' 'cargill' 'carlos' 'case' 'casino' 'catch' 'central' 'cfb'\n 'champion' 'charge' 'chicago' 'christian' 'cincinnati' 'city' 'climate'\n 'clinch' 'close' 'closer' 'clue' 'clutch' 'coach' 'coaching'\n 'cockeysville' 'college' 'colorado' 'column' 'come' 'coming' 'commander'\n 'commits' 'comparison' 'complete' 'concern' 'conference' 'contender'\n 'corner' 'could' 'cover' 'coverage' 'cowboy' 'cpa' 'craziness' 'crazy'\n 'crop' 'crowd' 'cub' 'cup' 'dalton' 'dame' 'daniel' 'david' 'debut'\n 'defense' 'deion' 'delight' 'deserved' 'desmond' 'desperate' 'despite'\n 'division' 'dolphin' 'doubleheader' 'draw' 'driven' 'drought' 'duck'\n 'duke' 'dy' 'eagle' 'earns' 'eberflus' 'edge' 'emerges' 'end' 'enter'\n 'episode' 'er' 'europe' 'evaluation' 'even' 'event' 'ever' 'everywhere'\n 'evokes' 'excited' 'exit' 'expert' 'eye' 'fact' 'fade' 'falcon' 'famu'\n 'fantasy' 'father' 'favored' 'favorite' 'fiba' 'field' 'fighter'\n 'fighting' 'final' 'fire' 'fired' 'first' 'five' 'fix' 'fiziev' 'fizzle'\n 'florida' 'flyweight' 'focus' 'folarin' 'football' 'force' 'form' 'found'\n 'franklin' 'game' 'gamrot' 'gen' 'giant' 'goal' 'god' 'goff' 'going'\n 'good' 'got' 'grade' 'grasso' 'guez' 'ham' 'hardy' 'harsh' 'head' 'heat'\n 'heavily' 'here' 'hero' 'highlight' 'hit' 'home' 'honor' 'hot' 'huge'\n 'hurricane' 'hype' 'image' 'infamous' 'injures' 'issue' 'ja' 'jack'\n 'jade' 'jay' 'jet' 'jimmy' 'john' 'join' 'jones' 'journey' 'jr' 'jude'\n 'julio' 'justin' 'kansa' 'keep' 'kelce' 'key' 'king' 'knee' 'knockout'\n 'larger' 'laugh' 'leaf' 'league' 'learned' 'life' 'line' 'lion' 'lionel'\n 'live' 'livestream' 'locked' 'lofty' 'logan' 'look' 'loose' 'loser'\n 'loses' 'losing' 'loss' 'lsu' 'lucas' 'madrid' 'mailbag' 'main' 'make'\n 'makeover' 'man' 'manager' 'manchester' 'marquee' 'maryland' 'match'\n 'mateusz' 'matt' 'matter' 'mature' 'meeting' 'megan' 'messi' 'miami'\n 'middleweight' 'milan' 'mlb' 'mojo' 'moment' 'monday' 'morning' 'move'\n 'must' 'mvp' 'nailed' 'napoli' 'nbas' 'near' 'network' 'new' 'newcastle'\n 'nfl' 'night' 'noche' 'normal' 'notify' 'notre' 'nowhere' 'number' 'nwsl'\n 'nycs' 'nz' 'odds' 'ode' 'offense' 'ohio' 'oklahoma' 'onuachu' 'opponent'\n 'option' 'oregon' 'oriole' 'padre' 'panther' 'pass' 'pat' 'patriot'\n 'penn' 'penultimate' 'perfect' 'peter' 'phillies' 'philly' 'pick'\n 'pirate' 'play' 'player' 'playing' 'playoff' 'plenty' 'plus' 'point'\n 'politics' 'pool' 'pound' 'power' 'prediction' 'pretender' 'preview'\n 'previewing' 'problem' 'prop' 'provides' 'pulisic' 'purdy' 'question'\n 'race' 'racer' 'racing' 'rafael' 'ram' 'ranger' 'ranking' 'rapinoe' 'ray'\n 'read' 'real' 'recap' 'record' 'recruit' 'red' 'regular' 'reign' 'relish'\n 'remain' 'removed' 'report' 'resident' 'retro' 'return' 'review' 'revved'\n 'ridder' 'ripe' 'rise' 'river' 'rock' 'rodon' 'rodr' 'rome' 'ross'\n 'rumor' 'run' 'running' 'ryder' 'saka' 'sander' 'say' 'schedule' 'score'\n 'sean' 'search' 'season' 'seat' 'sec' 'secure' 'sends' 'sept' 'series'\n 'served' 'shadow' 'shanahan' 'shevchenko' 'shift' 'show' 'showdown'\n 'since' 'skid' 'slate' 'smack' 'snap' 'sold' 'someone' 'sooner' 'sox'\n 'special' 'speed' 'spiro' 'sport' 'spot' 'spotlight' 'spread' 'st'\n 'stamp' 'stand' 'star' 'start' 'starting' 'state' 'station' 'stats'\n 'stay' 'stellar' 'stewart' 'stoppage' 'straight' 'strategy' 'streak'\n 'stream' 'strickland' 'struggling' 'success' 'summer' 'survivor'\n 'suwinski' 'suzuki' 'sv' 'sweep' 'swift' 'tailspin' 'take' 'takeaway'\n 'taking' 'tale' 'talking' 'tape' 'taylor' 'tcu' 'team' 'ten' 'test'\n 'texas' 'theater' 'thing' 'think' 'thirteen' 'thomas' 'thought' 'three'\n 'thriller' 'thursday' 'tight' 'time' 'title' 'tko' 'tnf' 'today' 'tom'\n 'top' 'total' 'tournament' 'trail' 'transformation' 'travis' 'trea'\n 'trojan' 'troy' 'trust' 'trying' 'tssaa' 'turkey' 'turner' 'tv' 'twin'\n 'two' 'ucf' 'ufc' 'unbeatable' 'unbeaten' 'united' 'unsung' 'upset'\n 'upside' 'usc' 'usmnt' 'uwf' 'vega' 'veloudos' 'video' 'view' 'vlad'\n 'wagon' 'warm' 'watch' 'watching' 'week' 'weekend' 'welcome' 'well'\n 'went' 'white' 'whitner' 'wild' 'wildcat' 'williams' 'wilson' 'win'\n 'winner' 'winning' 'wnba' 'woman' 'world' 'wr' 'wrong' 'wwe' 'yahoo'\n 'yankee' 'year' 'yet' 'youth']\n\n\n\n\nindividual player data\nlet’s clean the aaron judge game data with python:\nhere is a screen shot of the first few rows of the raw data:  \n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\n\n\nCode\n#reading in the file\naaronjudge = pd.read_csv('./data/raw_data/AaronJudgeData.csv')\n\n\n\n\nCode\n#how many rows are in this dataset?\naaronjudge.shape\n\n\n(111, 37)\n\n\n\n\nCode\n#what are the column names?\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1',\n       'Events', 'EV.1', 'maxEV', 'LA', 'Barrels', 'Barrel%', 'HardHit',\n       'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n#removing the repeated columns\ncolumns_to_remove = ['Date.1', 'Team.1', 'Opp.1', 'BO.1', 'Pos.1']\naaronjudge.drop(columns=columns_to_remove, inplace=True)\naaronjudge.columns\n\n\nIndex(['Date', 'Team', 'Opp', 'BO', 'Pos', 'PA', 'H', '2B', '3B', 'HR', 'R',\n       'RBI', 'SB', 'CS', 'BB%', 'K%', 'ISO', 'BABIP', 'EV', 'AVG', 'OBP',\n       'SLG', 'wOBA', 'wRC+', 'Events', 'EV.1', 'maxEV', 'LA', 'Barrels',\n       'Barrel%', 'HardHit', 'HardHit%'],\n      dtype='object')\n\n\n\n\nCode\n# i belive the initial row with the column names is repeated throughou the data. let's check\nprint((aaronjudge['Date'] == 'Date').sum())\n\n\n5\n\n\n\n\nCode\n# let's remove these rows and then check the shape again\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Date'].index, inplace=True)\naaronjudge.shape\n\n\n(106, 32)\n\n\n\n\nCode\n# there is also a total row which I want to remove as well. let's do that now\naaronjudge.drop(aaronjudge[aaronjudge['Date'] == 'Total'].index, inplace=True)\naaronjudge.shape\n\n\n(105, 32)\n\n\n\n\nCode\n# so far, I have removed 6 rows and 5 columns. \n\n# I want to create a \"location\" column based on the \"@\" in the \"Opp\" column\naaronjudge['location'] = aaronjudge['Opp'].apply(lambda x: 'away' if '@' in x else 'home')\n\n# Remove the \"@\" symbol from the values in the \"Opp\" column\naaronjudge['Opp'] = aaronjudge['Opp'].str.replace('@', '')\n\n# check value counts of the new \"location\" column\nprint(aaronjudge['location'].value_counts()) #this seems accurate\n\n\nhome    53\naway    52\nName: location, dtype: int64\n\n\n\n\nCode\nprint(aaronjudge['PA'].dtype)\nprint(aaronjudge['BB%'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# I want to create two new columns. The number of at bats per each game and the number of hard hits in each game. \n# for this project, we are going to calculate at-bats as should be the number of plate appearances minus walks (sacrifices and HBP are not included in this dataset)\n\n#first i have to remove the '%' symbol and convert 'BB%' to a float\n\naaronjudge['BB%'] = aaronjudge['BB%'].astype(str)\naaronjudge['BB%'] = aaronjudge['BB%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'BB%' column to three decimal places\naaronjudge['BB%'] = aaronjudge['BB%'].round(3)\n\n#print(aaronjudge['BB%'].mean())\n\n#convert 'PA' to a float\naaronjudge['PA'] = aaronjudge['PA'].astype(float)\n\n# now I can create the new at_bats column\naaronjudge['at_bats'] = aaronjudge['PA'] * (1 - aaronjudge['BB%'])\n\n#now lets see the average number of at bats vs the average number of plate appearances\nprint(aaronjudge['at_bats'].mean())\nprint(aaronjudge['PA'].mean())\n\n\n3.4857333333333336\n4.314285714285714\n\n\n\n\nCode\n# now I want to create a new column for hard hits per game\n# we can do this by multiplying the hard hit percentage by the events column (these columns were part of a different table that was merged with the original table)\n\nprint(aaronjudge['HardHit%'].dtype)\nprint(aaronjudge['Events'].dtype)\n\n\nobject\nobject\n\n\n\n\nCode\n# this code is very similar to what we just did\n\n#first i have to remove the '%' symbol and convert 'HardHit%' to a float\n\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].astype(str)\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].str.rstrip('%').astype(float) / 100.0\n\n# Round the 'HardHit%' column to three decimal places\naaronjudge['HardHit%'] = aaronjudge['HardHit%'].round(3)\n\n#print(aaronjudge['HardHit%'].mean())\n\n#convert 'Events' to a float\naaronjudge['Events'] = aaronjudge['Events'].astype(float)\n\n# now I can create the new hard_hits column\naaronjudge['hard_hits'] = (aaronjudge['Events'] * aaronjudge['HardHit%']).round(0)\n\n#now lets see the average number of hard_hits per game\nprint(aaronjudge['hard_hits'].mean())\n\n\n1.52\n\n\n\n\nCode\n# finally, let's create a correct hardhit% column that is based on the number of at-bats, not the number of times a player puts the ball in play\naaronjudge['correct_hardhit%'] = (aaronjudge['hard_hits'] / aaronjudge['at_bats']).round(2)\n\n# now let's see the average correct hardhit% for Aaron Judge\nprint(aaronjudge['correct_hardhit%'].mean())\n\n\n0.42829999999999996\n\n\n\n\nCode\n# sometimes in certain stadiums or based on the weather, the HardHit% data is missing\n# this causes the value of the new correct_hardhit% column to be NaN, so let's remove those few rows\naaronjudge.dropna(subset=['correct_hardhit%'], inplace=True)\n\n#let's check the shape again\naaronjudge.shape #loss of 5 rows\n\n\n(100, 36)\n\n\n\n\nCode\n# now we can save this to a csv file\naaronjudge.to_csv('./data/modified_data/aaronjudge.csv', index=False)\n\n\nhere is a screenshot of the first couple rows of the modified csv file:  \n\n\nExtra Joke\nWhat did the broom say to the vacuum?\n“I’m so tired of people pushing us around.”  \n\n\nCode\n# trying something out \nimport pandas as pd\n\n# Load the sentiment scores from the JSON file\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\n# Create lists to store data\ntitles = []  # List to store document titles\ndescriptions = []   # List to store document descriptions\nsentiment_labels = []  # List to store sentiment labels\n\n# Extract the scores, titles, descriptions, and labels\nfor idx, item in enumerate(sentiment_scores, start=1):\n    titles.append(item.get('title', ''))  # Get the title of the document\n    descriptions.append(item.get('description', ''))    # Get the description of the document\n    sentiment_score = item.get('sentiment_score', {})\n    \n    # Determine the sentiment label based on the compound score\n    if sentiment_score.get('compound', 0) &gt; 0:\n        sentiment_labels.append('positive')\n    elif sentiment_score.get('compound', 0) == 0:\n        sentiment_labels.append('neutral')\n    else:\n        sentiment_labels.append('negative')\n\n# Create a DataFrame\ndata = {\n    'Title': titles,\n    'Description': descriptions,\n    'Sentiment Label': sentiment_labels\n}\n\ndf_with_labels = pd.DataFrame(data)\n\ndf_with_labels.head()\n\n# Save to CSV\ndf_with_labels.to_csv('./data/modified_data/sentiment_scores_with_titles.csv', index=False)"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "*Disclaimer - this section is a work in progress"
  },
  {
    "objectID": "introduction.html#prior-research",
    "href": "introduction.html#prior-research",
    "title": "Introduction",
    "section": "Prior Research",
    "text": "Prior Research\nThe initial investigation into this topic Gilovich, Vallone, and Tversky (1985) was published in 1985. It analyzed assorted data, including professional basketball field goal data from the 1980-1981 season, professional basketball free-throw data from the 1980-1982 seasons, and a controlled shooting experiment. While the study found that over 91% of fans agreed that a player has a better chance of making a shot after having just made his last two or three shots than he does after having just missed his last two or three shots, none of their data showed any evidence of this phenomenon. Instead, Gilovich, Vallone, and Tversky (1985) argued that there is a wide-spread misperception of random sequences:\n\nPeople’s intuitive conceptions of randomness depart systematically from the laws of chance. It appears that people expect the essential characteristics of a chance process to be represented not only globally in the entire sequences, but also locally, in each of its parts. For instance, people expect even short sequences of heads and tails to reflect the fairness of a coin and contain roughly 50% heads and 50% tails. This conception of chance has been described as a ‘belief in the law of small numbers’ according to which the law of large numbers applies to small samples as well. A locally representative sequence, however, deviates systematically from chance expectation: It contains too many alternations and not enough long runs.\n\nBar-Eli, Avugos, and Raab (2006) published a review and critique of 20 years of “hot hand” research in 2006. This paper reviewed the Gilovich, Vallone, and Tversky (1985) study in addition to subsequent research that used data from various sports including basketball, baseball, golf, darts, tennis, bowling, and more. Baseball and basketball studies dominate the literature on this subject, yet the strongest support for the “hot hand” can be found in more individual sports such as horseshoe pitching and tennis.\n\nDemonstrations of hot hands per se are rare and often weak, due to various reasons: using and unrealistic model and questionable data, setting questionable definitions for hot and cold players, relating streakiness to difficulty of task, combining and analyzing data of all players a as a group, and other constraints related to the kind of sport studied.\n\nIn the end, this study found that the question remains unresolved.  MORE RESEARCH TO COME"
  },
  {
    "objectID": "introduction.html#the-debate",
    "href": "introduction.html#the-debate",
    "title": "Introduction",
    "section": "The Debate",
    "text": "The Debate\nMORE TO BE ADDED"
  },
  {
    "objectID": "introduction.html#questions",
    "href": "introduction.html#questions",
    "title": "Introduction",
    "section": "10 Questions",
    "text": "10 Questions\n\nWhat data is available for my topic?\nWhat does current literature on the topic argue?\nHow can I build off of the current research and approach the topic in a novel way?\nShould I limit the scope of my topic to sports or can I expand past that?\nHow should I define success? (the concept of the hot hand is that success breeds success)\nWhat is the best way to visualize the data?\nDo athletes and/or the public believe in this phenomenon?\nIs there any evidence that the hot hand exists?\nDoes the hot hand impact strategy of a game? Should it impact strategy?\nIf the hot hand does exist, in what sport (or area outside of sports) is there the most evidence in support of the phenomenon?"
  },
  {
    "objectID": "introduction.html#goals-and-hypothesis",
    "href": "introduction.html#goals-and-hypothesis",
    "title": "Introduction",
    "section": "Goals and Hypothesis",
    "text": "Goals and Hypothesis\n\nI would love to find some evidence that the hot hand exists"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilovich, Vallone, and Tversky (1985)↩︎\nBar-Eli, Avugos, and Raab (2006)↩︎"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "I will replace this later with my actual data"
  }
]